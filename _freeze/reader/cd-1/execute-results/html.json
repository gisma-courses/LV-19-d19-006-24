{
  "hash": "3993533d9421fe28f4b9466400af186e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Change Detection - Sentinel  \nauthor: Chris Reudenbach\ndate: '2024-12-06'\nbibliography: references.bib\nsubtitle: 'Clusteranalysis, Maximum-Likelihood and ML'\ntitle-block-banner: ../reader/images/grubenwiese-sp.jpg\ntitle-block-banner-color: white\neditor: \n  markdown: \n    wrap: sentence\n---\n\n\nIn the geosciences, remote sensing is the only measurement technique that allows complete coverage of large spatial areas, up to the entire Earth's surface.\nIts successful application requires both the use of existing methods and the adaptation and development of new ones.\n\n## Introduction\n\nIn geospatial or environmental informatics, the detection of changes to the Earth's surface using satellite, aircraft or drone images, known as change detection analysis, is an important application.\nThese results are often linked to biophysical, geophysical or anthropogenic processes in order to gain both a deeper understanding and the possibility of developing predictive models.\nMethods of image analysis are of outstanding importance for generating spatial information from the underlying processes.\nSince both the quantity and quality of this \"image data\" are playing an increasingly important role in environmental monitoring and modeling, it is becoming more and more necessary to integrate \"big data\" concepts into the analyses.\nThis means performing reproducible analyses with large amounts of data (\\>\\> 1 TB). This is essential for both scientific knowledge gain and future societal challenges.\n\nAs already explained in the introduction, we start with a scalable change detection analysis of forest damage in low mountain ranges, which is a typical application-oriented task.\nScalable means that we limit the analysis to a manageable area, the Nordwestharz, and to two time slices.\nHowever, the resulting algorithm can be applied to different or larger areas and to more time slices.\n\n## Goals\n\nThis example shows how change detection methods can be applied conventionally to individual satellite scenes and in a modern way in cloud computing environments using [`rstac`](https://cran.r-project.org/package= rstac) [@rstac] and [`gdalcubes`](https://cran.r-project.org/package=gdalcubes) [@gdalcubes] or [`openeo`](https://cran.r-project.org/package=openeo) [@openeo]. In addition to classical supervised classification methods such as Maximum Likelihood and Random Forest, the [`bfast`](https://cran.r-project.org/package=bfast) [@bfast] is used, which includes an unsupervised method for detecting structural breaks in vegetation index time series.\n\nOther packages used in this tutorial include [`stars`](https://cran.r-project.org/package=stars) [@stars], [`tmap`](https://cran.r-project.org/package=tmap) [@tmap] and [mapview](https://cran.r-project.org/package=tmap)  for creating interactive maps, [`sf`](https://cran.r-project.org/package=sf) for processing vector data, and [`colorspace`](https://cran.r-project.org/package=colorspace) [@colorspace] for visualizations with accessible colors.\n\nThis study employs a variety of approaches to time series and difference analyses with different indices, using the Harz Mountains as a case study for the period between 2018 and 2023. The objective is to analyze or classify the data.\n\n## Information from satellite imagery\n\nUnprocessed satellite images are not necessarily informative.\nWhile our eyes can interpret a true-color image relatively conclusively and intuitively, a reliable and reproducible, i.e. scientifically sound, interpretation requires other approaches.\nA major advantage of typical image analysis methods over visual interpretation is the derivation of additional, so-called *invisible* information.\n\nTo obtain useful or meaningful information, e.g. about the land cover in an area, we have to analyze the data according to the question at hand. Probably the best known and most widely used approach is the supervised classification of image data into categories of interest.\n\nIn this unit, you will learn about the classification of satellite image data. This includes both data acquisition on the Copernicus portal and the various steps from digitising the training data to evaluating the quality of the classifications. \n\nWe will cover the following topics:\n\n* [Theoretical principles](cd-1.qmd#theoretical-principles) \n* [Case Study Harz Mountains](cd-1.qmd#change-detection-case-study-harz-mountains)\n   - [Preparing the work environment](cd-1.qmd#setting-up-the-work-environment)       - [Retrieving Sentinel and auxilliary data](cd-1.qmd#step-1-retrieving-sentinel-data)\n   - [Unsupervised classification](cd-1.qmd#step2-overview-unsupervised-classification-k-means-clustering) (k-means clustering)\n   - [Recording training areas](cd-1.qmd#step-3---generating-training-data).\n   - [Supervised classification]cd-1.qmd#step-4---supervised-classification) (Random Forest, Maximum Likelihood)\n   - [Estimating model quality](cd-1.qmd#step-5-estimation-model-quality)\n\n## Theoretical principles\n\nPlease note that all types of classification usually require extensive data pre-processing.\nThe focus is then on model building and quality assessment, which can be seen as the technical basis for classification, in order to finally derive the interpretation of the results in terms of content in the data post-processing.\n\nWe will go through this process step by step.\n\n### Unsupervised Classification - k-means clustering\n\nProbably the best-known unsupervised classification technique is K-means clustering, which is also referred to as the *\"simplest machine learning algorithm\"*.\n\nK-means clustering is a technique commonly used in satellite image classification to group pixels with similar spectral characteristics. Treating each pixel as an observation, the algorithm assigns pixels to clusters based on their spectral values, with each cluster having a mean (or centroid) that represents its central spectral signature. This results in the segmentation of the image into distinct regions (similar to Voronoi cells) corresponding to land cover types, such as water, vegetation or urban areas, facilitating further analysis. It is often used to obtain an initial overview of whether the raster data can be sufficiently separated in feature space.\n\n<a title=\"Chire, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:K-means_convergence.gif\"><img width=\"512\" alt=\"K-means convergence\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/512px-K-means_convergence.gif?20170530143526\"></a>\n\nFigure: Convergence of k-means clustering from an unfavorable starting position (two initial cluster centers are fairly close). [Chire](https://commons.wikimedia.org/wiki/User:Chire) \\[CC BY-SA 4.0\\] via wikipedia.org\n\n### Supervised classification\n\nIn supervised land cover classification, a model is derived from a limited amount of training land cover data that predicts land cover for the entire data set.\nThe land cover types are defined *a priori*, and the model attempts to predict these types based on the similarity between the characteristics of the training data and the rest of the data set.\n\n![](images/supervised_classification.jpg)\nClassifiers (e.g. the maximum likelihood classifier) or machine learning algorithms (such as Random Forest) use the training data to determine descriptive models that represent statistical signatures, classification trees or other functions.\nWithin the limits of the quality of the training data, such models are suitable and representative for making predictions for areas if the predictors from the model are available for the entire area.\n\nWe now want to predict the spatial characteristics of clear-felling/no forest using a maximum likelihood classification and random forest, and apply standard methods of random validation and model quality assessment.\n\nThe goal is to separate clearcuts from all other pixels and to quantify the differences between 2019 and 2020.\n\n#### Maximum Likelihood Classification\n\nMaximum likelihood classification assumes that the distribution of data for each class and in each channel is normally distributed.\nUnder this assumption, the probability that a particular pixel belongs to a particular class is calculated.\nSince the probabilities can also be specified as a threshold, without this restriction, *all* pixels are assigned regardless of how unlikely they are.\nEach pixel is assigned to the class that has the highest probability (i.e., the maximum probability).\n\n![](images/max.png)\n\n#### Random forest\n\nRandom forests can be used for both regression and classification tasks, with the latter being particularly relevant in environmental remote sensing.\nLike any machine learning method, the random forest model learns to recognize patterns and structures in the data itself.\nSince the random forest algorithm also requires training data, it is also a supervised learning method.\n![](images/Random_forest_diagram_complete.png)!\n\nFigure: Simplified illustration of data classification by random forest during training.\nVenkata Jagannath \\[CC BY-SA 4.0\\] via wikipedia.org\n\nA random forest algorithm learns from the data by creating random decision trees â€“ hence the name.\nFor classification tasks, the algorithm takes a suitable instance of a decision tree from the training data set and assigns the corresponding class to the pixel.\nThis is repeated with all available decision trees.\nFinally, the pixel is assigned to the class that has the most trees, according to the winner-takes-all principle.\n\nFrom a pragmatic point of view, classification tasks generally require the following steps:\n\n-   Creation of a comprehensive input data set that contains one or more raster layers. Selection of training areas, i.e. subsets of the input data set for which the land cover type is known to the remote sensing expert. Knowledge of the land cover can be obtained, for example, from one's own or third-party in situ observations, management information or other remote sensing products (e.g. high-resolution aerial photographs). Training a model using the training sites. For validation purposes, the training sites are often subdivided into one or more test and training sites to evaluate the performance of the model algorithm. Applying the trained model to the entire data set, i.e. predicting the land cover type based on the similarity of the data at each site to the class characteristics of the training data set.\n\n## Change detection case study: Harz Mountains\n\n\n\n### Setting up the work environment\n\nYou can either use the data saved from the previous exercise or define, download and edit a new area.\nHowever, the work environment is usually loaded first.\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\n# ---- 0 Projekt Setup ----\nrequire(\"pacman\")\n#remotes::install_github(\"zivankaraman/CDSE\")\n# packages installing if necessary and loading\npacman::p_load(mapview, mapedit, tmap, tmaptools, raster, terra, stars, gdalcubes, sf,webshot, dplyr,CDSE,webshot, downloader, tidyverse,RStoolbox,rprojroot, exactextractr, randomForest, ranger, e1071, caret, link2GI, rstac, OpenStreetMap,colorspace,ows4R,httr)\n#--- Switch to determine whether digitization is required. If set to FALSE, the\nroot_folder = find_rstudio_root_file()\n\nndvi.col = function(n) {\n  rev(sequential_hcl(n, \"Green-Yellow\"))\n}\n\nano.col = diverging_hcl(7, palette = \"Red-Green\",  register = \"rg\")\n```\n:::\n\n\nPlease add any missing or defective packages in the above setup script (if error messages occur).\nOn the basis of the available Sentinel data, the first step should be to identify suitable data sets for a surface classification.\n\n### Defining the Area of Interest\n\nPlease note to project to three different CRS is for this examples convenience and clarity and somewhat superfluous. Only the corner coordinates of the sections are required and not the complete geometries. However, it creates more clarity for the later process to already have the data needed in different projections.\\\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# download the Harz region from the UBA WFS server\n# nre_regions <- \"https://geodienste.bfn.de/ogc/wfs/gliederungen?\"\n# regions_client <- WFSClient$new(nre_regions, \n#                             serviceVersion = \"2.0.0\",)\n# regions_client$getFeatureTypes(pretty = TRUE)\n# \n# url <- parse_url(nre_regions)\n# url$query <- list(service = \"wfs\",\n#                   #version = \"2.0.0\", # optional\n#                   request = \"GetFeature\",\n#                   typename = \"Haupteinheiten\",\n#                   srsName = \"EPSG:4326\"\n#                   )\n# request <- build_url(url)\n# \n# nre_regions_sf <- read_sf(request)\n# harz = nre_regions_sf |> \n#   filter(NAME_ORD3 %in% c( \"Oberharz\")) \n# plot(harz)\n# harz_bbox = bb(harz,projection=4326)\n# harz_32632 =sf::st_transform(harz,crs = 32632)\n# harz_bbox_32632 = bb(harz_32632,projection=32632)\n\n# Download training data which is also used for the extend\nutils::download.file(url=\"https://github.com/gisma/gismaData/raw/master/geoinfo/train_areas_2019_2020.gpkg\",destfile=file.path(\"../data/train_areas_2019_2020.gpkg\"))\ntrain_areas_2019_2020 = st_read(file.path(\"../data/train_areas_2019_2020.gpkg\"))\n\n# mapping the extents and boundaries of the choosen geometries\n# tmap_mode(\"view\")\n# tmap_options(check.and.fix = TRUE) + \n#   tm_basemap(server = c(\"Esri.WorldGrayCanvas\", \"OpenStreetMap\", \"Esri.WorldTopoMap\",\"Esri.WorldImagery\")) +\n#   #tm_shape(harz) +   \n#   tm_polygons(alpha = 0.4)\n\n#--- Reading the data from the directories\n\n##--- This describes how to process the Corine land use and land cover dataset\n## The necessary file can also be downloaded from the repository\n## An account is required for the download https://land.copernicus.eu/pan-european/corine-land-cover\n## Therefore, download the data manually and unzip \n## U2018_CLC2018_V2020_20u1.tif into the data directory \n## Then continue\nif (!file.exists(file.path(root_folder,\"data/corine_harz.tif\"))){\n  corine = rast(file.path(\"../data/U2018_CLC2018_V2020_20u1.tif\"))\n  corine = terra::project(corine,\"EPSG:4326\" )\n  corine_harz = terra::crop(corine,vect(train_areas_2019_2020))\n  terra::writeRaster(corine_harz,file.path(root_folder,\"data/corine_harz.tif\"),overwrite=TRUE)\n  }\ncorine_harz = rast(file.path(root_folder,\"data/corine_harz.tif\"))\n# Create a forest mask from corine\n# Agro-forestry areas code=22, Broad-leaved forest code=23,\n# Coniferous forest code=24, Mixed forest code=25\nm <- c(-100, 22, 0,\n       22, 26, 1,\n       26, 500, 0)\nrclmat <- matrix(m, ncol=3, byrow=TRUE)\nharz_forest_mask <- classify(corine_harz, rclmat, include.lowest=TRUE)\n\n\nmapview(corine_harz)+mapview(train_areas_2019_2020,zcol=\"class\")+harz_forest_mask\n\n# create extents\ne <- ext(harz_forest_mask)\np <- as.polygons(e, crs=\"EPSG:4326\")\ncm=sf::st_as_sf(p)\n```\n:::\n\n## Step 1: Retrieving Sentinel data\n\n### Aternative 1:  Using `gdalcubes`\n\nSentinel-2 is currently the most important platform for Earth observation in all areas, but especially for climate change, land use and ecological issues at all levels, from the upper micro to the global scale.\n\nThere are two operational Sentinel-2 satellites: Sentinel-2A and Sentinel-2B, both in sun-synchronous polar orbits and 180 degrees out of phase. This arrangement allows them to cover the mid-latitudes with an orbital period of about 5 days.\n\nThe Sentinel-2 data are therefore predestined to record spatial and temporal changes on the Earth's surface (the forest was green in early summer, it has disappeared by late summer). They are ideal for timely studies before and after natural disasters, or for biomass balancing, etc.\n\n#### Cloud-Optimised GeoTIFFs (COGs)\n\nUnfortunately, the official [Sentinel-2 archives](https://scihub.copernicus.eu/dhus/#/home) are anything but user-friendly. Even with very convenient tools such as [`sen2r`](https://sen2r.ranghetti.info/) it is sometimes tedious to process them.Technically, the processed product levels are available for download pre-processed as L1C and L2A products in JP2K format. The preferred file format is JP2K, which is storage efficient but has to be downloaded in its entirety locally by the user, resulting in high access costs and huge local storage requirements. The cloud-optimised GeoTIFFs (COGs) allow only the areas of interest to be downloaded and are also much faster to process. However, this requires optimised cloud services and a technically different access logic than in the processing chains used so far.\n\n#### SpatioTemporal Asset Catalog (STAC)\n\nThe [Spatial-Temporal Asset Catalogue] (https://stacspec.org/) (STAC) provides a common language for simplified indexing and discovery of geospatial data. A \"Spatio-Temporal Asset\" is a file that contains information in a specific space and time.\n\nThis approach allows any provider of spatio-temporal data (imagery, SAR, point clouds, data cubes, full motion video, etc.) to provide Spatio-Temporal Asset Catalogues (STAC) for their data. STAC focuses on an easy-to-implement standard that organisations can use to make their data available in a durable and reliable way.\n\n[Element84](https://www.element84.com/) has provided a public API called Earth-search, a central search catalogue for all public AWS datasets using STAC (including the new Sentinel-2 COGs), which contains more than 11.4 million Sentinel-2 scenes worldwide as of 1 November 2017.\n\nOne major challenge is the fact that most of the earth surface related remote sensing activities are heavily *\"disturbed\"* by the atmosphere, especially by clouds. So to find cloud free satellite imagery is a common and cumbersome task. This task is supported by the `rstac` package which provides a convenient tool to find and filter adequate Sentinel-2 images out of the COG data storage. However, to address the AOI we need to provide the extend via the `bbox` argument of the corresponding function `stac_search()`. So first we need to derive and transform the required bounding box to WGS84 geo-coordinates, easily done with the `sf` functions `st_bbox()` and `st_transform()`. In addition we adapt the projection of the referencing vector objects to all other later projection needs.\n\n#### Querying images with `rstac`\n\nUsing the `rstac` package, we first request all available images from 2018 to 208 that intersect with our region of interest. Here, since the polygon has WGS84 as CRS, we do **not** need to transform the bounding box before using the `stac_search()` function.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r stac2}}\n#| eval: false\n# search the data stack for the given period and area\nlibrary(rstac)\ns = stac(\"https://earth-search.aws.element84.com/v0\")\n\nitems <- s |>\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox= as.vector(harz_bbox), \n              datetime = c(\"2019-06-01/2021-09-01\"),\n              limit = 1000) |>\n  post_request() \nitems\n```\n````\n:::\n\n\nThis gives us 350 matching images recorded between Januar 2019 and December 2023.\n\n#### Creating a monthly Sentinel-2 data cube\n\nTo obtain a Sentinel data cube, a gdalcube image collection must be created from the STAC query result. To do this, the asset names must be explicitly named in order to apply the SCL channel with the quality characteristics per pixel (classification as clouds, cloud shadows, etc.). In this query, a filter is set to cloud cover <= 50%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gdalcubes)\ns2_collection <- stac_image_collection(items$features,\n                                      asset_names = c(\"B04\",\"B08\",\"SCL\"),\n#c(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\", \"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"SCL\"), \n                                      property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 5}) \ns2_collection\n```\n:::\n\n\nThe result is 118 images, i.e. approx. 1.8 images per month, from which we can now create a data cube. To do this, we use the UTM bounding box of our polygon as a spatial boundary, a spatial resolution of 10 metres, a bilinear spatial interpolation (useful for the spatially lower-resolution sentinel channels) and calculate monthly median values for all pixel values from the available images of a month. In addition, we add a buffer (b) on each side of the cube.\n\n\n\n\n\nThe *gdalcube image collection* can be considered as a proxy structure object which will be applied on the COGs.\n\n\n\n::: {.cell messages='false' warnings='false'}\n\n```{.r .cell-code}\nv = cube_view(srs = \"EPSG:32632\", \n              dx = 10, \n              dy = 10, \n              dt = \"P1M\",  \n              aggregation = \"median\", \n              extent = list(t0 = \"2019-06-01\",\n                            t1 = \"2021-09-01\", \n                            left = st_bbox(harz_32632)[\"xmin\"] , \n                            right = st_bbox(harz_32632)[\"xmax\"] ,\n                            bottom = st_bbox(harz_32632)[\"ymin\"] , \n                            top = st_bbox(harz_32632)[\"ymax\"] ),\n              resampling = \"bilinear\")\nv\n```\n:::\n\n\nNext we create a data cube, subset the red and near infrared bands and crop by our polygon, which simply sets pixel values outside the polygon to NA. We then save the data cube as a single netCDF file. Note that this is not necessary, but saving intermediate results sometimes makes debugging easier, especially if the methods applied afterwards are computationally intensive.\n\n\nOnly calling a final *action* will start the processing on the COG-Server. In this case 'write_ncdf'.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## ##\n#| \n# we \"download\" the data and write it t a netcdf file\n  s2.mask = image_mask(\"SCL\", values = c(3,8,9))\n  gdalcubes_options(parallel = 16, \n                    ncdf_compression_level = 5)\n  raster_cube(s2_collection, v, mask = s2.mask) |>\n    write_ncdf(\"data/harz_2019_2021_kndvi.nc\",overwrite=TRUE)\n```\n:::\n\n\n#### kNDVI\n\nBelow, we derive mean monthly [kNDVI](https://advances.sciencemag.org/content/7/9/eabc7447) values over all pixel time series.\n\n\n::: {.cell messages='false' warnings='false'}\n\n```{.r .cell-code}\nncdf_cube(\"data/harz_2019_2021_kndvi.nc\") |>\n    apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |>\n  reduce_time(\"mean(kNDVI)\") |>\n  plot(key.pos = 1,  col = ndvi.col, nbreaks = 12)\n```\n:::\n\n\n\n### Alternative 2: Copernicus Data Space Ecosystem API Wrapper CDSE\nThe [CDSE package](https://zivankaraman.github.io/CDSE/articles/CDSE.html) provides another simple way to get Sentinel/Copernicus data sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#------------\n# NOTE: You must create an Copernicus account and provide the token credtials have a look at:\n#       https://zivankaraman.github.io/CDSE/articles/BeforeYouStart.html#accessing-cdse-data-and-services\n#------------\n\nid <- Sys.getenv(\"CDSE_ID\")\nsecret <- Sys.getenv(\"CDSE_SECRET\")\nOAuthClient <- GetOAuthClient(id = id, secret = secret)\ncollections <- GetCollections(as_data_frame = TRUE)\ncollections\n\n\nimages <- SearchCatalog(bbox = st_bbox(train_areas_2019_2020), from = \"2018-05-01\", to = \"2022-12-31\", \n    collection = \"sentinel-2-l2a\", with_geometry = TRUE, client = OAuthClient)\n\nimages\n\nsummary(images$areaCoverage)\n\n# best 30 days without clouds\nday <- images[order(images$tileCloudCover), ]$acquisitionDate[1:30]\n\n# read specific processing scripts\nscript_file_raw = system.file(\"scripts\", \"RawBands.js\", package = \"CDSE\")\nscript_file_savi = \"savi.js\"\nscript_file_kndvi = \"kndvi.js\"\nscript_file_evi = \"evi.js\"\n\n# first day 2018_07_01\nraw_2018_07_01 = GetImage(bbox = st_bbox(train_areas_2019_2020), \n                          time_range = day[12], \n                          script = script_file_raw, \n                          collection = \"sentinel-2-l2a\", \n                          format = \"image/tiff\", \n                          mosaicking_order = \"leastCC\",\n                          resolution = 10, \n                          mask = TRUE, \n                          buffer = 0.01, \n                          client = OAuthClient)\nnames(raw_2018_07_01) = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")\nterra::plot(raw_2018_07_01, main = paste(names(raw_2018_07_01), day[12]), cex.main = 0.75)\n\nkndvi_2018_07_01 <- GetImage(bbox = st_bbox(train_areas_2019_2020),\n                             time_range = day[12], \n                             script = script_file_kndvi, \n                             collection = \"sentinel-2-l2a\", \n                             format = \"image/tiff\", \n                             mosaicking_order = \"leastCC\", \n                             resolution = 10, \n                             mask = TRUE, \n                             buffer = 0.01, \n                             client = OAuthClient)\nnames(kndvi_2018_07_01 )[1] = c(\"kNDVI\")\nterra::plot(kndvi_2018_07_01[[1]] , main = paste(names(kndvi_2018_07_01 ), day[12]), cex.main = 0.75)\n\nsavi_2018_07_01 <- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                            time_range = day[12], \n                            script = script_file_savi, \n                            collection = \"sentinel-2-l2a\", \n                            format = \"image/tiff\", \n                            mosaicking_order = \"leastCC\", \n                            resolution = 10,\n                            mask = TRUE, \n                            buffer = 0.01, \n                            client = OAuthClient)\nnames(savi_2018_07_01)[1] = c( \"SAVI\")\nterra::plot(savi_2018_07_01[[1]], main = paste(\"SAVI\", day[12]), cex.main = 0.75)\n\nevi_2018_07_01 <- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                           time_range = day[12], \n                           script = script_file_evi, \n                           collection = \"sentinel-2-l2a\", \n                           format = \"image/tiff\", \n                           mosaicking_order = \"leastCC\", \n                           resolution = 10, \n                           mask = TRUE, \n                           buffer = 0.01, \n                           client = OAuthClient)\nnames(evi_2018_07_01)[1] = c( \"EVI\")\nterra::plot(evi_2018_07_01[[1]], main = paste(\"EVI\",day[12]), cex.main = 0.75)\n\npred_stack_2018 = c(raw_2018_07_01,evi_2018_07_01[[1]],kndvi_2018_07_01[[1]],savi_2018_07_01[[1]])\n\n## second day 2022_06_23\nraw_2022_06_23 = GetImage(bbox = st_bbox(train_areas_2019_2020), \n                          time_range = day[1], \n                          script = script_file_raw, \n                          collection = \"sentinel-2-l2a\", \n                          format = \"image/tiff\", \n                          mosaicking_order = \"leastCC\", \n                          resolution = 10, \n                          mask = TRUE, \n                          buffer = 0.01, \n                          client = OAuthClient)\nnames(raw_2022_06_23) = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")\nterra::plot(raw_2022_06_23, main = paste(names(raw_2022_06_23), day[1]), cex.main = 0.75)\n\nkndvi_2022_06_23 <- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                             time_range = day[1], \n                             script = script_file_kndvi, \n                             collection = \"sentinel-2-l2a\", \n                             format = \"image/tiff\",\n                             mosaicking_order = \"leastCC\", \n                             resolution = 10, \n                             mask = TRUE, \n                             buffer = 0.01, \n                             client = OAuthClient)\nnames(kndvi_2022_06_23)[1] = c(\"kNDVI\")\nterra::plot(kndvi_2022_06_23[[1]] , main = paste(names(kndvi_2022_06_23 ), day[1]), cex.main = 0.75)\n\nsavi_2022_06_23 <- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                            time_range = day[1], \n                            script = script_file_savi, \n                            collection = \"sentinel-2-l2a\", \n                            format = \"image/tiff\", \n                            mosaicking_order = \"leastCC\", \n                            resolution = 10,\n                            mask = TRUE, \n                            buffer = 0.01, \n                            client = OAuthClient)\nnames(savi_2022_06_23)[1] = c( \"SAVI\")\nterra::plot(savi_2022_06_23[[1]], main = paste(\"SAVI\",day[1]), cex.main = 0.75)\n\nevi_2022_06_23 <- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                           time_range = day[1], \n                           script = script_file_evi, \n                           collection = \"sentinel-2-l2a\", \n                           format = \"image/tiff\",\n                           mosaicking_order = \"leastCC\",\n                           resolution = 10, \n                           mask = TRUE, \n                           buffer = 0.01, \n                           client = OAuthClient)\nnames(evi_2022_06_23)[1] = c( \"EVI\")\nterra::plot(evi_2022_06_23[[1]], main = paste(\"EVI\",day[1]), cex.main = 0.75)\n\npred_stack_2022 = c(raw_2022_06_23,evi_2022_06_23[[1]],kndvi_2022_06_23[[1]],savi_2022_06_23[[1]])\n\nterra::writeRaster(pred_stack_2018,file.path(root_folder,\"data/pred_stack_2018.tif\"),overwrite=TRUE)\nterra::writeRaster(pred_stack_2022,file.path(root_folder,\"data/pred_stack_2022.tif\"),overwrite=TRUE)\n```\n:::\n\n\n\n\n\n## Step2: Overview â€“ Unsupervised classification \n\n### k-means clustering\n\nIn our example (applied to 5 classes and executed with the function `unsuperClass` from the `RStoolbox` package), this looks as follows.\nThe cluster algorithm can achieve a fairly acceptable separation of the clearings/bald spots with 5 clusters, which makes a classification seem promising.\nAlso experiment with other cluster settings and discuss the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## k-means Ã¼ber RStoolbox\n\n#\"EVI\"   \"kNDVI\" \"SAVI\"\nprediction_kmeans_2018 = RStoolbox::unsuperClass(pred_stack_2018, \n                                                 nClasses = 5,\n                                                 norm = TRUE, \n                                                 algorithm = \"MacQueen\")\n# Klassifikation\nplot(prediction_kmeans_2018$map)\n\nprediction_kmeans_2022 = RStoolbox::unsuperClass(pred_stack_2022, \n                                                 nClasses = 5,norm = TRUE, \n                                                 algorithm = \"MacQueen\")\nplot(prediction_kmeans_2022$map)\n```\n:::\n\n\n![](images/kmeans-2018.png) ![](images/kmeans-2022.png)\n\n### bfast:  Spatial identification of magnitudes and time periods of kNDVI changes\n\nTo apply a more complex time series method such as *Breaks For Additive Seasonal and Trend* (BFAST), which is often used for land cover changes (e.g. [@bfast_rs2020]), the `bfastmonitor()` function in `gdalcubes` can be used, the data cube operations below allow you to provide custom user-defined R functions instead of string expressions that translate to built-in reducers.\n\nIn our example, `bfastmonitor` returns *change date* and *change magnitude* values per time series so we can use `reduce_time()`. The script below:\n1. calculates the [kNDVI](https://advances.sciencemag.org/content/7/9/eabc7447), \n1. applies `bfastmonitor()`, and properly handles errors e.g. due to missing data with `tryCatch()`, and \n1. finally writes out the resulting change dates and magnitudes of change for all pixels of the time series as a netCDF file. \n\nThe results shows the changes starting at 7/2019 until 10/2021.\n\n\n::: {.cell messages='false' warnings='false'}\n\n```{.r .cell-code}\nfigtrim <- function(path) {\n  img <- magick::image_trim(magick::image_read(path))\n  magick::image_write(img, path)\n  path\n}\n\ngdalcubes_options(parallel = 12)\n\n## start analysis\nsystem.time(\n  ncdf_cube(file.path(root_folder,\"data/harz_2019_2021_kndvi.nc\")) |>\n    reduce_time(names = c(\"change_date\", \"change_magnitude\", \"kndvi\"), \n                FUN = function(x) {\n                  kndvi = tanh(((x['B08',]-x['B04',])/(x['B08',]+x['B04',]))^2)\n                  if (all(is.na(kndvi))) {\n                    return(c(NA,NA))\n                    }\n                  kndvi_ts = ts(kndvi, start = c(2019, 1), frequency = 12)\n                  library(bfast)\n                  tryCatch({\n                    result = bfastmonitor(kndvi_ts, \n                                          start = c(2020,1), \n                                          history = \"all\", \n                                          level = 0.01)\n                    return(c(result$breakpoint, result$magnitude))\n                    }, error = function(x) {return(c(NA,NA))\n                      })\n                  }) |>\n    write_ncdf(file.path(root_folder,\"data/bfast_results.nc\"),overwrite = TRUE))\n```\n:::\n\n\nNow we can use the netCDF file and map the results with any preferred visualisation tool. In this case `tmap`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plotting it from the local ncdf  \ntmap_mode(\"view\")\ngdalcubes::ncdf_cube(file.path(root_folder,\"data/bfast_results.nc\")) |>\n  stars::st_as_stars() -> x\n\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[1]) + \n  tm_raster(n = 6)  +\n  tm_layout(\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid()\n\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[2])  + tm_raster() +\n  tm_layout(legend.title.size = 1,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            legend.text.size = 0.4,\n            legend.outside = TRUE) +\n  tm_grid()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# plotting it from the local ncdf  \ntmap_mode(\"view\")\n\n# Period of Change Map\ngdalcubes::ncdf_cube(file.path(root_folder,\"data/bfast_results.nc\")) |>\n  stars::st_as_stars() -> x\n\ntmap::tmap_save(tm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[1] ) + \n  tm_raster(n = 6)  +\n  tm_layout(legend.title.size = 1,\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid(), file = \"m1.html\" ) \n\n# Magnitude Change Map \ntmap::tmap_save(tm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[2])  + tm_raster() +\n  tm_layout(legend.title.size = 1,\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid(),file = \"m2.html\" ) \n```\n:::\n\n\n\n<iframe valign=\"center\" src=\"m2.html\" width=\"640\" height=\"800\" frameborder=\"0\">\n</iframe>\n<figcaption>*Magnitude Change Map *</figcaption>\n\n<p>\n\n<iframe valign=\"center\" src=\"m1.html\" width=\"640\" height=\"800\" frameborder=\"0\">\n</iframe>\n<figcaption>*Period of Change Map *</figcaption>\n\n\n\n## Step 3 - Generating training data\n\nFor a supervised classification, we need data that indicates which surface class defined areas of the satellite image belong to.\nThis data is referred to as training data and is very often obtained by manual digitization.\nThis can be done quite comfortably in RStudio if only a few training areas have to be digitized quickly and effectively.\n\nWe assume that we want to classify two types of land cover: *clearcut* and *other*.\n\nFor larger tasks, it makes sense to use the convenient method described in the current QGIS  documentation, for example in the [digitizing tutorial](https://docs.qgis.org/3.34/en/docs/training_manual/create_vector_data/create_new_vector.html#basic-ty-digitizing-polygons).\n\n[Digitizing training data in R](digitize.qmd){.btn .btn-outline-primary .btn role=\"button\" data-toggle=\"tooltip\" title=\"Digitizing training data using mapedit\"}\n\nAssuming we have digitized trainingdata using either QGIS or R We need now to extract the values according to the assigned areas: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_stack_2018 = rast(file.path(root_folder,\"data/pred_stack_2018.tif\"))\npred_stack_2022 = rast(file.path(root_folder,\"data/pred_stack_2022.tif\"))\n# use the provided training data set\n# Extract the training data for the digitized areas\ntDF_2019 = exactextractr::exact_extract(pred_stack_2018, filter(train_areas_2019_2020,year==2019), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\ntDF_2020 = exactextractr::exact_extract(pred_stack_2022, filter(train_areas_2019_2020,year==2020), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n# again, copy together into a file\ntDF_2019 = dplyr::bind_rows(tDF_2019)\ntDF_2019$year = 2019\ntDF_2020 = dplyr::bind_rows(tDF_2020)\ntDF_2020$year = 2020\n# Delete any rows that contain NA (no data) values\ntDF_2019 = tDF_2019[complete.cases(tDF_2019) ,]\ntDF_2020 = tDF_2020[complete.cases(tDF_2020) ,]\n\ntDF= rbind(tDF_2019,tDF_2020)\n\n# check the extracted data\nsummary(tDF)\n\n# Save as R internal data format\n# is stored in the repo and can therefore be loaded (line below)\nsaveRDS(tDF, paste0(\"../data/tDF_2018_2022.rds\"))\n```\n:::\n\n\n## Step 4 - supervised classification\n\nClassifiers (e.g. the maximum likelihood classifier) or machine learning algorithms (such as Random Forest) use the training data to determine descriptive models that represent statistical signatures, classification trees or other functions. Within the limits of the quality of the training data, such models are suitable and representative for making predictions for areas if the predictors from the model are available for the entire area.\n\nWe now want to predict the spatial characteristics of clear-felling/no forest using a maximum likelihood classification and random forest, and apply standard methods of random validation and model quality assessment.\n\nThe goal is to separate clearcuts from all other pixels and to quantify the differences between 2019 and 2020.\n\n### Maximum Likelihood Classification\n\n\n\nSince the maximum likelihood algorithm requires training data, it is a supervised learning method. This means that we, as users, have to provide the algorithm with data that conveys knowledge about the classes to be predicted. This data is then divided into training and test data.\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\n# ---- Maximum Likelihood Classification ----\n\n## Here the caret utility package is used\n# Setting a \"seed\" enables reproducible randomness\nset.seed(123)\ntDF = readRDS( file.path(root_folder,\"data/tDF_2018_2022.rds\"))\n# Randomly draw 15% of the data (training/test)\nidx = createDataPartition(tDF$class,list = FALSE,p = 0.05)\ntrainDat = tDF[idx,]\ntestDat = tDF[-idx,]\n\n# Response variable (= \"class\" column) must be of the \"factor\" data type\ntrainDat$class <- as.factor(trainDat$class)\ntestDat$class <- as.factor(testDat$class)\n\n\n# superClass() function from the RSToolbox package requires the table to be converted into the\n# required (old) SpatialdataPoint object\n\nsp_trainDat = trainDat\nsp_testDat = testDat \nsp::coordinates(sp_trainDat) = ~x+y\nsp::coordinates(sp_testDat) = ~x+y\ncrs(sp_trainDat) = crs(pred_stack_2018)\ncrs(sp_testDat) = crs(pred_stack_2018)\n\n\n# superClass method \"mlc\" trains the model and then classifies it\nprediction_mlc_2018 <- superClass(pred_stack_2018, trainData = sp_trainDat[,1:16],valData = sp_testDat[,1:16], responseCol = \"class\", model = \"mlc\", tuneLength = 1, trainPartition = 0.3,verbose = TRUE)\n\nprediction_mlc_2022 <- superClass(pred_stack_2022, trainData = sp_trainDat[,1:16],valData = sp_testDat[,1:16], responseCol = \"class\",model = \"mlc\", tuneLength = 1, trainPartition = 0.3,verbose = TRUE)\n\nterra::writeRaster(prediction_mlc_2018, \"../data/prediction_mlc_2018.tif\",overwrite=TRUE)\nterra::writeRaster(prediction_mlc_2022, \"../data/prediction_mlc_2022.tif\",overwrite=TRUE)\n```\n:::\n\n\n### Random forest\n\nA simplified version of the workflow proposed by Max Kuhn [@caret] and improved by Hanna Meyer et al. [@cast] is used for the random forest classification. For further understanding visit The [CAST documentation](https://hannameyer.github.io/CAST/)\n\n\n\n\n\n### Prediction on the original data\n\nNow we are ready to apply the verified model to our data set. In remote sensing, this is usually called classification.\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\n# Classification (also known as prediction)\nprediction_rf_2018  = raster::predict(pred_stack_2018 ,rf_model)\nprediction_rf_2022  = raster::predict(pred_stack_2022 ,rf_model)\nsaveRDS(prediction_rf_2018, file.path(root_folder,\"data/prediction_rf_2018.rds\"))\nsaveRDS(prediction_rf_2022, file.path(root_folder,\"data/prediction_rf_2022.rds\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## ##\nprediction_rf_2018 = readRDS(file.path(root_folder,\"data/prediction_rf_2018.rds\"))\nprediction_rf_2022 = readRDS(file.path(root_folder,\"data/prediction_rf_2022.rds\"))\nprediction_mlc_2018 = readRDS(file.path(root_folder,\"data/prediction_mlc_2018.rds\"))\nprediction_mlc_2022 = readRDS(file.path(root_folder,\"data/prediction_mlc_2022.rds\"))\n\n## ---- Visualisierung mit mapview ----\nmask = resample(harz_forest_mask,pred_stack_2022)\nmapview::mapshot(\n  mapview(mask*prediction_rf_2018 , alpha.regions = 0.5, maxpixels =  1693870,\n          col.regions = mapviewPalette(\"mapviewRasterColors\"),at = seq(0, 2, 1), legend = TRUE) +\n  mapview(mask*prediction_rf_2022, alpha.regions = 0.5, maxpixels =  1693870,\n          col.regions = mapviewPalette(\"mapviewRasterColors\"),at = seq(0, 2, 1), legend = FALSE) ,url = \"compare-class1.html\" )\nplot(prediction_mlc_2018$map)\nplot(prediction_mlc_2022$map)\n```\n:::\n\n\n<iframe valign=\"center\" src=\"compare-class1.html\" width=\"640\" height=\"960\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">\n\n</iframe>\n\n\nA visual comparison shows that the Random Forest and Maximum Likelihood classifications provide results of comparable quality. But does this impression stand up to quantitative analysis?\n\n### Step 5: Estimation model quality\n\nThe test data are now used for the independent quality check of the model. A confusion matrix indicates how accurately the model predicts the correct classes. The main diagonal of the matrix indicates the cases in which the model applies. In our classification of only two classes, however, a special case applies: [evaluation of a binary classifier](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers). Detailed explanations for the function used here can be found in the [caret help](https://topepo.github.io/caret/measuring-performance.html#measures-for-predicted-classes).\n\nThe main statements about model quality are:\n\n-   *'Positive' Class* = **clearcut**: is measured with the sensitivity (*true positive rate*), which indicates the probability that a positive object is correctly classified as positive.\n-   *'Negative Class'* = **other**: is measured with the specificity (*true negative rate*) and indicates the probability that a negative object is correctly classified as negative.\n-   *Positive and negative predictive values* indicate the actual performance for *clearcut* and *other*. They are corrected for the actual frequency distribution and are a measure of the precision and performance of the model with regard to the respective classes.\n\nDespite the high values, we see that the *clearcut* class drops off significantly here. This can certainly be taken as an indication of the need to improve the classification.\n\nOverall, however, the model can be considered good.\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\n## ##\n# ----Calculation of the confusion matrix  ----\ncm_rf <- confusionMatrix(data = predict(rf_model, newdata = testDat), testDat$class)\ncm_rf\n```\n:::\n\n\n\n## Further support\n\nConsider the following resources as examples of how a specific conceptual and technical approach to answering a question can be *\"crystallized\"* step by step from the wide range of instructions available on the internet. After a lot of research and critical cross-checking, a *\"state of research\"* that is currently considered to be certain within the scientific community can be identified, which can be regarded as a sufficient basis for good scientific practice.\n\nWork/read through the following selection of blogs and guides, even for practice purposes.\n\n-   [The core of GIScience](http://www.charim.net/sites/default/files/handbook/datamanagement/3/3.3/The%20core%20of%20GIScience%2C%20a%20system%20-based%20approach.pdf) [Download](ftp://ftp.itc.nl/pub/ders/CoreBook2014_metadata.pdf) The editors Tolpekin & Stein 2012 are providing an excellent insight into GI concepts.\n-   Robert J. Hijmans [rspatial - supervised classification](https://rspatial.org/raster/rs/5-supclassification.html)\n-   Ivan Lizarazo [RPubs Tutorial](https://rpubs.com/ials2un/rf_landcover)\n-   Sydney Goldstein [blog](https://urbanspatial.github.io/classifying_satellite_imagery_in_R/)\n-   JoÃ£o GonÃ§alves [supervised classification](https://www.r-exercises.com/2018/03/07/advanced-techniques-with-raster-data-part-2-supervised-classification/)\n-   Valentin Stefan [pixel-based supervised classification](https://valentinitnelav.github.io/satellite-image-classification-r/)\n\nIn the articles, you will always find both technical instructions and conceptual or specific technical questions and solutions. They are by no means a substitute for specialized scientific knowledge. But they show how technical and conceptual understanding can be developed step by step and, by \"replicating\" and applying, support the skills needed to approach questions independently.\n\nI would like to explicitly quote Valentin Stefan, the author of the blog post [pixel-based supervised classification](https://valentinitnelav.github.io/satellite-image-classification-r/):\n\n::: {.callout-note appearance=\"minimal\"}\n*\"\\[...\\] Consider this content a blog post and nothing more. It does not claim to be an exhaustive exercise or a substitute for your critical thinking \\[...\\].\"* :::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}