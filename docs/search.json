[
  {
    "objectID": "worksheets/ws-07.html",
    "href": "worksheets/ws-07.html",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "",
    "text": "Achieving reliable monitoring hinges on the reproducibility of both field data collection and subsequent analyses. In the context of spatial micro-scale analysis and modeling, the precise geo-object locations play a central role, presenting an additional challenge when considering the 3D structure. Incorporating the temporal aspect, such as accurately recording repeated flights, further elevates these demands. Whether utilizing low-cost or professional UAV systems, these factors must be acknowledged to enable meaningful analysis or modeling. While LiDAR systems are the preferred choice for acquiring forest structure, their high cost, both in acquisition and processing, poses challenges. A feasible alternative lies in off-the-shelf UAV systems with integrated image acquisition. These systems allow for the generation of not only orthorectified planar image data but also quasi-three-dimensional point clouds through photo reconstruction and image processing techniques, providing elevation information for each coordinate."
  },
  {
    "objectID": "worksheets/ws-07.html#aims-and-goals",
    "href": "worksheets/ws-07.html#aims-and-goals",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "Aims and Goals",
    "text": "Aims and Goals\nReproducibility in Data Acquisition and Analysis:\nAims: Establish a framework for reproducible and automated data acquisition and analysis, crucial for robust monitoring.\nGoals: Ensure that the 3D parameters and forest ecological metrics are calculated in a fully reproducible manner, enabling accurate spatial micro-scale modeling.\nIntegration of Temporal and Spatial Aspects:\nAims: Address the challenges posed by the 3D structure and temporal aspects in UAV-based monitoring. Goals: Achieve meaningful analysis and modeling by acknowledging and incorporating precise geo-object locations and accurate recording of repeated flights.\nCost-Effective Alternatives for Forest Structure Acquisition:\nAims: Explore alternatives to expensive LiDAR systems for acquiring forest structure. Goals: Utilize off-the-shelf UAV systems with integrated image acquisition to generate quasi-three-dimensional point clouds, providing elevation information in a cost-effective manner.\nAutomated and Reproducible Workflows:\nAims: Develop automated workflows that streamline data processing and analysis. Goals: Enable a robust and efficient classification method using machine learning object-based image analysis (OBIA), ensuring complex classification goals can be achieved with minimal technical expertise."
  },
  {
    "objectID": "worksheets/ws-07.html#project-repository",
    "href": "worksheets/ws-07.html#project-repository",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "Project Repository",
    "text": "Project Repository\n\nforenius-pp repository for additional functions"
  },
  {
    "objectID": "worksheets/ws-05.html",
    "href": "worksheets/ws-05.html",
    "title": "Worksheet 4: Pattern-based spatial analysis",
    "section": "",
    "text": "Read the study area polygon from the exdata/harz_borders.gpkg file using the read_sf() function from the sf package.\nRead the land cover raster data for Europe from the file exdata/lc_europe.tif using the function rast() from the package terra. Visualise both datasets.\nCrop and mask the raster to the polygon boundaries. Visualise the results.\nCompute a spatial signature for the study area. Can you understand its meaning?\nFind out which areas of the Europe raster are most similar to the study area (this may take a minute or so). Try different window sizes (e.g. 200 or 500).\n\n\n\n\n\n\n\n\n\n\nDownload Corine data from 2012 and 2018.\nDefine an appropriate scale for the assessment of forest structure changes at European level\nIdentify the areas with the smallest and largest changes\nProvide a short interpretation\nHow’s the Harz ranked?",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 4: Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "worksheets/ws-05.html#exercises-ws-4",
    "href": "worksheets/ws-05.html#exercises-ws-4",
    "title": "Worksheet 4: Pattern-based spatial analysis",
    "section": "",
    "text": "Read the study area polygon from the exdata/harz_borders.gpkg file using the read_sf() function from the sf package.\nRead the land cover raster data for Europe from the file exdata/lc_europe.tif using the function rast() from the package terra. Visualise both datasets.\nCrop and mask the raster to the polygon boundaries. Visualise the results.\nCompute a spatial signature for the study area. Can you understand its meaning?\nFind out which areas of the Europe raster are most similar to the study area (this may take a minute or so). Try different window sizes (e.g. 200 or 500).\n\n\n\n\n\n\n\n\n\n\nDownload Corine data from 2012 and 2018.\nDefine an appropriate scale for the assessment of forest structure changes at European level\nIdentify the areas with the smallest and largest changes\nProvide a short interpretation\nHow’s the Harz ranked?",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 4: Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "worksheets/ws-03.html",
    "href": "worksheets/ws-03.html",
    "title": "Worksheet 2: Landscape metrics part 1",
    "section": "",
    "text": "The marginal entropy and relative mutual information can be calculated using the landscapemetrics package’s functions: lsm_l_ent() and lsm_l_relmutinf(). Calculate both of these metrics for the exdata/lc_small.tif raster.\nRead the exdata/lc_europe.tif raster using rast() from the terra package and the exdata/polygons.gpkg vector data using the read_sf() function from the sf package. Calculate the marginal entropy and relative mutual information for each polygon using the sample_lsm() function.\nJoin the calculated values with the polygons (see https://r-spatialecology.github.io/landscapemetrics/articles/irregular_areas.html for more details).\nCalculate SHDI and AI for the polygons. Compare the values of SHDI and AI with the marginal entropy and relative mutual information (e.g., using a scatterplot or by calculating the correlation coefficient). Are the results similar?\n(Extra) Create your own polygonal grid using st_make_grid() function from the sf package for the area from the exdata/polygons.gpkg file. Calculate the marginal entropy and relative mutual information for each square using the sample_lsm() function. Visualize the results.",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 2: Landscape metrics part 1"
    ]
  },
  {
    "objectID": "worksheets/ws-03.html#exercises-ws-2",
    "href": "worksheets/ws-03.html#exercises-ws-2",
    "title": "Worksheet 2: Landscape metrics part 1",
    "section": "",
    "text": "The marginal entropy and relative mutual information can be calculated using the landscapemetrics package’s functions: lsm_l_ent() and lsm_l_relmutinf(). Calculate both of these metrics for the exdata/lc_small.tif raster.\nRead the exdata/lc_europe.tif raster using rast() from the terra package and the exdata/polygons.gpkg vector data using the read_sf() function from the sf package. Calculate the marginal entropy and relative mutual information for each polygon using the sample_lsm() function.\nJoin the calculated values with the polygons (see https://r-spatialecology.github.io/landscapemetrics/articles/irregular_areas.html for more details).\nCalculate SHDI and AI for the polygons. Compare the values of SHDI and AI with the marginal entropy and relative mutual information (e.g., using a scatterplot or by calculating the correlation coefficient). Are the results similar?\n(Extra) Create your own polygonal grid using st_make_grid() function from the sf package for the area from the exdata/polygons.gpkg file. Calculate the marginal entropy and relative mutual information for each square using the sample_lsm() function. Visualize the results.",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 2: Landscape metrics part 1"
    ]
  },
  {
    "objectID": "worksheets/ws-01.html",
    "href": "worksheets/ws-01.html",
    "title": "Prerequisies",
    "section": "",
    "text": "Participants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Prerequisies"
    ]
  },
  {
    "objectID": "worksheets/ws-01.html#prerequisites",
    "href": "worksheets/ws-01.html#prerequisites",
    "title": "Prerequisies",
    "section": "",
    "text": "Participants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Prerequisies"
    ]
  },
  {
    "objectID": "worksheets/ws-01.html#exercises",
    "href": "worksheets/ws-01.html#exercises",
    "title": "Prerequisies",
    "section": "Exercises",
    "text": "Exercises\nThe slides are accompanied by practical exercises. The best way to get them is to download the exercises repository as a ZIP file from https://github.com/gisma-courses/LV-19-d19-006-24/blob/main/assets/exercises-session1.zip and unpack it on your computer. Then, you can open the .Rproj file and start working on the exercises in RStudio.",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Prerequisies"
    ]
  },
  {
    "objectID": "templates/slides-template.html#section",
    "href": "templates/slides-template.html#section",
    "title": "YOUR TITLE",
    "section": "",
    "text": "This is a slide with a background image"
  },
  {
    "objectID": "templates/slides-template.html#bullets",
    "href": "templates/slides-template.html#bullets",
    "title": "YOUR TITLE",
    "section": "Bullets",
    "text": "Bullets\nRemove the incremental ::: bracketed div for plain lists\n\n\nthe quick\nbrown fox\njumps over\nthe lazy dog"
  },
  {
    "objectID": "templates/slides-template.html#columns",
    "href": "templates/slides-template.html#columns",
    "title": "YOUR TITLE",
    "section": "Columns",
    "text": "Columns\n\n\nSome text on the left of the slide\n\nSome text on the right of the slide"
  },
  {
    "objectID": "templates/slides-template.html#smaller-slide",
    "href": "templates/slides-template.html#smaller-slide",
    "title": "YOUR TITLE",
    "section": "Smaller Slide",
    "text": "Smaller Slide\nThe text on this slide will be, um, smaller."
  },
  {
    "objectID": "templates/slides-template.html#sneaky-info-asides-bootnotes",
    "href": "templates/slides-template.html#sneaky-info-asides-bootnotes",
    "title": "YOUR TITLE",
    "section": "Sneaky Info (Asides & Bootnotes)",
    "text": "Sneaky Info (Asides & Bootnotes)\nThis is cool! 1\nAdd reference-location: document to the YAML for end notes.\n\n\nI am at the bottom of the slide\nNo it is not"
  },
  {
    "objectID": "templates/slides-template.html#scrolly-slide",
    "href": "templates/slides-template.html#scrolly-slide",
    "title": "YOUR TITLE",
    "section": "Scrolly Slide",
    "text": "Scrolly Slide\nOverflowed content will be scrollable on this slide."
  },
  {
    "objectID": "templates/slides-template.html#bootnotes",
    "href": "templates/slides-template.html#bootnotes",
    "title": "YOUR TITLE",
    "section": "Bootnotes",
    "text": "Bootnotes"
  },
  {
    "objectID": "templates/slides-template.html#a-slide-with-a-plot",
    "href": "templates/slides-template.html#a-slide-with-a-plot",
    "title": "YOUR TITLE",
    "section": "A slide with a plot",
    "text": "A slide with a plot\n\n\n# ^^ could be fragment, slide, column, column-fragment\nggplot() +\n  geom_point(\n    data = mtcars,\n    aes(wt, mpg),\n    color = \"goldenrod\"\n  ) +\n  labs(\n    title = \"Some Dots\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nYOUR ORG\n\n\n\n\n\n\n\n \n\n\nthe incredible talk about making sense scales"
  },
  {
    "objectID": "slides/slides_session5.html#climate-data---best-practicse",
    "href": "slides/slides_session5.html#climate-data---best-practicse",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Climate data - best practicse",
    "text": "Climate data - best practicse\nFirst we need to setup our environment.\n\ninstall.packages(\"geodata\")\ndevtools::install_github(\"Nowosad/spquery\")\ndevtools::install_github(\"Nowosad/patternogram\")\ndevtools::install_github(\"Nowosad/supercells\")\nlibrary(terra); library(sf);library(geodata);library(mapview);library(spquery);library(patternogram)\n\nUsing the geodata package (helper package for terra) we can download in a very comfortable often used core data sets. In this case Worldclim and\n\n# German border on top level (nation)\nde = geodata::gadm(country = \"DEU\",level = 0,path = tempdir())\n# Worldclim historical climate data for Germany https://www.worldclim.org/data/index.html\nwc_tmin_de = worldclim_country(\"Germany\", var=\"tmin\", path=tempdir())\n# WCRP Coupled Model Intercomparison Project data https://www.wcrp-climate.org/wgcm-cmip\ncmip_tmin_2_5  = geodata::cmip6_world(\"CNRM-CM6-1\", \"585\", \"2061-2080\", var=\"tmin\", res=10, path=tempdir())\n# cropping and masking\nwc_tmin_de = crop(wc_tmin_de,de,mask=T)\ncmip_tmin_2_5_de = crop(cmip_tmin_2_5 , de,mask=T)\n# set plotting layout\nnf &lt;- layout( matrix(c(1,2), ncol=2) )\n\nplot(cmip_tmin_2_5_de)\nplot(wc_tmin_de)\n\n\n\n\nNote: Please check the websites for the meaning of the data",
    "crumbs": [
      "FAQ",
      "Slides",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#minimum-temperature-c",
    "href": "slides/slides_session5.html#minimum-temperature-c",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Minimum temperature (°C)",
    "text": "Minimum temperature (°C)\n\n\n\n\n\nCMIP6 downscaled future climate projection for 2061-2080 [model: CNRM-ESM2-1; ssp: “585”]\n\n\n\n\n\n\nWorldClim version 2.1 climate data for 1970-2000",
    "crumbs": [
      "FAQ",
      "Slides",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#identifying-and-comparing-similar-spatial-patterns",
    "href": "slides/slides_session5.html#identifying-and-comparing-similar-spatial-patterns",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Identifying and comparing similar spatial patterns",
    "text": "Identifying and comparing similar spatial patterns\nUsing the package spquery for finding similarities in continous raster data (e.g., raster time-series)\n\n\n\n# get Marburg and make it a spatial point\nmr = st_sf(st_sfc(st_point(c( 8.770833, 50.810001)),\n                  crs = \"EPSG:4326\"))\nvec = as.numeric(extract(wc_tmin_de, mr, ID = FALSE))\nvec\n# search for similarity in the data set\nsearch_tmin = spq_search(vec, cmip_tmin_2_5_de,\n                         dist_fun = \"euclidean\")\n\n# visualize\nplot(search_tmin,\n     plg = list( loc = \"topright\",title = \"Dissimilarity\"))\nplot(mr, add=TRUE, col='red', lwd=4)\n\n\n\n\nComparison historical Marburg vs CMIP\n\n\n\n\n\n# some cleaning\ncrop= crop( wc_tmin_de,cmip_tmin_2_5_de)\nres = resample(crop,cmip_tmin_2_5_de)\n\n# call comparison\ncompare_tmin = spq_compare(cmip_tmin_2_5_de, res,\n                           dist_fun = \"euclidean\")\n\n# visualize\nplot(compare_tmin,\n     plg = list( loc = \"topright\",title = \"Dissimilarity\"), \n     col =viridis(256))\nplot(mr, add=TRUE, col='red', lwd=4)\n\n\n\n\nComparison Worldclim vs CMIP",
    "crumbs": [
      "FAQ",
      "Slides",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#patternogram",
    "href": "slides/slides_session5.html#patternogram",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "patternogram",
    "text": "patternogram\nDescribing the range of spatial autocorrelation\n\nexplore spatial autocorrelations of predictors in machine learning models\ndetect spatial autocorrelation in various data structures\ncompare the spatial autocorrelation of variables over time\ninvestigate spatial autocorrelation of categorical spatial patterns\n\n\nlibrary(patternogram)\nlibrary(ggplot2)\npg1_100 = patternogram(wc_tmin_de, sample_size = 10)\npg2_100 = patternogram(cmip_tmin_2_5_de, sample_size = 10)\npg1_500 = patternogram(wc_tmin_de, sample_size = 100)\npg2_500 = patternogram(cmip_tmin_2_5_de, sample_size = 100)\npg1_1000 = patternogram(wc_tmin_de, sample_size = 1000)\npg2_1000 = patternogram(cmip_tmin_2_5_de, sample_size = 1000)\nggplot() + geom_point(data=pg1_100, aes(x=dist, y=dissimilarity), color='red4') +\n           geom_point(data=pg2_100, aes(x=dist, y=dissimilarity), color='red') +\n           geom_point(data=pg1_500, aes(x=dist, y=dissimilarity), color='blue') +\n           geom_point(data=pg2_500, aes(x=dist, y=dissimilarity), color='lightblue') +\n           geom_point(data=pg1_1000, aes(x=dist, y=dissimilarity), color='green') +\n           geom_point(data=pg2_1000, aes(x=dist, y=dissimilarity), color='darkgreen')            \n\n\nlibrary(patternogram)\nlibrary(ggplot2)\npg1_100 = patternogram(wc_tmin_de, width = 100, sample_size = 10)\npg2_100 = patternogram(cmip_tmin_2_5_de, width = 100 ,sample_size = 10)\npg1_500 = patternogram(wc_tmin_de,  width = 100,sample_size = 10)\npg2_500 = patternogram(cmip_tmin_2_5_de,  width = 100,sample_size = 10)\npg1_1000 = patternogram(wc_tmin_de,  width = 100,sample_size = 10)\npg2_1000 = patternogram(cmip_tmin_2_5_de, width = 100,sample_size = 10)\nggplot() + geom_point(data=pg1_100, aes(x=dist, y=dissimilarity), color='red4') +\n           geom_point(data=pg2_100, aes(x=dist, y=dissimilarity), color='red') +\n           geom_point(data=pg1_500, aes(x=dist, y=dissimilarity), color='blue') +\n           geom_point(data=pg2_500, aes(x=dist, y=dissimilarity), color='lightblue') +\n           geom_point(data=pg1_1000, aes(x=dist, y=dissimilarity), color='green') +\n           geom_point(data=pg2_1000, aes(x=dist, y=dissimilarity), color='darkgreen')",
    "crumbs": [
      "FAQ",
      "Slides",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#slic-supercells",
    "href": "slides/slides_session5.html#slic-supercells",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Slic /supercells",
    "text": "Slic /supercells\nsupercells: an extension of SLIC (Simple Linear Iterative Clustering; Achanta et al. (2012), doi:10.1109/TPAMI.2012.120) that can be applied to non-imagery geospatial rasters that carry:\n\npattern information (co-occurrence matrices)\ncompositional information (histograms)\ntime-series information (ordered sequences)\nother forms of information for which the use of Euclidean distance may not be justified\nSegmentation/regionalization: partitioning space into smaller segments while minimizing internal inhomogeneity and maximizing external isolation\n\n\n\n\nSLIC/supercells are a way to improve the output and reduce the cost of segmentation",
    "crumbs": [
      "FAQ",
      "Slides",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#code-examples",
    "href": "slides/slides_session5.html#code-examples",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Code examples",
    "text": "Code examples\n\nlibrary(terra)\nlibrary(supercells)\n# Version 1\nmintemp_zones = supercells(cmip_tmin_2_5_de, k = 250, compactness = 4)\nplot(cmip_tmin_2_5_de[[1]]); plot(mintemp_zones, add = TRUE, col = NA)\n# Version 2\nmintemp_zones = supercells(cmip_tmin_2_5_de, k = 50, compactness = 4)\nplot(cmip_tmin_2_5_de[[1]]); plot(mintemp_zones, add = TRUE, col = NA)\n# Version 3\nmintemp_zones = supercells(cmip_tmin_2_5_de, k = 250, compactness = 1)\nplot(cmip_tmin_2_5_de[[1]]); plot(mintemp_zones, add = TRUE, col = NA)\n\n\n\n\n\n\n\n\n\nk = 250, compactness = 4\n\n\n\n\n\n\n\nk = 50, compactness = 4\n\n\n\n\n\n\n\nk = 250, compactness = 1\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "FAQ",
      "Slides",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#code-examples---regionalisation-based-on-supercells",
    "href": "slides/slides_session5.html#code-examples---regionalisation-based-on-supercells",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Code Examples - Regionalisation based on supercells",
    "text": "Code Examples - Regionalisation based on supercells\n\n# create supercells based on the 2D time-series ---------------------------\nsp = supercells(c(ta, pr), step = 15, compactness = 0.01, dist_fun = dtw_2d)\nplot(sp)\n\n# create 3, 7, 11, and 15 regions based on the 2D time-series -------------\nsp_regions = ?map_dfr(c(3, 7, 11, 15), regionalize_dtw_2d, sp)\n\ntm_shape(sp_regions) +\n  tm_polygons() +\n  tm_facets(\"k\")",
    "crumbs": [
      "FAQ",
      "Slides",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#exercises",
    "href": "slides/slides_session5.html#exercises",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Exercises",
    "text": "Exercises\nThe goal: to regionalize Germany’s climates\n\nUse Worldclim versus CMIP data\nUse the upper helper functions and code for Great Britain\n\nExtended SLIC workflow uses the dynamic time warping (DTW) distance function rather than the Euclidean distance.\n\n\n\n&lt;gisma 2024&gt;\n\n\n\nCMIP6 downscaled future climate projection for 2061-2080 [model: CNRM-ESM2-1; ssp: “585”]\nWorldClim version 2.1 climate data for 1970-2000\nComparison historical Marburg vs CMIP\nComparison Worldclim vs CMIP\nk = 250, compactness = 4\nk = 50, compactness = 4\nk = 250, compactness = 4\nk = 50, compactness = 4\nk = 250, compactness = 1",
    "crumbs": [
      "FAQ",
      "Slides",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session3.html#what-is-the-meaning",
    "href": "slides/slides_session3.html#what-is-the-meaning",
    "title": "Landscape Metrics Reloaded",
    "section": "What is the meaning?",
    "text": "What is the meaning?\n\n\n\nSpatial patterns can be quantified using landscape metrics (O’Neill et al. 1988; Turner and Gardner 1991; Li and Reynolds 1993; He et al. 2000; Jaeger 2000; Kot i in. 2006; McGarigal 2014).\n\n\n\nAnd well, we can calculate technically very complex metrics and indices, but what is the meaning?\n\n\n\nParametrization of two IT metrics\n\n\n\nMarginal entropy [H(x)] - diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative mutual information [U] - clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns Reloaded"
    ]
  },
  {
    "objectID": "slides/slides_session3.html#back-to-theory",
    "href": "slides/slides_session3.html#back-to-theory",
    "title": "Landscape Metrics Reloaded",
    "section": "Back to theory",
    "text": "Back to theory\nSo let’s go one step back:\n\nRead the following section of Fragstats Manual and answer the following question:\n\n\n\nWhat are the basic concept an meaning meaning of landscape metrics?\n\n\n\n\n\nForm teams of two and please try to answer the following questions:\n\n\n\n\nWhat is now the specific purpose in scientific application?\nWhat can be expressed with them at all?",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns Reloaded"
    ]
  },
  {
    "objectID": "slides/slides_session3.html#now-try-to-find-an-interpretation",
    "href": "slides/slides_session3.html#now-try-to-find-an-interpretation",
    "title": "Landscape Metrics Reloaded",
    "section": "Now try to find an interpretation",
    "text": "Now try to find an interpretation\n\n\n\nParametrization of two IT metrics\n\n\n\nMarginal entropy [H(x)] - diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative mutual information [U] - clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns Reloaded"
    ]
  },
  {
    "objectID": "slides/slides_session3.html#ercercise",
    "href": "slides/slides_session3.html#ercercise",
    "title": "Landscape Metrics Reloaded",
    "section": "Ercercise",
    "text": "Ercercise\nPlease navigate to the landscapemetrics package.\nRun the following tutorials with the example data of this course:\n\n\n\n\nSample lsm\nIrregular Areas\nUtilities\n\n\n\n\nFor the irregular vector data download the OSM data e.g. from Geofabrik and use the category landuse. Note you need to merge Niedersachsen and Sachsen-Anhalt\nAddon\n\n\n\n\nDownload the FragStats tutorial Analyzing a single grid.\nRun the tutorial once with FragStats and then use the data and indices for the same analysis with landscapemetrics instead.\n\n\n\n\n\n\n\n&lt;gisma 2024&gt;\n\n\n\nParametrization of two IT metrics\nParametrization of two IT metrics",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns Reloaded"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#spatial-patterns",
    "href": "slides/slides_session1.html#spatial-patterns",
    "title": "Spatial Patterns",
    "section": "Spatial Patterns",
    "text": "Spatial Patterns\nDiscovering and describing spatial patterns is an important part of many geographical studies, and spatial patterns are linked to natural and social processes.\n\n\n\n\n\n\n\n\nCorona Virus Distribution, https://coronavirus.jhu.edu/map.html\n\n\n\n\n\n\n\nIPCC Interactive Atlas, Bias Adjusted TX35\n\n\n\n\n\n\nFigure 1: Examples for spatial patterns",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#spatial-patterns-of-categorical-raster-data",
    "href": "slides/slides_session1.html#spatial-patterns-of-categorical-raster-data",
    "title": "Spatial Patterns",
    "section": "Spatial patterns of categorical raster data",
    "text": "Spatial patterns of categorical raster data\nNumerous geographical studies are linked to all kind of classified raster based spatial data\n\n\n\n\n\nhttps://doi.org/10.1016/j.agee.2020.107052\n\n\n\n\n\n\n\n\nhttps://doi.org/10.1016/j.agee.2020.107052",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#point-patterns-versus-continous-patterns",
    "href": "slides/slides_session1.html#point-patterns-versus-continous-patterns",
    "title": "Spatial Patterns",
    "section": "Point patterns versus continous patterns",
    "text": "Point patterns versus continous patterns\n\n\n\n\n\nPoint Distributions, (https://gisgeography.com/spatial-patterns\n\n\n\n\n\n\nRaster based patterns, Source: https://www.cell.com/cell/fulltext/S0092-8674%2823%2901219-9",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#dynamics-of-spatial-patterns",
    "href": "slides/slides_session1.html#dynamics-of-spatial-patterns",
    "title": "Spatial Patterns",
    "section": "Dynamics of spatial patterns",
    "text": "Dynamics of spatial patterns\n\n\n\nDynamic evaluation of landscape transformations, https://www.landscape-online.org/index.php/lo/article/view/LO.2022.1097/LO.2022.1097",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#quantification-of-categorical-spatial-patterns",
    "href": "slides/slides_session1.html#quantification-of-categorical-spatial-patterns",
    "title": "Spatial Patterns",
    "section": "Quantification of categorical spatial patterns",
    "text": "Quantification of categorical spatial patterns\nSpatial patterns can be quantified using landscape metrics (O’Neill et al. 1988; Turner and Gardner 1991; Li and Reynolds 1993; He et al. 2000; Jaeger 2000; Kot i in. 2006; McGarigal 2014).\nSoftware such as FRAGSTATS, GuidosToolbox, or landscapemetrics has proven useful in many scientific studies (&gt; 12,000 citations).\n\nThere is a relationship between an area’s pattern composition and configuration and ecosystem characteristics, such as vegetation diversity, animal distributions, and water quality within this area (Hunsaker i Levine, 1995; Fahrig i Nuttle, 2005; Klingbeil i Willig, 2009; Holzschuh et al., 2010; Fahrig et al., 2011; Carrara et al., 2015; Arroyo-Rodŕıguez et al. 2016; Duflot et al., 2017, many others..)",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#example-data",
    "href": "slides/slides_session1.html#example-data",
    "title": "Spatial Patterns",
    "section": "Example data",
    "text": "Example data\n\nLand cover data for the year 2016 from the CCI-LC project\nSimplified into nine main categories\nPartitioned into 30 x 30 kilometers square blocks\n13,909 categorical rasters (100x100 cells) https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2020JD033031\n\n  —",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#example-data-1",
    "href": "slides/slides_session1.html#example-data-1",
    "title": "Spatial Patterns",
    "section": "Example Data",
    "text": "Example Data\nRandomely selected 16 rasters with different proportions of forest (green) areas:",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#landscape-metrics",
    "href": "slides/slides_session1.html#landscape-metrics",
    "title": "Spatial Patterns",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\n\nIn the last 40 or so years, several hundred different landscape metrics were developed\nThey quantify the composition and configuration of spatial patterns of categorical rasters\nGeneral assumption is that the spatial pattern of a landscape influences the processes that occur within it\nLandscape metrics can be calculated for three different levels: patch, class, and landscape (here we focus on the landscape level)\nThey can be divided into several groups: (1) area and edge metrics, (2) shape metrics, (3) core metrics, (4) aggregation metrics, (5) diversity metrics, (6) complexity metrics",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#landscape-metrics-1",
    "href": "slides/slides_session1.html#landscape-metrics-1",
    "title": "Spatial Patterns",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\nImportant considerations:\n\nScale: the size of the area over which the metrics are calculated\nExtent: the borders of the study area\nSpatial resolution: the size of the raster cells\nCategorization: the categories used in the analysis\nRedundancy: many metrics are highly correlated",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#landscape-metrics-2",
    "href": "slides/slides_session1.html#landscape-metrics-2",
    "title": "Spatial Patterns",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\nSHDI - Shannon’s diversity index - takes both the number of classes and the abundance of each class into account; larger values indicate higher diversity\n\nAI - Aggregation index - from 0 for maximally disaggregated to 100 for maximally aggregated classes",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#code-examples",
    "href": "slides/slides_session1.html#code-examples",
    "title": "Spatial Patterns",
    "section": "Code examples",
    "text": "Code examples\nTaken from : “The landscapemetrics and motif packages for measuring landscape patterns and processes”\nRead and visualize the data\nlibrary(landscapemetrics)\nlibrary(terra)\nr9 = rast(\"exdata/r9.tif\")\nr1 = rast(\"exdata/r1.tif\")\nplot(r1); plot(r9)",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#get-the-indices",
    "href": "slides/slides_session1.html#get-the-indices",
    "title": "Spatial Patterns",
    "section": "Get the indices",
    "text": "Get the indices\nlsm_l_shdi(r9)\n\n# A tibble: 1 × 6\n  layer level     class    id metric value\n  &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 landscape    NA    NA shdi    1.06\n\nlsm_l_ai(r9)\n\n# A tibble: 1 × 6\n  layer level     class    id metric value\n  &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 landscape    NA    NA ai      82.1\n\ncalculate_lsm(r9, what = c(\"lsm_l_shdi\", \"lsm_l_ai\"))\n\n# A tibble: 2 × 6\n  layer level     class    id metric value\n  &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 landscape    NA    NA ai     82.1 \n2     1 landscape    NA    NA shdi    1.06\n\ntwo_r = list(r1, r9)\ncalculate_lsm(two_r, what = c(\"lsm_l_shdi\", \"lsm_l_ai\"))\n\n# A tibble: 4 × 6\n  layer level     class    id metric   value\n  &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     1 landscape    NA    NA ai     98.7   \n2     1 landscape    NA    NA shdi    0.0811\n3     2 landscape    NA    NA ai     82.1   \n4     2 landscape    NA    NA shdi    1.06",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#code-examples-1",
    "href": "slides/slides_session1.html#code-examples-1",
    "title": "Spatial Patterns",
    "section": "Code examples",
    "text": "Code examples\nmat_window = matrix(1, nrow = 11, ncol = 11)\nw_result = window_lsm(r9, window = mat_window, what = \"lsm_l_ai\")\nplot(r9); plot(w_result$layer_1$lsm_l_ai)",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#package-landscapemetrics",
    "href": "slides/slides_session1.html#package-landscapemetrics",
    "title": "Spatial Patterns",
    "section": "Package landscapemetrics",
    "text": "Package landscapemetrics\nhttps://r-spatialecology.github.io/landscapemetrics/\n# A tibble: 133 × 5\n   metric name                                type           level function_name\n   &lt;chr&gt;  &lt;chr&gt;                               &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;        \n 1 area   patch area                          area and edge… patch lsm_p_area   \n 2 cai    core area index                     core area met… patch lsm_p_cai    \n 3 circle related circumscribing circle       shape metric   patch lsm_p_circle \n 4 contig contiguity index                    shape metric   patch lsm_p_contig \n 5 core   core area                           core area met… patch lsm_p_core   \n# ℹ 134 more rows\n\nSampling around points of interest\nMoving window calculations\nCalculating landscape metrics for irregular areas\nVisualizations\nMore…",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#exercises",
    "href": "slides/slides_session1.html#exercises",
    "title": "Spatial Patterns",
    "section": "Exercises",
    "text": "Exercises\n\nRead the data from exdata/lc_small.tif and visualize it. What is the location of the data? What are the extent of the data and its spatial resolution? How many categories it contains?\nCalculate Aggregation Index (AI) for the raster. Interpret the results.\nCalculate Total Edge (TE) for the raster. Interpret the results. Next, read the data from `exdata/lc_small2.tif, calculate AI and TE for this raster, and compare the results with the previous raster.\nCalculate Total Edge (TE) for the raster, but this time in a moving window of 9 by 9 cells. Visualize the results.\n(Extra) Using the read_sf() function from the sf package, read the exdata/points.gpkg file. Next, calculate SHDI and AI of an area of 3000 meters from each sampling point (see the sample_lsm() function).\n\n\n\n\n&lt;gisma 2023&gt;\n\n\n\nCorona Virus Distribution, https://coronavirus.jhu.edu/map.html\nIPCC Interactive Atlas, Bias Adjusted TX35\nhttps://doi.org/10.1016/j.agee.2020.107052\nhttps://doi.org/10.1016/j.agee.2020.107052\nPoint Distributions, (https://gisgeography.com/spatial-patterns\nRaster based patterns, Source: https://www.cell.com/cell/fulltext/S0092-8674%2823%2901219-9\nDynamic evaluation of landscape transformations, https://www.landscape-online.org/index.php/lo/article/view/LO.2022.1097/LO.2022.1097",
    "crumbs": [
      "FAQ",
      "Slides",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slide1.html#header-12",
    "href": "slides/slide1.html#header-12",
    "title": "Slides and extensions",
    "section": "Header (1|2)",
    "text": "Header (1|2)\nThe support of header and footer logic is provided by the plugin reveal-header. it is activated by:\nfilters:\n  - reveal-header"
  },
  {
    "objectID": "slides/slide1.html#header-22",
    "href": "slides/slide1.html#header-22",
    "title": "Slides and extensions",
    "section": "Header (2|2)",
    "text": "Header (2|2)\nIn this example you will find a basic header and footer text, pagination and a logo in the upper left corner .\n---\ntitle: \"Slides and extensions\"\nsubtitle: \"basically shows the 3 extensions samples\"\ntitle-slide-attributes:\n  data-background-image: slide1/mof.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nformat: \n  revealjs:\n    slide-number: true\n    footer: &lt;gisma 2023&gt;\n    header: This is the header extension\n    header-logo: slide1/logooil.jpg\n[...]\n---"
  },
  {
    "objectID": "slides/slide1.html#spotlight-12",
    "href": "slides/slide1.html#spotlight-12",
    "title": "Slides and extensions",
    "section": "Spotlight (1|2)",
    "text": "Spotlight (1|2)\nThe support of a pointer or similar pointing features is provided by the plugin spotlight. it is activated by:\nrevealjs-plugins:\n  - spotlight"
  },
  {
    "objectID": "slides/slide1.html#spotlight-22",
    "href": "slides/slide1.html#spotlight-22",
    "title": "Slides and extensions",
    "section": "Spotlight (2|2)",
    "text": "Spotlight (2|2)\nCurrently the spotlight is set to a red dot pointer. Just press the left mouse button and use it. It is defined in the header:\n---\n[...]\nformat: \n  revealjs:\n    slide-number: true\n    footer: &lt;gisma 2023&gt;\n    header: This is the header extension\n    header-logo: slide1/logooil.jpg\n    spotlight:\n      useAsPointer: true\n      size: 5\n\nfilters:\n  - roughnotation\n  - reveal-header\nrevealjs-plugins:\n  - spotlight\n---"
  },
  {
    "objectID": "slides/slide1.html#highlighting-concept",
    "href": "slides/slide1.html#highlighting-concept",
    "title": "Slides and extensions",
    "section": "Highlighting concept",
    "text": "Highlighting concept\nThe support of complex highlighting etc. is provided by the plugin roughnotation. it is activated by:\nfilters:\n  - roughnotation\nTo activate the highlighting interactively press the r key. It will start any notation animations:\nI will be highlighted, and so will these words right here"
  },
  {
    "objectID": "slides/slide1.html#options",
    "href": "slides/slide1.html#options",
    "title": "Slides and extensions",
    "section": "Options",
    "text": "Options\nThere are many types of options we can use (Press r to show)\n\ntype\nanimate\nanimationDuration\ncolor\nstrokeWidth\nmultiline multiline multiline multiline multiline multiline multiline multiline multiline multiline\niterations\nrtl"
  },
  {
    "objectID": "slides/slide1.html#options-1",
    "href": "slides/slide1.html#options-1",
    "title": "Slides and extensions",
    "section": "Options",
    "text": "Options\n(Press r to show)\nThe options are applied by adding arguments like so {.rn rn-color=orange rn-type=circle}\nSo to add a orange circle or turn off animations by adding rn-animate=false\nNote that the arguments are all prefixed with rn-, are not comma-separated, logical values are written as true or false and that strings do not have to be in quotes"
  },
  {
    "objectID": "slides/slide1.html#options---types",
    "href": "slides/slide1.html#options---types",
    "title": "Slides and extensions",
    "section": "Options - types",
    "text": "Options - types\n(Press r to show)\n\n\nUnderline\nBox\nCircle\nHighlight\nStrike-Through\nCrossed-off\n\nMany types to choose from!\nHyphenated options can be used like so rn-type=strike-through"
  },
  {
    "objectID": "slides/slide1.html#options---multiline",
    "href": "slides/slide1.html#options---multiline",
    "title": "Slides and extensions",
    "section": "Options - Multiline",
    "text": "Options - Multiline\n(Press r to show)\nThe options rn-multiline=true can be added to make a highligher work across multiple lines.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed accumsan nisi hendrerit augue molestie tempus. Phasellus purus quam, aliquet nec commodo quis, pharetra ut orci. Donec laoreet ligula nisl, placerat molestie mauris luctus id. Fusce dapibus non libero nec lobortis."
  },
  {
    "objectID": "slides/slide1.html#all-about-time",
    "href": "slides/slide1.html#all-about-time",
    "title": "Slides and extensions",
    "section": "All about Time",
    "text": "All about Time\n(Press r to show)\nUnless otherwise specified, all annotations will occur at the same time. Set the rn-index to specify order\nNo rn-index\nrn-index set to 1\nrn-index set to 2\nrn-index set to 3\nrn-index set to 4"
  },
  {
    "objectID": "slides/slide1.html#fenced-divs",
    "href": "slides/slide1.html#fenced-divs",
    "title": "Slides and extensions",
    "section": "Fenced divs",
    "text": "Fenced divs\nYou can also use fenced divs if you want to apply the changes to larger sections of of the slide\n::: {.rn rn-type=box rn-color=red}\nHere is some text\n\nAnd there is more here\n:::\n\nHere is some text\nAnd there is more here"
  },
  {
    "objectID": "slides/slide1.html#known-issues",
    "href": "slides/slide1.html#known-issues",
    "title": "Slides and extensions",
    "section": "Known issues",
    "text": "Known issues\ndoesn’t show correctly in RStudio IDE\nDepending on Browser and setting use the CTRL +/- zoom to place the highlights at the correct places"
  },
  {
    "objectID": "slides/slide1.html#basic-reference",
    "href": "slides/slide1.html#basic-reference",
    "title": "Slides and extensions",
    "section": "Basic Reference",
    "text": "Basic Reference\nFind more informations at Quarto RevealJS Documentation\n\n\n\n&lt;gisma 2023&gt;\n\n\n\n\n\n\n\n \n\n\nThis is the header extension"
  },
  {
    "objectID": "reader/cd-1.html",
    "href": "reader/cd-1.html",
    "title": "Change Detection - Sentinel",
    "section": "",
    "text": "In the geosciences, remote sensing is the only measurement technique that allows complete coverage of large spatial areas, up to the entire Earth’s surface. Its successful application requires both the use of existing methods and the adaptation and development of new ones.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#introduction",
    "href": "reader/cd-1.html#introduction",
    "title": "Change Detection - Sentinel",
    "section": "Introduction",
    "text": "Introduction\nIn geospatial or environmental informatics, the detection of changes to the Earth’s surface using satellite, aircraft or drone images, known as change detection analysis, is an important application. These results are often linked to biophysical, geophysical or anthropogenic processes in order to gain both a deeper understanding and the possibility of developing predictive models. Methods of image analysis are of outstanding importance for generating spatial information from the underlying processes. Since both the quantity and quality of this “image data” are playing an increasingly important role in environmental monitoring and modeling, it is becoming more and more necessary to integrate “big data” concepts into the analyses. This means performing reproducible analyses with large amounts of data (&gt;&gt; 1 TB). This is essential for both scientific knowledge gain and future societal challenges.\nAs already explained in the introduction, we start with a scalable change detection analysis of forest damage in low mountain ranges, which is a typical application-oriented task. Scalable means that we limit the analysis to a manageable area, the Nordwestharz, and to two time slices. However, the resulting algorithm can be applied to different or larger areas and to more time slices.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#goals",
    "href": "reader/cd-1.html#goals",
    "title": "Change Detection - Sentinel",
    "section": "Goals",
    "text": "Goals\nThis example shows how change detection methods can be applied conventionally to individual satellite scenes and in a modern way in cloud computing environments using rstac (Brazil Data Cube Team 2021) and gdalcubes (Appel and Pebesma 2019) or openeo (Lahn 2024). In addition to classical supervised classification methods such as Maximum Likelihood and Random Forest, the bfast (Verbesselt, Zeileis, and Herold 2012) is used, which includes an unsupervised method for detecting structural breaks in vegetation index time series.\nOther packages used in this tutorial include stars (Pebesma 2019), tmap (Tennekes 2018) and mapview for creating interactive maps, sf for processing vector data, and colorspace (Zeileis et al. 2020) for visualizations with accessible colors.\nThis study employs a variety of approaches to time series and difference analyses with different indices, using the Harz Mountains as a case study for the period between 2018 and 2023. The objective is to analyze or classify the data.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#information-from-satellite-imagery",
    "href": "reader/cd-1.html#information-from-satellite-imagery",
    "title": "Change Detection - Sentinel",
    "section": "Information from satellite imagery",
    "text": "Information from satellite imagery\nUnprocessed satellite images are not necessarily informative. While our eyes can interpret a true-color image relatively conclusively and intuitively, a reliable and reproducible, i.e. scientifically sound, interpretation requires other approaches. A major advantage of typical image analysis methods over visual interpretation is the derivation of additional, so-called invisible information.\nTo obtain useful or meaningful information, e.g. about the land cover in an area, we have to analyze the data according to the question at hand. Probably the best known and most widely used approach is the supervised classification of image data into categories of interest.\nIn this unit, you will learn about the classification of satellite image data. This includes both data acquisition on the Copernicus portal and the various steps from digitising the training data to evaluating the quality of the classifications.\nWe will cover the following topics:\n\nTheoretical principles\nCase Study Harz Mountains\n\nPreparing the work environment - Retrieving Sentinel and auxilliary data\nUnsupervised classification (k-means clustering)\nRecording training areas.\nSupervised classificationcd-1.qmd#step-4—supervised-classification) (Random Forest, Maximum Likelihood)\nEstimating model quality",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#theoretical-principles",
    "href": "reader/cd-1.html#theoretical-principles",
    "title": "Change Detection - Sentinel",
    "section": "Theoretical principles",
    "text": "Theoretical principles\nPlease note that all types of classification usually require extensive data pre-processing. The focus is then on model building and quality assessment, which can be seen as the technical basis for classification, in order to finally derive the interpretation of the results in terms of content in the data post-processing.\nWe will go through this process step by step.\n\nUnsupervised Classification - k-means clustering\nProbably the best-known unsupervised classification technique is K-means clustering, which is also referred to as the “simplest machine learning algorithm”.\nK-means clustering is a technique commonly used in satellite image classification to group pixels with similar spectral characteristics. Treating each pixel as an observation, the algorithm assigns pixels to clusters based on their spectral values, with each cluster having a mean (or centroid) that represents its central spectral signature. This results in the segmentation of the image into distinct regions (similar to Voronoi cells) corresponding to land cover types, such as water, vegetation or urban areas, facilitating further analysis. It is often used to obtain an initial overview of whether the raster data can be sufficiently separated in feature space.\n\nFigure: Convergence of k-means clustering from an unfavorable starting position (two initial cluster centers are fairly close). Chire [CC BY-SA 4.0] via wikipedia.org\n\n\nSupervised classification\nIn supervised land cover classification, a model is derived from a limited amount of training land cover data that predicts land cover for the entire data set. The land cover types are defined a priori, and the model attempts to predict these types based on the similarity between the characteristics of the training data and the rest of the data set.\n Classifiers (e.g. the maximum likelihood classifier) or machine learning algorithms (such as Random Forest) use the training data to determine descriptive models that represent statistical signatures, classification trees or other functions. Within the limits of the quality of the training data, such models are suitable and representative for making predictions for areas if the predictors from the model are available for the entire area.\nWe now want to predict the spatial characteristics of clear-felling/no forest using a maximum likelihood classification and random forest, and apply standard methods of random validation and model quality assessment.\nThe goal is to separate clearcuts from all other pixels and to quantify the differences between 2019 and 2020.\n\nMaximum Likelihood Classification\nMaximum likelihood classification assumes that the distribution of data for each class and in each channel is normally distributed. Under this assumption, the probability that a particular pixel belongs to a particular class is calculated. Since the probabilities can also be specified as a threshold, without this restriction, all pixels are assigned regardless of how unlikely they are. Each pixel is assigned to the class that has the highest probability (i.e., the maximum probability).\n\n\n\nRandom forest\nRandom forests can be used for both regression and classification tasks, with the latter being particularly relevant in environmental remote sensing. Like any machine learning method, the random forest model learns to recognize patterns and structures in the data itself. Since the random forest algorithm also requires training data, it is also a supervised learning method. !\nFigure: Simplified illustration of data classification by random forest during training. Venkata Jagannath [CC BY-SA 4.0] via wikipedia.org\nA random forest algorithm learns from the data by creating random decision trees – hence the name. For classification tasks, the algorithm takes a suitable instance of a decision tree from the training data set and assigns the corresponding class to the pixel. This is repeated with all available decision trees. Finally, the pixel is assigned to the class that has the most trees, according to the winner-takes-all principle.\nFrom a pragmatic point of view, classification tasks generally require the following steps:\n\nCreation of a comprehensive input data set that contains one or more raster layers. Selection of training areas, i.e. subsets of the input data set for which the land cover type is known to the remote sensing expert. Knowledge of the land cover can be obtained, for example, from one’s own or third-party in situ observations, management information or other remote sensing products (e.g. high-resolution aerial photographs). Training a model using the training sites. For validation purposes, the training sites are often subdivided into one or more test and training sites to evaluate the performance of the model algorithm. Applying the trained model to the entire data set, i.e. predicting the land cover type based on the similarity of the data at each site to the class characteristics of the training data set.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#change-detection-case-study-harz-mountains",
    "href": "reader/cd-1.html#change-detection-case-study-harz-mountains",
    "title": "Change Detection - Sentinel",
    "section": "Change detection case study: Harz Mountains",
    "text": "Change detection case study: Harz Mountains\n\nSetting up the work environment\nYou can either use the data saved from the previous exercise or define, download and edit a new area. However, the work environment is usually loaded first.\n\n# ---- 0 Projekt Setup ----\nrequire(\"pacman\")\n#remotes::install_github(\"zivankaraman/CDSE\")\n# packages installing if necessary and loading\npacman::p_load(mapview, mapedit, tmap, tmaptools, raster, terra, stars, gdalcubes, sf,webshot, dplyr,CDSE,webshot, downloader, tidyverse,RStoolbox,rprojroot, exactextractr, randomForest, ranger, e1071, caret, link2GI, rstac, OpenStreetMap,colorspace,ows4R,httr)\n#--- Switch to determine whether digitization is required. If set to FALSE, the\nroot_folder = find_rstudio_root_file()\n\nndvi.col = function(n) {\n  rev(sequential_hcl(n, \"Green-Yellow\"))\n}\n\nano.col = diverging_hcl(7, palette = \"Red-Green\",  register = \"rg\")\n\nPlease add any missing or defective packages in the above setup script (if error messages occur). On the basis of the available Sentinel data, the first step should be to identify suitable data sets for a surface classification.\n\n\nDefining the Area of Interest\nPlease note to project to three different CRS is for this examples convenience and clarity and somewhat superfluous. Only the corner coordinates of the sections are required and not the complete geometries. However, it creates more clarity for the later process to already have the data needed in different projections.\n\n\n# #| eval: false\n#| echo: true\n\n# download the Harz region from the UBA WFS server\n# nre_regions &lt;- \"https://geodienste.bfn.de/ogc/wfs/gliederungen?\"\n# regions_client &lt;- WFSClient$new(nre_regions, \n#                             serviceVersion = \"2.0.0\",)\n# regions_client$getFeatureTypes(pretty = TRUE)\n# \n# url &lt;- parse_url(nre_regions)\n# url$query &lt;- list(service = \"wfs\",\n#                   #version = \"2.0.0\", # optional\n#                   request = \"GetFeature\",\n#                   typename = \"Haupteinheiten\",\n#                   srsName = \"EPSG:4326\"\n#                   )\n# request &lt;- build_url(url)\n# \n# nre_regions_sf &lt;- read_sf(request)\n# harz = nre_regions_sf |&gt; \n#   filter(NAME_ORD3 %in% c( \"Oberharz\")) \n# plot(harz)\n# harz_bbox = bb(harz,projection=4326)\n# harz_32632 =sf::st_transform(harz,crs = 32632)\n# harz_bbox_32632 = bb(harz_32632,projection=32632)\n\n# Download training data which is also used for the extend\nutils::download.file(url=\"https://github.com/gisma/gismaData/raw/master/geoinfo/train_areas_2019_2020.gpkg\",destfile=file.path(\"../data/train_areas_2019_2020.gpkg\"))\ntrain_areas_2019_2020 = st_read(file.path(\"../data/train_areas_2019_2020.gpkg\"))\n\nReading layer `train_areas_2019_2020' from data source \n  `/home/creu/edu/gisma-courses/LV-19-d19-006-24/data/train_areas_2019_2020.gpkg' \n  using driver `GPKG'\nSimple feature collection with 87 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 10.21106 ymin: 51.84931 xmax: 10.39679 ymax: 51.94446\nGeodetic CRS:  WGS 84\n\n# mapping the extents and boundaries of the choosen geometries\n# tmap_mode(\"view\")\n# tmap_options(check.and.fix = TRUE) + \n#   tm_basemap(server = c(\"Esri.WorldGrayCanvas\", \"OpenStreetMap\", \"Esri.WorldTopoMap\",\"Esri.WorldImagery\")) +\n#   #tm_shape(harz) +   \n#   tm_polygons(alpha = 0.4)\n\n#--- Reading the data from the directories\n\n##--- This describes how to process the Corine land use and land cover dataset\n## The necessary file can also be downloaded from the repository\n## An account is required for the download https://land.copernicus.eu/pan-european/corine-land-cover\n## Therefore, download the data manually and unzip \n## U2018_CLC2018_V2020_20u1.tif into the data directory \n## Then continue\nif (!file.exists(file.path(root_folder,\"data/corine_harz.tif\"))){\n  corine = rast(file.path(\"../data/U2018_CLC2018_V2020_20u1.tif\"))\n  corine = terra::project(corine,\"EPSG:4326\" )\n  corine_harz = terra::crop(corine,vect(train_areas_2019_2020))\n  terra::writeRaster(corine_harz,file.path(root_folder,\"data/corine_harz.tif\"),overwrite=TRUE)\n  }\ncorine_harz = rast(file.path(root_folder,\"data/corine_harz.tif\"))\n# Create a forest mask from corine\n# Agro-forestry areas code=22, Broad-leaved forest code=23,\n# Coniferous forest code=24, Mixed forest code=25\nm &lt;- c(-100, 22, 0,\n       22, 26, 1,\n       26, 500, 0)\nrclmat &lt;- matrix(m, ncol=3, byrow=TRUE)\nharz_forest_mask &lt;- classify(corine_harz, rclmat, include.lowest=TRUE)\n\n\nmapview(corine_harz)+mapview(train_areas_2019_2020,zcol=\"class\")+harz_forest_mask\n\n\n\n\n# create extents\ne &lt;- ext(harz_forest_mask)\np &lt;- as.polygons(e, crs=\"EPSG:4326\")\ncm=sf::st_as_sf(p)\nharz_bbox = bb(train_areas_2019_2020,projection=4326)\nharz_32632 =sf::st_transform(train_areas_2019_2020,crs = 32632)\nharz_bbox_32632 = bb(harz_32632,projection=32632)",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-1-retrieving-sentinel-data",
    "href": "reader/cd-1.html#step-1-retrieving-sentinel-data",
    "title": "Change Detection - Sentinel",
    "section": "Step 1: Retrieving Sentinel data",
    "text": "Step 1: Retrieving Sentinel data\n\nAternative 1: Using gdalcubes\nSentinel-2 is currently the most important platform for Earth observation in all areas, but especially for climate change, land use and ecological issues at all levels, from the upper micro to the global scale.\nThere are two operational Sentinel-2 satellites: Sentinel-2A and Sentinel-2B, both in sun-synchronous polar orbits and 180 degrees out of phase. This arrangement allows them to cover the mid-latitudes with an orbital period of about 5 days.\nThe Sentinel-2 data are therefore predestined to record spatial and temporal changes on the Earth’s surface (the forest was green in early summer, it has disappeared by late summer). They are ideal for timely studies before and after natural disasters, or for biomass balancing, etc.\n\nCloud-Optimised GeoTIFFs (COGs)\nUnfortunately, the official Sentinel-2 archives are anything but user-friendly. Even with very convenient tools such as sen2r it is sometimes tedious to process them.Technically, the processed product levels are available for download pre-processed as L1C and L2A products in JP2K format. The preferred file format is JP2K, which is storage efficient but has to be downloaded in its entirety locally by the user, resulting in high access costs and huge local storage requirements. The cloud-optimised GeoTIFFs (COGs) allow only the areas of interest to be downloaded and are also much faster to process. However, this requires optimised cloud services and a technically different access logic than in the processing chains used so far.\n\n\nSpatioTemporal Asset Catalog (STAC)\nThe [Spatial-Temporal Asset Catalogue] (https://stacspec.org/) (STAC) provides a common language for simplified indexing and discovery of geospatial data. A “Spatio-Temporal Asset” is a file that contains information in a specific space and time.\nThis approach allows any provider of spatio-temporal data (imagery, SAR, point clouds, data cubes, full motion video, etc.) to provide Spatio-Temporal Asset Catalogues (STAC) for their data. STAC focuses on an easy-to-implement standard that organisations can use to make their data available in a durable and reliable way.\nElement84 has provided a public API called Earth-search, a central search catalogue for all public AWS datasets using STAC (including the new Sentinel-2 COGs), which contains more than 11.4 million Sentinel-2 scenes worldwide as of 1 November 2017.\nOne major challenge is the fact that most of the earth surface related remote sensing activities are heavily “disturbed” by the atmosphere, especially by clouds. So to find cloud free satellite imagery is a common and cumbersome task. This task is supported by the rstac package which provides a convenient tool to find and filter adequate Sentinel-2 images out of the COG data storage. However, to address the AOI we need to provide the extend via the bbox argument of the corresponding function stac_search(). So first we need to derive and transform the required bounding box to WGS84 geo-coordinates, easily done with the sf functions st_bbox() and st_transform(). In addition we adapt the projection of the referencing vector objects to all other later projection needs.\n\n\nQuerying images with rstac\nUsing the rstac package, we first request all available images from 2018 to 208 that intersect with our region of interest. Here, since the polygon has WGS84 as CRS, we do not need to transform the bounding box before using the stac_search() function.\n\n# #| eval: false\n#| echo: fenced\n# search the data stack for the given period and area\nlibrary(rstac)\ns = stac(\"https://earth-search.aws.element84.com/v0\")\n\nitems &lt;- s |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox= as.vector(harz_bbox), \n              datetime = c(\"2019-06-01/2021-09-01\"),\n              limit = 1000) |&gt;\n  post_request() \nitems\n\n###Items\n- matched feature(s): 334\n- features (334 item(s) / 0 not fetched):\n  - S2B_32UNC_20210901_0_L2A\n  - S2B_32UNC_20210829_0_L2A\n  - S2A_32UNC_20210827_0_L2A\n  - S2A_32UNC_20210824_0_L2A\n  - S2B_32UNC_20210822_0_L2A\n  - S2B_32UNC_20210819_0_L2A\n  - S2A_32UNC_20210817_0_L2A\n  - S2A_32UNC_20210814_0_L2A\n  - S2B_32UNC_20210812_0_L2A\n  - S2B_32UNC_20210809_0_L2A\n  - ... with 324 more feature(s).\n- assets: \nAOT, B01, B02, B03, B04, B05, B06, B07, B08, B09, B11, B12, B8A, info, metadata, overview, SCL, thumbnail, visual, WVP\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, properties.sentinel:boa_offset_applied, stac_extensions, stac_version, type\n\n\nThis gives us 350 matching images recorded between Januar 2019 and December 2023.\n\n\nCreating a monthly Sentinel-2 data cube\nTo obtain a Sentinel data cube, a gdalcube image collection must be created from the STAC query result. To do this, the asset names must be explicitly named in order to apply the SCL channel with the quality characteristics per pixel (classification as clouds, cloud shadows, etc.). In this query, a filter is set to cloud cover &lt;= 50%.\n\n# #| eval: false\n\nlibrary(gdalcubes)\ns2_collection &lt;- stac_image_collection(items$features,\n                                      asset_names = c(\"B04\",\"B08\",\"SCL\"),\n#c(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\", \"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"SCL\"), \n                                      property_filter = function(x) {x[[\"eo:cloud_cover\"]] &lt; 5}) \ns2_collection\n\nImage collection object, referencing 42 images with 3 bands\nImages:\n                      name     left      top   bottom    right\n1 S2A_32UNC_20210718_0_L2A 8.999721 52.35046 51.35264 10.61137\n2 S2B_32UNC_20210630_0_L2A 9.473193 52.34714 51.35264 10.61137\n3 S2B_32UNC_20210531_1_L2A 9.468597 52.34718 51.35264 10.61137\n4 S2A_32UNC_20210516_1_L2A 9.479513 52.34710 51.35264 10.61137\n5 S2B_32UNC_20210514_0_L2A 8.999721 52.35046 51.35266 10.60492\n6 S2B_32UNC_20210511_0_L2A 9.475491 52.34712 51.35264 10.61137\n             datetime        srs\n1 2021-07-18T10:36:33 EPSG:32632\n2 2021-06-30T10:26:33 EPSG:32632\n3 2021-05-31T10:26:33 EPSG:32632\n4 2021-05-16T10:26:32 EPSG:32632\n5 2021-05-14T10:36:29 EPSG:32632\n6 2021-05-11T10:26:30 EPSG:32632\n[ omitted 36 images ] \n\nBands:\n  name offset scale unit nodata image_count\n1  B04      0     1                      42\n2  B08      0     1                      42\n3  SCL      0     1                      42\n\n\nThe result is 118 images, i.e. approx. 1.8 images per month, from which we can now create a data cube. To do this, we use the UTM bounding box of our polygon as a spatial boundary, a spatial resolution of 10 metres, a bilinear spatial interpolation (useful for the spatially lower-resolution sentinel channels) and calculate monthly median values for all pixel values from the available images of a month. In addition, we add a buffer (b) on each side of the cube.\nThe gdalcube image collection can be considered as a proxy structure object which will be applied on the COGs.\n\n# #| eval: false\n\nv = cube_view(srs = \"EPSG:32632\", \n              dx = 10, \n              dy = 10, \n              dt = \"P1M\",  \n              aggregation = \"median\", \n              extent = list(t0 = \"2019-06-01\",\n                            t1 = \"2021-09-01\", \n                            left = st_bbox(harz_32632)[\"xmin\"] , \n                            right = st_bbox(harz_32632)[\"xmax\"] ,\n                            bottom = st_bbox(harz_32632)[\"ymin\"] , \n                            top = st_bbox(harz_32632)[\"ymax\"] ),\n              resampling = \"bilinear\")\nv\n\nA data cube view object\n\nDimensions:\n               low             high count pixel_size\nt       2019-06-01       2021-09-30    28        P1M\ny 5744990.36803304 5755740.36803304  1075         10\nx 583320.314607046 596010.314607046  1269         10\n\nSRS: \"EPSG:32632\"\nTemporal aggregation method: \"median\"\nSpatial resampling method: \"bilinear\"\n\n\nNext we create a data cube, subset the red and near infrared bands and crop by our polygon, which simply sets pixel values outside the polygon to NA. We then save the data cube as a single netCDF file. Note that this is not necessary, but saving intermediate results sometimes makes debugging easier, especially if the methods applied afterwards are computationally intensive.\nOnly calling a final action will start the processing on the COG-Server. In this case ‘write_ncdf’.\n\n## ##\n#| \n# we \"download\" the data and write it t a netcdf file\n  s2.mask = image_mask(\"SCL\", values = c(3,8,9))\n  gdalcubes_options(parallel = 16, \n                    ncdf_compression_level = 5)\n  raster_cube(s2_collection, v, mask = s2.mask) |&gt;\n    write_ncdf(file.path(root_folder,\"data/harz_2019_2021_kndvi.nc\"),overwrite=TRUE)\n\n\n\nkNDVI\nBelow, we derive mean monthly kNDVI values over all pixel time series.\n\n# #| eval: false\n#| \nncdf_cube(file.path(root_folder,\"data/harz_2019_2021_kndvi.nc\")) |&gt;\n    apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  reduce_time(\"mean(kNDVI)\") |&gt;\n  plot(key.pos = 1,  col = ndvi.col, nbreaks = 12)\n\n\n\n\n\n\n\n\n\n\n\nAlternative 2: Copernicus Data Space Ecosystem API Wrapper CDSE\nThe CDSE package provides another simple way to get Sentinel/Copernicus data sets.\n\n# #| eval: false\n#------------\n# NOTE: You must create an Copernicus account and provide the token credtials have a look at:\n#       https://zivankaraman.github.io/CDSE/articles/BeforeYouStart.html#accessing-cdse-data-and-services\n#------------\n\nid &lt;- Sys.getenv(\"CDSE_ID\")\nsecret &lt;- Sys.getenv(\"CDSE_SECRET\")\nOAuthClient &lt;- GetOAuthClient(id = id, secret = secret)\ncollections &lt;- GetCollections(as_data_frame = TRUE)\ncollections\n\n                  id                title\n1     sentinel-2-l1c       Sentinel 2 L1C\n2 sentinel-3-olci-l2   Sentinel 3 OLCI L2\n3    sentinel-3-olci      Sentinel 3 OLCI\n4   sentinel-3-slstr     Sentinel 3 SLSTR\n5     sentinel-1-grd       Sentinel 1 GRD\n6     sentinel-2-l2a       Sentinel 2 L2A\n7     sentinel-5p-l2 Sentinel 5 Precursor\n                                                   description\n1                     Sentinel 2 imagery processed to level 1C\n2 Sentinel 3 data derived from imagery captured by OLCI sensor\n3                   Sentinel 3 imagery captured by OLCI sensor\n4                  Sentinel 3 imagery captured by SLSTR sensor\n5                     Sentinel 1 Ground Range Detected Imagery\n6                     Sentinel 2 imagery processed to level 2A\n7      Sentinel 5 Precursor imagery captured by TROPOMI sensor\n                 since instrument  gsd bands constellation long.min lat.min\n1 2015-11-01T00:00:00Z        msi   10    13    sentinel-2     -180     -56\n2 2016-04-17T11:33:13Z       olci  300    NA          &lt;NA&gt;     -180     -85\n3 2016-04-17T11:33:13Z       olci  300    21          &lt;NA&gt;     -180     -85\n4 2016-04-17T11:33:13Z      slstr 1000    11          &lt;NA&gt;     -180     -85\n5 2014-10-03T00:00:00Z      c-sar   NA    NA    sentinel-1     -180     -85\n6 2016-11-01T00:00:00Z        msi   10    12    sentinel-2     -180     -56\n7 2018-04-30T00:18:50Z    tropomi 5500    NA          &lt;NA&gt;     -180     -85\n  long.max lat.max\n1      180      83\n2      180      85\n3      180      85\n4      180      85\n5      180      85\n6      180      83\n7      180      85\n\nimages &lt;- SearchCatalog(bbox = st_bbox(train_areas_2019_2020), from = \"2018-05-01\", to = \"2022-12-31\", \n    collection = \"sentinel-2-l2a\", with_geometry = TRUE, client = OAuthClient)\n\nalthough coordinates are longitude/latitude, st_intersects assumes that they\nare planar\n\n\nalthough coordinates are longitude/latitude, st_intersection assumes that they\nare planar\n\nimages\n\nSimple feature collection with 688 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 8.999721 ymin: 51.35264 xmax: 10.61137 ymax: 52.35046\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   acquisitionDate tileCloudCover areaCoverage   satellite\n1       2022-12-30         100.00          100 sentinel-2a\n2       2022-12-27          79.15          100 sentinel-2a\n3       2022-12-25          99.99          100 sentinel-2b\n4       2022-12-22          99.45          100 sentinel-2b\n5       2022-12-20          97.60          100 sentinel-2a\n6       2022-12-17          29.56          100 sentinel-2a\n7       2022-12-15           3.80          100 sentinel-2b\n8       2022-12-10          95.63          100 sentinel-2a\n9       2022-12-07          96.07          100 sentinel-2a\n10      2022-12-05          99.99          100 sentinel-2b\n   acquisitionTimestampUTC acquisitionTimestampLocal\n1      2022-12-30 10:36:29       2022-12-30 11:36:29\n2      2022-12-27 10:26:31       2022-12-27 11:26:31\n3      2022-12-25 10:36:29       2022-12-25 11:36:29\n4      2022-12-22 10:26:31       2022-12-22 11:26:31\n5      2022-12-20 10:36:28       2022-12-20 11:36:28\n6      2022-12-17 10:26:31       2022-12-17 11:26:31\n7      2022-12-15 10:36:28       2022-12-15 11:36:28\n8      2022-12-10 10:36:30       2022-12-10 11:36:30\n9      2022-12-07 10:26:33       2022-12-07 11:26:33\n10     2022-12-05 10:36:27       2022-12-05 11:36:27\n                                                            sourceId long.min\n1  S2A_MSIL2A_20221230T103431_N0509_R108_T32UNC_20221230T134707.SAFE 8.999721\n2  S2A_MSIL2A_20221227T102431_N0509_R065_T32UNC_20221227T140052.SAFE 9.476927\n3  S2B_MSIL2A_20221225T103349_N0509_R108_T32UNC_20221225T114808.SAFE 8.999721\n4  S2B_MSIL2A_20221222T102339_N0509_R065_T32UNC_20221222T113435.SAFE 9.485113\n5  S2A_MSIL2A_20221220T103441_N0509_R108_T32UNC_20221220T134756.SAFE 8.999721\n6  S2A_MSIL2A_20221217T102431_N0509_R065_T32UNC_20221217T141955.SAFE 9.478651\n7  S2B_MSIL2A_20221215T103339_N0509_R108_T32UNC_20221215T114317.SAFE 8.999721\n8  S2A_MSIL2A_20221210T103431_N0509_R108_T32UNC_20221210T142357.SAFE 8.999721\n9  S2A_MSIL2A_20221207T102411_N0509_R065_T32UNC_20221207T135807.SAFE 9.476926\n10 S2B_MSIL2A_20221205T103319_N0400_R108_T32UNC_20221205T113923.SAFE 8.999721\n    lat.min long.max  lat.max                       geometry\n1  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n2  51.35264 10.61137 52.34711 POLYGON ((9.889693 52.34711...\n3  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n4  51.35264 10.61137 52.34705 POLYGON ((9.898795 52.34705...\n5  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n6  51.35264 10.61137 52.34710 POLYGON ((9.891455 52.3471,...\n7  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n8  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n9  51.35264 10.61137 52.34711 POLYGON ((9.890282 52.34711...\n10 51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n\nsummary(images$areaCoverage)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.48  100.00  100.00   99.83  100.00  100.00 \n\n# best 30 days without clouds\nday &lt;- images[order(images$tileCloudCover), ]$acquisitionDate[1:30]\n\n# read specific processing scripts\nscript_file_raw = system.file(\"scripts\", \"RawBands.js\", package = \"CDSE\")\nscript_file_savi = \"savi.js\"\nscript_file_kndvi = \"kndvi.js\"\nscript_file_evi = \"evi.js\"\n\n# first day 2018_07_01\nraw_2018_07_01 = GetImage(bbox = st_bbox(train_areas_2019_2020), \n                          time_range = day[12], \n                          script = script_file_raw, \n                          collection = \"sentinel-2-l2a\", \n                          format = \"image/tiff\", \n                          mosaicking_order = \"leastCC\",\n                          resolution = 10, \n                          mask = TRUE, \n                          buffer = 0.01, \n                          client = OAuthClient)\n\nWarning in st_buffer.sfc(bounds, dist = buffer, joinStyle = \"MITRE\", mitreLimit\n= 999999): st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\n\nWarning in st_centroid.sfc(bounds): st_centroid does not give correct centroids\nfor longitude/latitude data\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: /tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: TIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\nnames(raw_2018_07_01) = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")\nterra::plot(raw_2018_07_01, main = paste(names(raw_2018_07_01), day[12]), cex.main = 0.75)\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\n\n\n\n\n\n\nkndvi_2018_07_01 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020),\n                             time_range = day[12], \n                             script = script_file_kndvi, \n                             collection = \"sentinel-2-l2a\", \n                             format = \"image/tiff\", \n                             mosaicking_order = \"leastCC\", \n                             resolution = 10, \n                             mask = TRUE, \n                             buffer = 0.01, \n                             client = OAuthClient)\n\nWarning in st_buffer.sfc(bounds, dist = buffer, joinStyle = \"MITRE\", mitreLimit\n= 999999): st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\n\nWarning in st_centroid.sfc(bounds): st_centroid does not give correct centroids\nfor longitude/latitude data\n\n\nWarning in readLines(script): unvollständige letzte Zeile in 'kndvi.js'\ngefunden\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: /tmp/Rtmpt2vxPR/file68067619224b6: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: TIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\nnames(kndvi_2018_07_01 )[1] = c(\"kNDVI\")\nterra::plot(kndvi_2018_07_01[[1]] , main = paste(names(kndvi_2018_07_01 ), day[12]), cex.main = 0.75)\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067619224b6: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\n\n\n\n\n\n\nsavi_2018_07_01 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                            time_range = day[12], \n                            script = script_file_savi, \n                            collection = \"sentinel-2-l2a\", \n                            format = \"image/tiff\", \n                            mosaicking_order = \"leastCC\", \n                            resolution = 10,\n                            mask = TRUE, \n                            buffer = 0.01, \n                            client = OAuthClient)\n\nWarning in st_buffer.sfc(bounds, dist = buffer, joinStyle = \"MITRE\", mitreLimit\n= 999999): st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\n\nWarning in st_centroid.sfc(bounds): st_centroid does not give correct centroids\nfor longitude/latitude data\n\n\nWarning in readLines(script): unvollständige letzte Zeile in 'savi.js' gefunden\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: /tmp/Rtmpt2vxPR/file6806765b5a86b: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: TIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\nnames(savi_2018_07_01)[1] = c( \"SAVI\")\nterra::plot(savi_2018_07_01[[1]], main = paste(\"SAVI\", day[12]), cex.main = 0.75)\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806765b5a86b: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\n\n\n\n\n\n\nevi_2018_07_01 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                           time_range = day[12], \n                           script = script_file_evi, \n                           collection = \"sentinel-2-l2a\", \n                           format = \"image/tiff\", \n                           mosaicking_order = \"leastCC\", \n                           resolution = 10, \n                           mask = TRUE, \n                           buffer = 0.01, \n                           client = OAuthClient)\n\nWarning in st_buffer.sfc(bounds, dist = buffer, joinStyle = \"MITRE\", mitreLimit\n= 999999): st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\n\nWarning in st_centroid.sfc(bounds): st_centroid does not give correct centroids\nfor longitude/latitude data\n\n\nWarning in readLines(script): unvollständige letzte Zeile in 'evi.js' gefunden\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: /tmp/Rtmpt2vxPR/file6806727840b84: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: TIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\nnames(evi_2018_07_01)[1] = c( \"EVI\")\nterra::plot(evi_2018_07_01[[1]], main = paste(\"EVI\",day[12]), cex.main = 0.75)\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806727840b84: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\n\n\n\n\n\n\npred_stack_2018 = c(raw_2018_07_01,evi_2018_07_01[[1]],kndvi_2018_07_01[[1]],savi_2018_07_01[[1]])\n\n## second day 2022_06_23\nraw_2022_06_23 = GetImage(bbox = st_bbox(train_areas_2019_2020), \n                          time_range = day[1], \n                          script = script_file_raw, \n                          collection = \"sentinel-2-l2a\", \n                          format = \"image/tiff\", \n                          mosaicking_order = \"leastCC\", \n                          resolution = 10, \n                          mask = TRUE, \n                          buffer = 0.01, \n                          client = OAuthClient)\n\nWarning in st_buffer.sfc(bounds, dist = buffer, joinStyle = \"MITRE\", mitreLimit\n= 999999): st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\n\nWarning in st_centroid.sfc(bounds): st_centroid does not give correct centroids\nfor longitude/latitude data\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: /tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: TIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\nnames(raw_2022_06_23) = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")\nterra::plot(raw_2022_06_23, main = paste(names(raw_2022_06_23), day[1]), cex.main = 0.75)\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\n\n\n\n\n\n\nkndvi_2022_06_23 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                             time_range = day[1], \n                             script = script_file_kndvi, \n                             collection = \"sentinel-2-l2a\", \n                             format = \"image/tiff\",\n                             mosaicking_order = \"leastCC\", \n                             resolution = 10, \n                             mask = TRUE, \n                             buffer = 0.01, \n                             client = OAuthClient)\n\nWarning in st_buffer.sfc(bounds, dist = buffer, joinStyle = \"MITRE\", mitreLimit\n= 999999): st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\n\nWarning in st_centroid.sfc(bounds): st_centroid does not give correct centroids\nfor longitude/latitude data\n\n\nWarning in readLines(script): unvollständige letzte Zeile in 'kndvi.js'\ngefunden\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: /tmp/Rtmpt2vxPR/file6806756306a2a: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: TIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\nnames(kndvi_2022_06_23)[1] = c(\"kNDVI\")\nterra::plot(kndvi_2022_06_23[[1]] , main = paste(names(kndvi_2022_06_23 ), day[1]), cex.main = 0.75)\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806756306a2a: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\n\n\n\n\n\n\nsavi_2022_06_23 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                            time_range = day[1], \n                            script = script_file_savi, \n                            collection = \"sentinel-2-l2a\", \n                            format = \"image/tiff\", \n                            mosaicking_order = \"leastCC\", \n                            resolution = 10,\n                            mask = TRUE, \n                            buffer = 0.01, \n                            client = OAuthClient)\n\nWarning in st_buffer.sfc(bounds, dist = buffer, joinStyle = \"MITRE\", mitreLimit\n= 999999): st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\n\nWarning in st_centroid.sfc(bounds): st_centroid does not give correct centroids\nfor longitude/latitude data\n\n\nWarning in readLines(script): unvollständige letzte Zeile in 'savi.js' gefunden\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: /tmp/Rtmpt2vxPR/file6806739e627f4: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: TIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\nnames(savi_2022_06_23)[1] = c( \"SAVI\")\nterra::plot(savi_2022_06_23[[1]], main = paste(\"SAVI\",day[1]), cex.main = 0.75)\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806739e627f4: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\n\n\n\n\n\n\nevi_2022_06_23 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                           time_range = day[1], \n                           script = script_file_evi, \n                           collection = \"sentinel-2-l2a\", \n                           format = \"image/tiff\",\n                           mosaicking_order = \"leastCC\",\n                           resolution = 10, \n                           mask = TRUE, \n                           buffer = 0.01, \n                           client = OAuthClient)\n\nWarning in st_buffer.sfc(bounds, dist = buffer, joinStyle = \"MITRE\", mitreLimit\n= 999999): st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\n\nWarning in st_centroid.sfc(bounds): st_centroid does not give correct centroids\nfor longitude/latitude data\n\n\nWarning in readLines(script): unvollständige letzte Zeile in 'evi.js' gefunden\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: /tmp/Rtmpt2vxPR/file680674ed52d7d: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in new_CppObject_xp(fields$.module, fields$.pointer, ...): GDAL Message\n1: TIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\nnames(evi_2022_06_23)[1] = c( \"EVI\")\nterra::plot(evi_2022_06_23[[1]], main = paste(\"EVI\",day[1]), cex.main = 0.75)\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680674ed52d7d: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$sampleRegularRaster(size): GDAL Message 1:\nTIFFReadDirectory:Sum of Photometric type-related color channels and\nExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as\nExtraSamples.\n\n\n\n\n\n\n\n\npred_stack_2022 = c(raw_2022_06_23,evi_2022_06_23[[1]],kndvi_2022_06_23[[1]],savi_2022_06_23[[1]])\n\nterra::writeRaster(pred_stack_2018,file.path(root_folder,\"data/pred_stack_2018.tif\"),overwrite=TRUE)\n\nWarning in x@ptr$writeRaster(opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$writeRaster(opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806727840b84: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$writeRaster(opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067619224b6: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$writeRaster(opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806765b5a86b: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\nterra::writeRaster(pred_stack_2022,file.path(root_folder,\"data/pred_stack_2022.tif\"),overwrite=TRUE)\n\nWarning in x@ptr$writeRaster(opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$writeRaster(opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680674ed52d7d: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$writeRaster(opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806756306a2a: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$writeRaster(opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806739e627f4: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step2-overview-unsupervised-classification",
    "href": "reader/cd-1.html#step2-overview-unsupervised-classification",
    "title": "Change Detection - Sentinel",
    "section": "Step2: Overview – Unsupervised classification",
    "text": "Step2: Overview – Unsupervised classification\n\nk-means clustering\nIn our example (applied to 5 classes and executed with the function unsuperClass from the RStoolbox package), this looks as follows. The cluster algorithm can achieve a fairly acceptable separation of the clearings/bald spots with 5 clusters, which makes a classification seem promising. Also experiment with other cluster settings and discuss the results.\n\n# #| eval: false\n## k-means über RStoolbox\n\n#\"EVI\"   \"kNDVI\" \"SAVI\"\nprediction_kmeans_2018 = RStoolbox::unsuperClass(pred_stack_2018, \n                                                 nClasses = 5,\n                                                 norm = TRUE, \n                                                 algorithm = \"MacQueen\")\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806727840b84: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067619224b6: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806765b5a86b: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067168e0cef: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806727840b84: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file68067619224b6: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806765b5a86b: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n# Klassifikation\nplot(prediction_kmeans_2018$map)\n\n\n\n\n\n\n\nprediction_kmeans_2022 = RStoolbox::unsuperClass(pred_stack_2022, \n                                                 nClasses = 5,norm = TRUE, \n                                                 algorithm = \"MacQueen\")\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680674ed52d7d: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806756306a2a: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806739e627f4: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680677316d2af: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file680674ed52d7d: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806756306a2a: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\n\nWarning in x@ptr$scale(center, docenter, scale, doscale, opt): GDAL Message 1:\n/tmp/Rtmpt2vxPR/file6806739e627f4: TIFFReadDirectory:Sum of Photometric\ntype-related color channels and ExtraSamples doesn't match SamplesPerPixel.\nDefining non-color channels as ExtraSamples.\n\nplot(prediction_kmeans_2022$map)\n\n\n\n\n\n\n\n\n \n\n\nbfast: Spatial identification of magnitudes and time periods of kNDVI changes\nTo apply a more complex time series method such as Breaks For Additive Seasonal and Trend (BFAST), which is often used for land cover changes (e.g. (Wu et al. 2020)), the bfastmonitor() function in gdalcubes can be used, the data cube operations below allow you to provide custom user-defined R functions instead of string expressions that translate to built-in reducers.\nIn our example, bfastmonitor returns change date and change magnitude values per time series so we can use reduce_time(). The script below: 1. calculates the kNDVI, 1. applies bfastmonitor(), and properly handles errors e.g. due to missing data with tryCatch(), and 1. finally writes out the resulting change dates and magnitudes of change for all pixels of the time series as a netCDF file.\nThe results shows the changes starting at 7/2019 until 10/2021.\n\nfigtrim &lt;- function(path) {\n  img &lt;- magick::image_trim(magick::image_read(path))\n  magick::image_write(img, path)\n  path\n}\n\ngdalcubes_options(parallel = 12)\n\n## start analysis\nsystem.time(\n  ncdf_cube(file.path(root_folder,\"data/harz_2019_2021_kndvi.nc\")) |&gt;\n    reduce_time(names = c(\"change_date\", \"change_magnitude\", \"kndvi\"), \n                FUN = function(x) {\n                  kndvi = tanh(((x['B08',]-x['B04',])/(x['B08',]+x['B04',]))^2)\n                  if (all(is.na(kndvi))) {\n                    return(c(NA,NA))\n                    }\n                  kndvi_ts = ts(kndvi, start = c(2019, 1), frequency = 12)\n                  library(bfast)\n                  tryCatch({\n                    result = bfastmonitor(kndvi_ts, \n                                          start = c(2020,1), \n                                          history = \"all\", \n                                          level = 0.01)\n                    return(c(result$breakpoint, result$magnitude))\n                    }, error = function(x) {return(c(NA,NA))\n                      })\n                  }) |&gt;\n    write_ncdf(file.path(root_folder,\"data/bfast_results.nc\"),overwrite = TRUE))\n\nNow we can use the netCDF file and map the results with any preferred visualisation tool. In this case tmap.\n\n# plotting it from the local ncdf  \ntmap_mode(\"view\")\ngdalcubes::ncdf_cube(file.path(root_folder,\"data/bfast_results.nc\")) |&gt;\n  stars::st_as_stars() -&gt; x\n\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[1]) + \n  tm_raster(n = 6)  +\n  tm_layout(\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid()\n\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[2])  + tm_raster() +\n  tm_layout(legend.title.size = 1,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            legend.text.size = 0.4,\n            legend.outside = TRUE) +\n  tm_grid()\n\n\n# plotting it from the local ncdf  \ntmap_mode(\"view\")\n\n# Period of Change Map\ngdalcubes::ncdf_cube(file.path(root_folder,\"data/bfast_results.nc\")) |&gt;\n  stars::st_as_stars() -&gt; x\n\ntmap::tmap_save(tm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[1] ) + \n  tm_raster(n = 6)  +\n  tm_layout(legend.title.size = 1,\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid(), file = \"m1.html\" ) \n\n# Magnitude Change Map \ntmap::tmap_save(tm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[2])  + tm_raster() +\n  tm_layout(legend.title.size = 1,\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid(),file = \"m2.html\" ) \n\n\n\n\nMagnitude Change Map \n\n\n\n\n\nPeriod of Change Map",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-3---generating-training-data",
    "href": "reader/cd-1.html#step-3---generating-training-data",
    "title": "Change Detection - Sentinel",
    "section": "Step 3 - Generating training data",
    "text": "Step 3 - Generating training data\nFor a supervised classification, we need data that indicates which surface class defined areas of the satellite image belong to. This data is referred to as training data and is very often obtained by manual digitization. This can be done quite comfortably in RStudio if only a few training areas have to be digitized quickly and effectively.\nWe assume that we want to classify two types of land cover: clearcut and other.\nFor larger tasks, it makes sense to use the convenient method described in the current QGIS documentation, for example in the digitizing tutorial.\nDigitizing training data in R\nAssuming we have digitized trainingdata using either QGIS or R We need now to extract the values according to the assigned areas:\n\n# #| eval: false\n\npred_stack_2018 = rast(file.path(root_folder,\"data/pred_stack_2018.tif\"))\npred_stack_2022 = rast(file.path(root_folder,\"data/pred_stack_2022.tif\"))\n# use the provided training data set\n# Extract the training data for the digitized areas\ntDF_2019 = exactextractr::exact_extract(pred_stack_2018, filter(train_areas_2019_2020,year==2019), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\ntDF_2020 = exactextractr::exact_extract(pred_stack_2022, filter(train_areas_2019_2020,year==2020), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\n# again, copy together into a file\ntDF_2019 = dplyr::bind_rows(tDF_2019)\ntDF_2019$year = 2019\ntDF_2020 = dplyr::bind_rows(tDF_2020)\ntDF_2020$year = 2020\n# Delete any rows that contain NA (no data) values\ntDF_2019 = tDF_2019[complete.cases(tDF_2019) ,]\ntDF_2020 = tDF_2020[complete.cases(tDF_2020) ,]\n\ntDF= rbind(tDF_2019,tDF_2020)\n\n# check the extracted data\nsummary(tDF)\n\n    class                B01              B02              B03      \n Length:69883       Min.   :  86.0   Min.   :  45.0   Min.   :  66  \n Class :character   1st Qu.: 177.0   1st Qu.: 170.0   1st Qu.: 338  \n Mode  :character   Median : 216.0   Median : 230.0   Median : 466  \n                    Mean   : 333.4   Mean   : 378.6   Mean   : 598  \n                    3rd Qu.: 459.0   3rd Qu.: 530.0   3rd Qu.: 814  \n                    Max.   :2168.0   Max.   :5620.0   Max.   :7272  \n      B04              B05              B06            B07            B08      \n Min.   :  20.0   Min.   :  15.0   Min.   :   0   Min.   :   0   Min.   :   0  \n 1st Qu.: 190.0   1st Qu.: 642.0   1st Qu.:1751   1st Qu.:1981   1st Qu.:2042  \n Median : 278.0   Median : 854.0   Median :2498   Median :2929   Median :3168  \n Mean   : 555.7   Mean   : 994.3   Mean   :2357   Mean   :2902   Mean   :3085  \n 3rd Qu.: 850.0   3rd Qu.:1332.0   3rd Qu.:3107   3rd Qu.:3946   3rd Qu.:4164  \n Max.   :9344.0   Max.   :5904.0   Max.   :5991   Max.   :6260   Max.   :7032  \n      B8A            B09            B11            B12            EVI        \n Min.   :   0   Min.   :   0   Min.   :  19   Min.   :  17   Min.   :  0.00  \n 1st Qu.:2144   1st Qu.:2219   1st Qu.:1497   1st Qu.: 706   1st Qu.: 12.00  \n Median :3274   Median :3314   Median :1923   Median : 908   Median : 73.00  \n Mean   :3168   Mean   :3214   Mean   :1784   Mean   :1019   Mean   : 84.35  \n 3rd Qu.:4286   3rd Qu.:4349   3rd Qu.:2199   3rd Qu.:1395   3rd Qu.:121.00  \n Max.   :6582   Max.   :6119   Max.   :5012   Max.   :4536   Max.   :255.00  \n     kNDVI             SAVI              x               y        \n Min.   :  0.00   Min.   :  6.00   Min.   :10.21   Min.   :51.85  \n 1st Qu.:  0.00   1st Qu.: 14.00   1st Qu.:10.25   1st Qu.:51.90  \n Median : 27.00   Median : 73.00   Median :10.32   Median :51.93  \n Mean   : 64.77   Mean   : 84.91   Mean   :10.30   Mean   :51.92  \n 3rd Qu.:119.00   3rd Qu.:118.00   3rd Qu.:10.36   3rd Qu.:51.94  \n Max.   :255.00   Max.   :255.00   Max.   :10.40   Max.   :51.94  \n      cell         coverage_fraction      year     \n Min.   : 158288   Min.   :0.0000    Min.   :2019  \n 1st Qu.: 274404   1st Qu.:1.0000    1st Qu.:2019  \n Median : 449043   Median :1.0000    Median :2019  \n Mean   : 596758   Mean   :0.9199    Mean   :2019  \n 3rd Qu.: 813870   3rd Qu.:1.0000    3rd Qu.:2020  \n Max.   :1655492   Max.   :1.0000    Max.   :2020  \n\n# Save as R internal data format\n# is stored in the repo and can therefore be loaded (line below)\nsaveRDS(tDF, paste0(\"../data/tDF_2018_2022.rds\"))",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-4---supervised-classification",
    "href": "reader/cd-1.html#step-4---supervised-classification",
    "title": "Change Detection - Sentinel",
    "section": "Step 4 - supervised classification",
    "text": "Step 4 - supervised classification\nClassifiers (e.g. the maximum likelihood classifier) or machine learning algorithms (such as Random Forest) use the training data to determine descriptive models that represent statistical signatures, classification trees or other functions. Within the limits of the quality of the training data, such models are suitable and representative for making predictions for areas if the predictors from the model are available for the entire area.\nWe now want to predict the spatial characteristics of clear-felling/no forest using a maximum likelihood classification and random forest, and apply standard methods of random validation and model quality assessment.\nThe goal is to separate clearcuts from all other pixels and to quantify the differences between 2019 and 2020.\n\nMaximum Likelihood Classification\nSince the maximum likelihood algorithm requires training data, it is a supervised learning method. This means that we, as users, have to provide the algorithm with data that conveys knowledge about the classes to be predicted. This data is then divided into training and test data.\n\n# #| eval: false\n# ---- Maximum Likelihood Classification ----\n\n## Here the caret utility package is used\n# Setting a \"seed\" enables reproducible randomness\nset.seed(123)\ntDF = readRDS( file.path(root_folder,\"data/tDF_2018_2022.rds\"))\n# Randomly draw 15% of the data (training/test)\nidx = createDataPartition(tDF$class,list = FALSE,p = 0.05)\ntrainDat = tDF[idx,]\ntestDat = tDF[-idx,]\n\n# Response variable (= \"class\" column) must be of the \"factor\" data type\ntrainDat$class &lt;- as.factor(trainDat$class)\ntestDat$class &lt;- as.factor(testDat$class)\n\n\n# superClass() function from the RSToolbox package requires the table to be converted into the\n# required (old) SpatialdataPoint object\n\nsp_trainDat = trainDat\nsp_testDat = testDat \nsp::coordinates(sp_trainDat) = ~x+y\nsp::coordinates(sp_testDat) = ~x+y\ncrs(sp_trainDat) = crs(pred_stack_2018)\ncrs(sp_testDat) = crs(pred_stack_2018)\n\n\n# superClass method \"mlc\" trains the model and then classifies it\nprediction_mlc_2018 &lt;- superClass(pred_stack_2018, trainData = sp_trainDat[,1:16],valData = sp_testDat[,1:16], responseCol = \"class\", model = \"mlc\", tuneLength = 1, trainPartition = 0.3,verbose = TRUE, filename=file.path(root_folder,\"data/prediction_mlc_2018.tif\"))\n\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\nendCapStyle, : st_buffer does not correctly buffer longitude/latitude data\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\nendCapStyle, : st_buffer does not correctly buffer longitude/latitude data\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\nendCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n\n\nMaximum Likelihood Classification \n\n3490 samples\n  15 predictor\n   2 classes: 'clearcut', 'other' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 2793, 2792, 2792, 2791, 2792 \nResampling results:\n\n  Accuracy   Kappa    \n  0.8999987  0.3932103\n\n[[1]]\n  TrainAccuracy TrainKappa method\n1     0.8999987  0.3932103 custom\n\n[[2]]\nCross-Validated (5 fold) Confusion Matrix \n\n(entries are average cell counts across resamples)\n \n          Reference\nPrediction clearcut other\n  clearcut     26.2  68.6\n  other         1.2 602.0\n                         \n Accuracy (average) : 0.9\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction clearcut other\n  clearcut     1265  2538\n  other          63 23130\n                                          \n               Accuracy : 0.9037          \n                 95% CI : (0.9001, 0.9071)\n    No Information Rate : 0.9508          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.4532          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.95256         \n            Specificity : 0.90112         \n         Pos Pred Value : 0.33263         \n         Neg Pred Value : 0.99728         \n             Prevalence : 0.04919         \n         Detection Rate : 0.04686         \n   Detection Prevalence : 0.14087         \n      Balanced Accuracy : 0.92684         \n                                          \n       'Positive' Class : clearcut        \n                                          \n\nprediction_mlc_2022 &lt;- superClass(pred_stack_2022, trainData = sp_trainDat[,1:16],valData = sp_testDat[,1:16], responseCol = \"class\",model = \"mlc\", tuneLength = 1, trainPartition = 0.3,verbose = TRUE,filename=file.path(root_folder,\"data/prediction_mlc_2022.tif\"))\n\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\nendCapStyle, : st_buffer does not correctly buffer longitude/latitude data\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\nendCapStyle, : st_buffer does not correctly buffer longitude/latitude data\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\nendCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n\n\nMaximum Likelihood Classification \n\n3490 samples\n  15 predictor\n   2 classes: 'clearcut', 'other' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 2791, 2792, 2792, 2793, 2792 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9469852  0.5640534\n\n[[1]]\n  TrainAccuracy TrainKappa method\n1     0.9469852  0.5640534 custom\n\n[[2]]\nCross-Validated (5 fold) Confusion Matrix \n\n(entries are average cell counts across resamples)\n \n          Reference\nPrediction clearcut other\n  clearcut     26.2  35.8\n  other         1.2 634.8\n                           \n Accuracy (average) : 0.947\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction clearcut other\n  clearcut     1289  1789\n  other          39 23879\n                                          \n               Accuracy : 0.9323          \n                 95% CI : (0.9292, 0.9353)\n    No Information Rate : 0.9508          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.5545          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.97063         \n            Specificity : 0.93030         \n         Pos Pred Value : 0.41878         \n         Neg Pred Value : 0.99837         \n             Prevalence : 0.04919         \n         Detection Rate : 0.04775         \n   Detection Prevalence : 0.11402         \n      Balanced Accuracy : 0.95047         \n                                          \n       'Positive' Class : clearcut        \n                                          \n\nsaveRDS(prediction_mlc_2018, file.path(root_folder,\"data/prediction_mlc_2018.rds\"))\nsaveRDS(prediction_mlc_2022, file.path(root_folder,\"data/prediction_mlc_2022.rds\"))\n\n\n\nRandom forest\nA simplified version of the workflow proposed by Max Kuhn (Kuhn and Max 2008) and improved by Hanna Meyer et al. (Meyer et al. 2024) is used for the random forest classification. For further understanding visit The CAST documentation\n\n\nPrediction on the original data\nNow we are ready to apply the verified model to our data set. In remote sensing, this is usually called classification.\n\n# #| eval: false\n\nrf_model = readRDS(file.path(root_folder,\"data/rf_model.rds\"))\n\n# Classification (also known as prediction)\nprediction_rf_2018  = terra::predict(pred_stack_2018 ,rf_model)\nprediction_rf_2022  = terra::predict(pred_stack_2022 ,rf_model)\nsaveRDS(prediction_rf_2018, file.path(root_folder,\"data/prediction_rf_2018.rds\"))\nsaveRDS(prediction_rf_2022, file.path(root_folder,\"data/prediction_rf_2022.rds\"))\n\n\n# #| eval: false\n## ##\nprediction_rf_2018 = readRDS(file.path(root_folder,\"data/prediction_rf_2018.rds\"))\nprediction_rf_2022 = readRDS(file.path(root_folder,\"data/prediction_rf_2022.rds\"))\nprediction_mlc_2018 = rast(file.path(root_folder,\"data/prediction_mlc_2018.tif\"))\nprediction_mlc_2022 = rast(file.path(root_folder,\"data/prediction_mlc_2022.tif\"))\n\n## ---- Visualisierung mit mapview ----\nmask = resample(harz_forest_mask,pred_stack_2022)\n\nplot(mask*prediction_rf_2022 - mask*prediction_rf_2018) \n\n\n\n\n\n\n\nplot(mask*prediction_mlc_2022-mask*prediction_mlc_2018)\n\n\n\n\n\n\n\n\n\n\nA visual comparison shows that the Random Forest and Maximum Likelihood classifications provide results of comparable quality. But does this impression stand up to quantitative analysis?\n\n\nStep 5: Estimation model quality\nThe test data are now used for the independent quality check of the model. A confusion matrix indicates how accurately the model predicts the correct classes. The main diagonal of the matrix indicates the cases in which the model applies. In our classification of only two classes, however, a special case applies: evaluation of a binary classifier. Detailed explanations for the function used here can be found in the caret help.\nThe main statements about model quality are:\n\n‘Positive’ Class = clearcut: is measured with the sensitivity (true positive rate), which indicates the probability that a positive object is correctly classified as positive.\n‘Negative Class’ = other: is measured with the specificity (true negative rate) and indicates the probability that a negative object is correctly classified as negative.\nPositive and negative predictive values indicate the actual performance for clearcut and other. They are corrected for the actual frequency distribution and are a measure of the precision and performance of the model with regard to the respective classes.\n\nDespite the high values, we see that the clearcut class drops off significantly here. This can certainly be taken as an indication of the need to improve the classification.\nOverall, however, the model can be considered good.\n\n# #| eval: false\n## ##\n# ----Calculation of the confusion matrix  ----\ncm_rf &lt;- confusionMatrix(data = predict(rf_model, newdata = testDat), testDat$class)\ncm_rf\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction clearcut other\n  clearcut     1414   246\n  other        1186 63542\n                                          \n               Accuracy : 0.9784          \n                 95% CI : (0.9773, 0.9795)\n    No Information Rate : 0.9608          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6533          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.54385         \n            Specificity : 0.99614         \n         Pos Pred Value : 0.85181         \n         Neg Pred Value : 0.98168         \n             Prevalence : 0.03916         \n         Detection Rate : 0.02130         \n   Detection Prevalence : 0.02500         \n      Balanced Accuracy : 0.76999         \n                                          \n       'Positive' Class : clearcut",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#further-support",
    "href": "reader/cd-1.html#further-support",
    "title": "Change Detection - Sentinel",
    "section": "Further support",
    "text": "Further support\nConsider the following resources as examples of how a specific conceptual and technical approach to answering a question can be “crystallized” step by step from the wide range of instructions available on the internet. After a lot of research and critical cross-checking, a “state of research” that is currently considered to be certain within the scientific community can be identified, which can be regarded as a sufficient basis for good scientific practice.\nWork/read through the following selection of blogs and guides, even for practice purposes.\n\nThe core of GIScience Download The editors Tolpekin & Stein 2012 are providing an excellent insight into GI concepts.\nRobert J. Hijmans rspatial - supervised classification\nIvan Lizarazo RPubs Tutorial\nSydney Goldstein blog\nJoão Gonçalves supervised classification\nValentin Stefan pixel-based supervised classification\n\nIn the articles, you will always find both technical instructions and conceptual or specific technical questions and solutions. They are by no means a substitute for specialized scientific knowledge. But they show how technical and conceptual understanding can be developed step by step and, by “replicating” and applying, support the skills needed to approach questions independently.\nI would like to explicitly quote Valentin Stefan, the author of the blog post pixel-based supervised classification:\n\n\n\n\n\n\n“[…] Consider this content a blog post and nothing more. It does not claim to be an exhaustive exercise or a substitute for your critical thinking […].” :::",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "base/impressum.html#content-responsibility",
    "href": "base/impressum.html#content-responsibility",
    "title": "Impressum",
    "section": "Content Responsibility",
    "text": "Content Responsibility\nThe responsibility for the content rests with the instructors. Statements, opinions and/or conclusions are the ones from the instructors and do not necessarily reflect the opinion of the representatives of Marburg University."
  },
  {
    "objectID": "base/impressum.html#content-license",
    "href": "base/impressum.html#content-license",
    "title": "Impressum",
    "section": "Content License",
    "text": "Content License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\nPrivacy Policy\n\n\nAs of 21. October 2021\n\n\nIntroduction\n\n\nWith the following data protection declaration, we would like to inform you about the types of your personal data (hereinafter also referred to as “data” for short) that we process, for what purposes and to what extent. The privacy policy applies to all processing of personal data carried out by us, both in the context of the provision of our services and in particular on our websites, in mobile applications and within external online presences, such as our social media profiles (hereinafter collectively referred to as “Online Offerings”).\n\n\nThe terms used are not gender-specific.\n\n\nResponsible\n\n\nDr Christoph ReudenbachDeutschhaustr 1035037 Marburg\n\n\nEmail address: reudenbach@uni-marburg.de.\n\n\nImprint: https://www.uni-marburg.de/de/impressum.\n\n\nOverview of Processing\n\n\nThe following overview summarizes the types of data processed and the purposes of their processing, and refers to the data subjects.\n\n\nTypes of Data Processed\n\n\n\nContent data (e.g. input in online forms).\n\n\nContact data (e.g. email, phone numbers).\n\n\nMeta/communication data (e.g. device information, IP addresses).\n\n\nUse data (e.g. websites visited, interest in content, access times).\n\n\n\nCategories of data subjects\n\n\n\nCommunication partners.\n\n\nUsers (e.g.. Website visitors, users of online services).\n\n\n\nPurposes of processing\n\n\n\nDirect marketing (e.g., by email or postal mail).\n\n\nContact requests and communications.\n\n\n\nRelevant legal basis\n\n\nThe following is an overview of the legal basis of the GDPR on the basis of which we process personal data. Please note that in addition to the provisions of the GDPR, national data protection regulations may apply in your or our country of residence or domicile. Furthermore, should more specific legal bases be decisive in individual cases, we will inform you of these in the data protection declaration.\n\n \n\n\nConsent (Art. 6 para. 1 p. 1 lit. a. DSGVO) - The data subject has given his or her consent to the processing of personal data concerning him or her for a specific purpose or purposes.\n\n\nRegistered interests (Art. 6 para. 1 p. 1 lit. f. DSGVO) - Processing is necessary to protect the legitimate interests of the controller or a third party, unless such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require the protection of personal data.\n\n\n\nNational data protection regulations in Germany: In addition to the data protection regulations of the General Data Protection Regulation, national regulations on data protection apply in Germany. These include, in particular, the Act on Protection against Misuse of Personal Data in Data Processing (Federal Data Protection Act - BDSG). In particular, the BDSG contains special regulations on the right to information, the right to erasure, the right to object, the processing of special categories of personal data, processing for other purposes and transmission, as well as automated decision-making in individual cases, including profiling. Furthermore, it regulates data processing for employment purposes (Section 26 BDSG), in particular with regard to the establishment, implementation or termination of employment relationships as well as the consent of employees. Furthermore, state data protection laws of the individual federal states may apply.\n\n \n\nSecurity measures\n\n\nWe take appropriate technical and organizational measures in accordance with the legal requirements, taking into account the state of the art, the implementation costs and the nature, scope, circumstances and purposes of the processing, as well as the different probabilities of occurrence and the extent of the threat to the rights and freedoms of natural persons, in order to ensure a level of protection appropriate to the risk.\n\n.\n\nMeasures include, in particular, ensuring the confidentiality, integrity, and availability of data by controlling physical and electronic access to data as well as access to, entry into, disclosure of, assurance of availability of, and segregation of data concerning them. Furthermore, we have established procedures to ensure the exercise of data subjects’ rights, the deletion of data, and responses to data compromise. Furthermore, we take the protection of personal data into account as early as the development or selection of hardware, software as well as procedures in accordance with the principle of data protection, through technology design and through data protection-friendly default settings.\n\n \n\nDeletion of data\n\n\nThe data processed by us will be deleted in accordance with legal requirements as soon as their consents permitted for processing are revoked or other permissions cease to apply (e.g. if the purpose of processing this data has ceased to apply or it is not necessary for the purpose).\n\n \n\nIf the data are not deleted because they are required for other and legally permissible purposes, their processing will be limited to these purposes. That is, the data will be blocked and not processed for other purposes. This applies, for example, to data that must be retained for reasons of commercial or tax law or whose storage is necessary for the assertion, exercise or defense of legal claims or for the protection of the rights of another natural person or legal entity.\n\n \n\nOur privacy notices may also include further information on the retention and deletion of data that takes precedence for the processing operations in question.\n\n \n\nUse of cookies\n\n\nCookies are text files that contain data from websites or domains visited and are stored by a browser on the user’s computer. The primary purpose of a cookie is to store information about a user during or after their visit within an online site. Stored information may include, for example, language settings on a website, login status, a shopping cart, or where a video was watched. We further include in the term cookies other technologies that perform the same functions as cookies (e.g., when user details are stored using pseudonymous online identifiers, also referred to as “user IDs”)\n\n.\n\nThe following cookie types and functions are distinguished:\n\n\n\nTemporary cookies (also: session or session cookies): Temporary cookies are deleted at the latest after a user has left an online offer and closed his browser.\n\n\nPermanent cookies: Permanent cookies remain stored even after closing the browser. For example, the login status can be saved or preferred content can be displayed directly when the user revisits a website. Likewise, the interests of users used for range measurement or marketing purposes can be stored in such a cookie.\n\n\nFirst-party cookies: First-party cookies are set by ourselves.\n\n\nThird-party cookies (also: third-party cookies): Third-party cookies are mainly used by advertisers (so-called third parties) to process user information.\n\n\nNecessary (also: essential or absolutely necessary) cookies: Cookies may be absolutely necessary for the operation of a website (e.g. to store logins or other user input or for security reasons).\n\n\nStatistics, marketing and personalization cookies: Furthermore, cookies are usually also used in the context of range measurement and when the interests of a user or his behavior (e.g. viewing certain content, use of functions, etc.) on individual web pages are stored in a user profile. Such profiles are used, for example, to show users content that matches their potential interests. This process is also referred to as “tracking”, i.e., tracking the potential interests of users. Insofar as we use cookies or “tracking” technologies, we will inform you separately in our privacy policy or in the context of obtaining consent.\n\n\n\nNotes on legal bases: On which legal basis we process your personal data using cookies depends on whether we ask you for consent. If this is the case and you consent to the use of cookies, the legal basis for the processing of your data is the declared consent. Otherwise, the data processed with the help of cookies is processed on the basis of our legitimate interests (e.g. in a business operation of our online offer and its improvement) or, if the use of cookies is necessary to fulfill our contractual obligations.\n\n.\n\nDuration of storage: If we do not provide you with explicit information about the storage period of permanent cookies (e.g. in the context of a so-called cookie opt-in), please assume that the storage period can be up to two years.\n\n.\n\nGeneral information on revocation and objection (opt-out):  Depending on whether the processing is based on consent or legal permission, you have the option at any time to revoke any consent given or to object to the processing of your data by cookie technologies (collectively referred to as “opt-out”). You can initially declare your objection by means of your browser settings, e.g. by deactivating the use of cookies (whereby this may also restrict the functionality of our online offer). An objection to the use of cookies for online marketing purposes can also be declared by means of a variety of services, especially in the case of tracking, via the websites https://optout.aboutads.info and https://www.youronlinechoices.com/. In addition, you can receive further objection notices in the context of the information on the service providers and cookies used.\n\n.\n\nProcessing of cookie data on the basis of consent: We use a cookie consent management procedure, in the context of which the consent of users to the use of cookies, or the processing and providers mentioned in the cookie consent management procedure can be obtained and managed and revoked by users. Here, the declaration of consent is stored in order not to have to repeat its query and to be able to prove the consent in accordance with the legal obligation. The storage can take place on the server side and/or in a cookie (so-called opt-in cookie, or with the help of comparable technologies), in order to be able to assign the consent to a user or their device. Subject to individual information on the providers of cookie management services, the following information applies: The duration of the storage of consent can be up to two years. Here, a pseudonymous user identifier is formed and stored with the time of consent, information on the scope of consent (e.g., which categories of cookies and/or service providers) as well as the browser, system and end device used.\n\n.\n\n\nTypes of data processed: Usage data (e.g. websites visited, interest in content, access times), meta/communication data (e.g. device information, IP addresses).\n\n\nPersons concerned: Users (e.g. website visitors, users of online services).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nSurveys and polls\n\n\nThe surveys and polls (hereinafter “surveys”) conducted by us are evaluated anonymously. Personal data is only processed insofar as this is necessary for the provision and technical implementation of the surveys (e.g. processing of the IP address to display the survey in the user’s browser or to enable a resumption of the survey with the help of a temporary cookie (session cookie)) or users have consented.\n\n.\n\nNotes on legal basis: If we ask participants for consent to process their data, this is the legal basis of the processing, otherwise the processing of participants’ data is based on our legitimate interests in conducting an objective survey.\n\n \n\n\nTypes of data processed: Contact data (e.g. email, phone numbers), content data (e.g. input in online forms), usage data (e.g. web pages visited, interest in content, access times), meta/communication data (e.g. device information, IP addresses).\n\n\nParticipants concerned: Communication partners.\n\n\nPurposes of processing: Contact requests and communication, direct marketing (e.g. by e-mail or postal mail).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nChange and Update Privacy Policy\n\n\nWe encourage you to periodically review the contents of our Privacy Policy. We adapt the Privacy Policy as soon as the changes in the data processing activities we carry out make it necessary. We will inform you as soon as the changes require an act of cooperation on your part (e.g. consent) or other individual notification.\n\n.\n\nWhere we provide addresses and contact information for companies and organizations in this Privacy Policy, please note that addresses may change over time and please check the information before contacting us.\n\n.\n\nRights of data subjects\n\n\nAs a data subject, you are entitled to various rights under the GDPR, which arise in particular from Art. 15 to 21 DSGVO:\n\n\n\nRight to object: You have the right to object at any time, on grounds relating to your particular situation, to the processing of personal data relating to you which is carried out on the basis of Art. 6(1)(e) or (f) DSGVO; this also applies to profiling based on these provisions. If the personal data concerning you is processed for the purpose of direct marketing, you have the right to object at any time to the processing of personal data concerning you for the purpose of such marketing; this also applies to profiling, insofar as it is associated with such direct marketing.\n\n\nRight of withdrawal in the case of consent: You have the right to withdraw any consent you have given at any time.\n\n\nRight of access: You have the right to request confirmation as to whether data in question is being processed and to information about this data, as well as further information and copy of the data in accordance with the legal requirements.\n\n\nRight of rectification: You have the right, in accordance with the legal requirements, to request the completion of the data concerning you or the correction of incorrect data concerning you.\n\n\nRight to erasure and restriction of processing: You have, in accordance with the law, the right to request that data concerning you be erased without undue delay, or alternatively, in accordance with the law, to request restriction of the processing of the data.\n\n\nRight to data portability: You have the right to receive data concerning you, which you have provided to us, in a structured, common and machine-readable format in accordance with the legal requirements, or to demand its transfer to another responsible party.\n\n\nComplaint to supervisory authority: Without prejudice to any other administrative or judicial remedy, you have the right to lodge a complaint with a supervisory authority, in particular in the Member State of your habitual residence, place of work or the place of the alleged infringement, if you consider that the processing of personal data concerning you infringes the requirements of the GDPR.\n\n\n.\n\nDefinitions of Terms\n\n\nThis section provides you with an overview of the terms used in this Privacy Policy. Many of the terms are taken from the law and defined primarily in Article 4 of the GDPR. The legal definitions are binding. The following explanations, on the other hand, are primarily intended to aid understanding. The terms are sorted alphabetically.\n\n \n\n\nPersonal data: “Personal data” means any information relating to an identified or identifiable natural person (hereinafter “data subject”); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier (eg. e.g. cookie) or to one or more special characteristics that are an expression of the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.\n\n\nController: The “controller” is the natural or legal person, public authority, agency or other body which alone or jointly with others determines the purposes and means of the processing of personal data.\n\n\nProcessing: “Processing” means any operation or set of operations which is performed upon personal data, whether or not by automatic means. The term is broad and includes virtually any handling of data, whether collecting, evaluating, storing, transmitting or deleting.\n\n\n\nCreated with free Datenschutz-Generator.de by Dr. Thomas Schwenke"
  },
  {
    "objectID": "base/impressum.html#comments-suggestions",
    "href": "base/impressum.html#comments-suggestions",
    "title": "Impressum",
    "section": "Comments & Suggestions",
    "text": "Comments & Suggestions"
  },
  {
    "objectID": "base/about.html",
    "href": "base/about.html",
    "title": "About this site",
    "section": "",
    "text": "About this site\nThis page summarizes the essential workflows , basic literature and web resources from the distributed course systems , documents and field protocols into a knowledge base.\nAlthough the web space is topic-centered any keyword can be searched using the full text search.\nThe creation of new pages, the editing of existing pages can be triggered directly via the right column online.\nOffline there are several visual editors and full integration with Rstudio etc."
  },
  {
    "objectID": "base/faq.html",
    "href": "base/faq.html",
    "title": "Frequently asked Questions",
    "section": "",
    "text": "This is a senseless question to meet a meaningfull answer\n\n\n\n\n\n\n\n\n\nThis is a meaningful answer to a senseless question\n\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nThis is a even more meaningful answer to a senseless question"
  },
  {
    "objectID": "base/faq.html#make-sense-topic",
    "href": "base/faq.html#make-sense-topic",
    "title": "Frequently asked Questions",
    "section": "",
    "text": "This is a senseless question to meet a meaningfull answer\n\n\n\n\n\n\n\n\n\nThis is a meaningful answer to a senseless question\n\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nThis is a even more meaningful answer to a senseless question"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intended learning outcomes",
    "section": "",
    "text": "One could claim that the fact living on the surface of the earth and only get to know a small space through direct personal experience is the most important motivation for most of the geographic work. Compensation for this lack of direct experience has been and is being made, especially in scientific geography, with the help of efficient spatio-temporal techniques of abstraction.\nKnowledge of spatial and/or temporal aspects of our environment is increasingly in demand for action-relevant relationships. Whether we ask as tourists, consumers, producers or planners spatial information, or even knowledge.\nGeographic Information Science (GIS) is based on versatile and powerful software tools that are used in modeling, analysis, data mining merging and numerous other spatio-temporal applications. Nevertheless the most powerful tool is our mind developing the concepts and developing the necessary algorithms.\n\nIntended learning outcomes\nAt the end of this course you should be able\n\nto understand, adapt and develop geographic information science methods\nto design workflows suitable to solve common spatio temporal data-related issues\nto deploy your workflows using geo-information science tools, R scripts and collaborative code management platforms for task management and issue tracking\nto critically evaluate your spatio-temporal analysis\nto communicate your workflow and analysis results\n\n\n\nCourse features\nThe course is intended as a blended learning module in our study program although the provided introductions, explanations and examples might be useful for self-study, too.\n\n\nDeliverables\nThe graded course certificate will be based on a team portfolio hosted as a team repository on GitHub. The individual portfolio items are defined in the respective course assignments along with the information if they will be marked or not. Marked portfolio items encompass the presentation and peer-review the paper which inform about the results of two problem solving assignments related to the computation and analysis of the geographic information systems products.\n\n\nPreparation and prerequisites\nThe courses assumes basic knowledge and skills in remote sensing and GIS."
  },
  {
    "objectID": "reader/agisrs-1.html",
    "href": "reader/agisrs-1.html",
    "title": "aGIS Ressources",
    "section": "",
    "text": "del Río, M., Pretzsch, H., Alberdi, I. et al. Characterization of the structure, dynamics, and productivity of mixed-species stands: review and perspectives. Eur J Forest Res 135, 23–49 (2016). https://doi.org/10.1007/s10342-015-0927-6. (VPN necessary)\nEstimating Forest Structure Indices for Evalution of Forest Bird Habitats by an Airborne Laser-Scanner H. Hashimoto, J. Imanishi, A. Hagiwara, Y. Morimoto, K. Kitada Conference Paper\nBarnes, C.; Balzter, H.; Barrett, K.; Eddy, J.; Milner, S.; Suárez, J.C. Individual Tree Crown Delineation from Airborne Laser Scanning for Diseased Larch Forest Stands. Remote Sens. 2017, 9, 231. https://doi.org/10.3390/rs9030231\nJakubowski, M.K.; Li, W.; Guo, Q.; Kelly, M. Delineating Individual Trees from Lidar Data: A Comparison of Vector- and Raster-based Segmentation Approaches. Remote Sens. 2013, 5, 4163-4186. https://doi.org/10.3390/rs5094163\nSpecial Issue Lidar Remote Sensing of Forest Structure, Biomass and Dynamics\n\n\n\n\n\nR — the interpreter can be installed on any operation system.\nRStudio — we recommend to use R Studio for (interactive) programming with R.\nGit environment for your operating system.\nFor Windows users with little experience on the command line we recommend GitHub Desktop.\nRQGIS installation guide.\n\n\n\n\n\nJean-Romain Roussel, David Auty, Nicholas C. Coops, Piotr Tompalski, Tristan R.H. Goodbody, Andrew Sánchez Meador, Jean-François Bourdon, Florian de Boissieu, Alexis Achim, lidR: An R package for analysis of Airborne Laser Scanning (ALS) data, Remote Sensing of Environment, Volume 251, 2020, 112061, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2020.112061.\nThe lidR package book Perfect manual for most lidr related stuff\n\n\n\n\n\nr-bloggers best practices introduction\nUSGS best practices introduction\nworkflowR\nusethis\nMichael Dorman Introduction to Spatial Data Programming with R gives a lot of support for all kind of spatio temporal programming issues\nRobert Hijmans Spatial Data Science with R website provides a valuable insight in the raster and terra package with a lot of tutorials.\nColin Gillespie Efficient R programming If you want to know it really have allook at this book.\n\n\n\n\n\nGeocomputation with R HIGHLY Recommended From the authors trio Lovelace, Nowosad & Muenchow\n\n\n\n\n\nThe core of GIScience Download The editors Tolpekin & Stein 2012 are providing an excellent insight into GI concepts."
  },
  {
    "objectID": "reader/agisrs-1.html#data-readings-and-more",
    "href": "reader/agisrs-1.html#data-readings-and-more",
    "title": "aGIS Ressources",
    "section": "",
    "text": "del Río, M., Pretzsch, H., Alberdi, I. et al. Characterization of the structure, dynamics, and productivity of mixed-species stands: review and perspectives. Eur J Forest Res 135, 23–49 (2016). https://doi.org/10.1007/s10342-015-0927-6. (VPN necessary)\nEstimating Forest Structure Indices for Evalution of Forest Bird Habitats by an Airborne Laser-Scanner H. Hashimoto, J. Imanishi, A. Hagiwara, Y. Morimoto, K. Kitada Conference Paper\nBarnes, C.; Balzter, H.; Barrett, K.; Eddy, J.; Milner, S.; Suárez, J.C. Individual Tree Crown Delineation from Airborne Laser Scanning for Diseased Larch Forest Stands. Remote Sens. 2017, 9, 231. https://doi.org/10.3390/rs9030231\nJakubowski, M.K.; Li, W.; Guo, Q.; Kelly, M. Delineating Individual Trees from Lidar Data: A Comparison of Vector- and Raster-based Segmentation Approaches. Remote Sens. 2013, 5, 4163-4186. https://doi.org/10.3390/rs5094163\nSpecial Issue Lidar Remote Sensing of Forest Structure, Biomass and Dynamics\n\n\n\n\n\nR — the interpreter can be installed on any operation system.\nRStudio — we recommend to use R Studio for (interactive) programming with R.\nGit environment for your operating system.\nFor Windows users with little experience on the command line we recommend GitHub Desktop.\nRQGIS installation guide.\n\n\n\n\n\nJean-Romain Roussel, David Auty, Nicholas C. Coops, Piotr Tompalski, Tristan R.H. Goodbody, Andrew Sánchez Meador, Jean-François Bourdon, Florian de Boissieu, Alexis Achim, lidR: An R package for analysis of Airborne Laser Scanning (ALS) data, Remote Sensing of Environment, Volume 251, 2020, 112061, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2020.112061.\nThe lidR package book Perfect manual for most lidr related stuff\n\n\n\n\n\nr-bloggers best practices introduction\nUSGS best practices introduction\nworkflowR\nusethis\nMichael Dorman Introduction to Spatial Data Programming with R gives a lot of support for all kind of spatio temporal programming issues\nRobert Hijmans Spatial Data Science with R website provides a valuable insight in the raster and terra package with a lot of tutorials.\nColin Gillespie Efficient R programming If you want to know it really have allook at this book.\n\n\n\n\n\nGeocomputation with R HIGHLY Recommended From the authors trio Lovelace, Nowosad & Muenchow\n\n\n\n\n\nThe core of GIScience Download The editors Tolpekin & Stein 2012 are providing an excellent insight into GI concepts."
  },
  {
    "objectID": "reader/agisrs-1.html#data",
    "href": "reader/agisrs-1.html#data",
    "title": "aGIS Ressources",
    "section": "Data",
    "text": "Data\n\nMarburg open Forest database\n\nThe MOF public data of Marburg Open Forest is a comprehensive data base for validation and testing purposes\nThe data server for MOF data. Please ignore insecure site warning."
  },
  {
    "objectID": "reader/digitize.html",
    "href": "reader/digitize.html",
    "title": "Untitled",
    "section": "",
    "text": "With mapedit, each class must be digitized individually. Once the training areas are available as vector data, the features of the respective raster stack can be extracted into a table according to the digitized classes and corrected for possible missing values.\n\n\nFor this exercise, we use mapedit, a small but powerful package that allows you to digitize and edit vector data in Rstudio or an external browser. In combination with mapview, any [color composite] (https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/composites/) can also be used as a basis for digitization.\n\n## ##\n# ---- 0 Projekt Setup ----\nrequire(\"pacman\")\n#remotes::install_github(\"zivankaraman/CDSE\")\n# packages installing if necessary and loading\npacman::p_load(mapview, mapedit, tmap, tmaptools, raster, terra, stars, gdalcubes, sf, dplyr,CDSE, downloader, tidyverse,RStoolbox,rprojroot, exactextractr, randomForest, ranger, e1071, caret, link2GI, rstac, OpenStreetMap,colorspace)\n\n#--- Switch to determine whether digitization is required. If set to FALSE, the\nroot_folder = find_rstudio_root_file()\n\nm1 = tm_shape(pred_stack_2019) + tm_rgb(r=4, g=3, b=2) +\n  tm_layout(legend.outside.position = \"right\",\n            legend.outside = T,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            panel.labels = c(\"r=1, g=2, b=3\")) +\n  tm_grid()\n\nm2 = tm_shape(pred_stack_2019) + tm_rgb(r=8, g=4, b=3) +\n  tm_layout(legend.outside.position = \"right\",\n            legend.outside = T,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            panel.labels = c(\"r=8, g=4, b=3\")) +\n  tm_grid()\ntmap::tmap_arrange(m1,m2)\n\n\nThe planes can be switched using the plane control. In true-color composites, the visible spectral channels Red (B04), Green (B03), and Blue (B02) are mapped to the corresponding red, green, and blue color channels, respectively, producing an image of the surface that closely resembles the natural “color” as it would be seen by a human sitting on the spacecraft. False color images are often created using the spectral channels for near-infrared, red, and green. They are particularly useful for assessing vegetation because plants reflect near-infrared and green light while absorbing red light (red-edge effect). Dense vegetation appears a darker red. Cities and open ground appear gray or light brown, water appears blue or black.\n\n## ##\n\n#---- Digitization of training data ----\n\nif (digitize) {\n# For the supervised classification, we need training areas. You can digitize them as shown below or alternatively use QGis, for example\n\n# clearcut\n\n# For the false color composite r = 8, g = 4, b = 3, maxpixels = 1693870)\n# maxpixels has significantly higher memory requirements, vegetation in red\n# below the true color composite\ntrain_area_2019 &lt;- mapview::viewRGB(pred_stack_2019, r = 4, g = 3, b = 2, maxpixels = 1693870) %&gt;% mapedit::editMap()\n# Adding the attributes class (text) and id/year (integer)\nclearcut_2019 &lt;- train_area_2019$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"clearcut\", id = 1,year=2019)\ntrain_area_2020 &lt;- mapview::viewRGB(pred_stack_2020, r = 4, g = 3, b = 2,maxpixels = 1693870) %&gt;% mapedit::editMap()\nclearcut_2020 &lt;- train_area_2020$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"clearcut\", id = 1,year=2020)\n\n# other: all areas not belonging to clear cutting as representative as possible\ntrain_area_2019 &lt;- mapview::viewRGB(pred_stack_2019, r = 4, g = 3, b = 2) %&gt;% mapedit::editMap()\nother_2019 &lt;- train_area_2019$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"other\", id = 2,year=2019)\ntrain_area_2020 &lt;- mapview::viewRGB(pred_stack_2020, r = 4, g = 3, b = 2) %&gt;% mapedit::editMap()\nother_2020 &lt;- train_area_2020$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"other\", id = 2,year=2020)\n\ntrain_areas_2019_2020 &lt;- rbind(clearcut_2019,clearcut_2020, other_2019,other_2020) # Reproject to the raster file\ntrain_areas_2019 = sf::st_transform(train_areas_2019_2020,crs = sf::st_crs(pred_stack_2019))\nmapview(filter(train_areas_2019_2020,year==2019), zcol=\"class\")\n# save geometries\nst_write(train_areas_2019_2020,paste0(envrmt$path_data,\"train_areas_2019_2020.gpkg\"))\n\n# Extract the training data for the digitized areas\ntDF_2019 = exactextractr::exact_extract(pred_stack_2019, filter(train_areas_2019_2020,year==2019), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\ntDF_2020 = exactextractr::exact_extract(pred_stack_2020, filter(train_areas_2019_2020,year==2020), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n# again, copy together into a file\ntDF_2019 = dplyr::bind_rows(tDF_2019)\ntDF_2019$year = 2019\ntDF_2020 = dplyr::bind_rows(tDF_2020)\ntDF_2020$year = 2020\n# Delete any rows that contain NA (no data) values\ntDF_2019 = tDF_2019[complete.cases(tDF_2019) ,]\ntDF_2020 = tDF_2020[complete.cases(tDF_2020) ,]\n\ntDF= rbind(tDF_2019,tDF_2020)\n\n# check the extracted data\nsummary(tDF)\n\n# Save as R internal data format\n# is stored in the repo and can therefore be loaded (line below)\nsaveRDS(tDF, paste0(envrmt$path_data,\"tDF.rds\"))\n\n\n\n} else {\ntDF = readRDS(paste0(envrmt$path_data,\"tDF.rds\"))\n}\n\nThe result is a table with training data for 2019 and 2020. The data set contains all raster information for all bands covered by the polygons for the classes “clearcut” and “other”.\n\n## ##\nhead(tDF)"
  },
  {
    "objectID": "reader/digitize.html#creating-training-areas-with-mapedit",
    "href": "reader/digitize.html#creating-training-areas-with-mapedit",
    "title": "Untitled",
    "section": "",
    "text": "With mapedit, each class must be digitized individually. Once the training areas are available as vector data, the features of the respective raster stack can be extracted into a table according to the digitized classes and corrected for possible missing values.\n\n\nFor this exercise, we use mapedit, a small but powerful package that allows you to digitize and edit vector data in Rstudio or an external browser. In combination with mapview, any [color composite] (https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/composites/) can also be used as a basis for digitization.\n\n## ##\n# ---- 0 Projekt Setup ----\nrequire(\"pacman\")\n#remotes::install_github(\"zivankaraman/CDSE\")\n# packages installing if necessary and loading\npacman::p_load(mapview, mapedit, tmap, tmaptools, raster, terra, stars, gdalcubes, sf, dplyr,CDSE, downloader, tidyverse,RStoolbox,rprojroot, exactextractr, randomForest, ranger, e1071, caret, link2GI, rstac, OpenStreetMap,colorspace)\n\n#--- Switch to determine whether digitization is required. If set to FALSE, the\nroot_folder = find_rstudio_root_file()\n\nm1 = tm_shape(pred_stack_2019) + tm_rgb(r=4, g=3, b=2) +\n  tm_layout(legend.outside.position = \"right\",\n            legend.outside = T,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            panel.labels = c(\"r=1, g=2, b=3\")) +\n  tm_grid()\n\nm2 = tm_shape(pred_stack_2019) + tm_rgb(r=8, g=4, b=3) +\n  tm_layout(legend.outside.position = \"right\",\n            legend.outside = T,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            panel.labels = c(\"r=8, g=4, b=3\")) +\n  tm_grid()\ntmap::tmap_arrange(m1,m2)\n\n\nThe planes can be switched using the plane control. In true-color composites, the visible spectral channels Red (B04), Green (B03), and Blue (B02) are mapped to the corresponding red, green, and blue color channels, respectively, producing an image of the surface that closely resembles the natural “color” as it would be seen by a human sitting on the spacecraft. False color images are often created using the spectral channels for near-infrared, red, and green. They are particularly useful for assessing vegetation because plants reflect near-infrared and green light while absorbing red light (red-edge effect). Dense vegetation appears a darker red. Cities and open ground appear gray or light brown, water appears blue or black.\n\n## ##\n\n#---- Digitization of training data ----\n\nif (digitize) {\n# For the supervised classification, we need training areas. You can digitize them as shown below or alternatively use QGis, for example\n\n# clearcut\n\n# For the false color composite r = 8, g = 4, b = 3, maxpixels = 1693870)\n# maxpixels has significantly higher memory requirements, vegetation in red\n# below the true color composite\ntrain_area_2019 &lt;- mapview::viewRGB(pred_stack_2019, r = 4, g = 3, b = 2, maxpixels = 1693870) %&gt;% mapedit::editMap()\n# Adding the attributes class (text) and id/year (integer)\nclearcut_2019 &lt;- train_area_2019$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"clearcut\", id = 1,year=2019)\ntrain_area_2020 &lt;- mapview::viewRGB(pred_stack_2020, r = 4, g = 3, b = 2,maxpixels = 1693870) %&gt;% mapedit::editMap()\nclearcut_2020 &lt;- train_area_2020$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"clearcut\", id = 1,year=2020)\n\n# other: all areas not belonging to clear cutting as representative as possible\ntrain_area_2019 &lt;- mapview::viewRGB(pred_stack_2019, r = 4, g = 3, b = 2) %&gt;% mapedit::editMap()\nother_2019 &lt;- train_area_2019$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"other\", id = 2,year=2019)\ntrain_area_2020 &lt;- mapview::viewRGB(pred_stack_2020, r = 4, g = 3, b = 2) %&gt;% mapedit::editMap()\nother_2020 &lt;- train_area_2020$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"other\", id = 2,year=2020)\n\ntrain_areas_2019_2020 &lt;- rbind(clearcut_2019,clearcut_2020, other_2019,other_2020) # Reproject to the raster file\ntrain_areas_2019 = sf::st_transform(train_areas_2019_2020,crs = sf::st_crs(pred_stack_2019))\nmapview(filter(train_areas_2019_2020,year==2019), zcol=\"class\")\n# save geometries\nst_write(train_areas_2019_2020,paste0(envrmt$path_data,\"train_areas_2019_2020.gpkg\"))\n\n# Extract the training data for the digitized areas\ntDF_2019 = exactextractr::exact_extract(pred_stack_2019, filter(train_areas_2019_2020,year==2019), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\ntDF_2020 = exactextractr::exact_extract(pred_stack_2020, filter(train_areas_2019_2020,year==2020), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n# again, copy together into a file\ntDF_2019 = dplyr::bind_rows(tDF_2019)\ntDF_2019$year = 2019\ntDF_2020 = dplyr::bind_rows(tDF_2020)\ntDF_2020$year = 2020\n# Delete any rows that contain NA (no data) values\ntDF_2019 = tDF_2019[complete.cases(tDF_2019) ,]\ntDF_2020 = tDF_2020[complete.cases(tDF_2020) ,]\n\ntDF= rbind(tDF_2019,tDF_2020)\n\n# check the extracted data\nsummary(tDF)\n\n# Save as R internal data format\n# is stored in the repo and can therefore be loaded (line below)\nsaveRDS(tDF, paste0(envrmt$path_data,\"tDF.rds\"))\n\n\n\n} else {\ntDF = readRDS(paste0(envrmt$path_data,\"tDF.rds\"))\n}\n\nThe result is a table with training data for 2019 and 2020. The data set contains all raster information for all bands covered by the polygons for the classes “clearcut” and “other”.\n\n## ##\nhead(tDF)"
  },
  {
    "objectID": "slides/slidelist.html",
    "href": "slides/slidelist.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nChange detection part 1\n\n\nChris Reudenbach\n\n\n\n\nLandscape Metrics Reloaded\n\n\nChris Reudenbach\n\n\n\n\nLandscape Patterns\n\n\nChris Reudenbach\n\n\n\n\nPattern-based spatial analysis\n\n\nChris Reudenbach\n\n\n\n\nSlides and extensions\n\n\ngisma team\n\n\n\n\nSpatial Patterns\n\n\nChris Reudenbach\n\n\n\n\nSpatial patterns of non-categorical rasters\n\n\nChris Reudenbach\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/slides_session2.html#landscape-metrics",
    "href": "slides/slides_session2.html#landscape-metrics",
    "title": "Landscape Patterns",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\n\nProblem no. 1: which of the hundreds of spatial metrics should we choose?\nProblem no. 2: many landscape metrics are highly correlated…",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#possible-approach---pca-of-type-samples-of-landscape-metrics",
    "href": "slides/slides_session2.html#possible-approach---pca-of-type-samples-of-landscape-metrics",
    "title": "Landscape Patterns",
    "section": "Possible Approach - PCA of type samples of landscape metrics",
    "text": "Possible Approach - PCA of type samples of landscape metrics\nWe performed a principal component analysis (PCA) using 17 landscape-level metrics:",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#pca-of-landscape-metrics",
    "href": "slides/slides_session2.html#pca-of-landscape-metrics",
    "title": "Landscape Patterns",
    "section": "PCA of landscape metrics",
    "text": "PCA of landscape metrics\n\n\n\n\n\nPCA 1\n\n\n\n\n\n\nPCA 2\n\n\n\n\n\nFirst two principal components explained ~71% of variability",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#pca-of-landscapes",
    "href": "slides/slides_session2.html#pca-of-landscapes",
    "title": "Landscape Patterns",
    "section": "PCA of landscapes",
    "text": "PCA of landscapes\nThe result allows to distinguish between:\n\nsimple and complex rasters (left&lt;-&gt;right)\nagmented and consolidated rasters (bottom&lt;-&gt;top)\n\n\n\n\nPCA 2\n\n\nHowever, there are still some problems here…",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#pca-of-landscapes-1",
    "href": "slides/slides_session2.html#pca-of-landscapes-1",
    "title": "Landscape Patterns",
    "section": "PCA of landscapes",
    "text": "PCA of landscapes\n\nWe performed a second PCA using data from the United Kingdom only\nNext, we predicted the results on the data for the whole Europe\n\n\n\n\n\n\nPCA 1\n\n\n\n\n\n\nPCA 2",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#pca-of-landscapes-2",
    "href": "slides/slides_session2.html#pca-of-landscapes-2",
    "title": "Landscape Patterns",
    "section": "PCA of landscapes",
    "text": "PCA of landscapes\n\n\n\nPCA 1 PCA 2 UK EU\n\n\nIssues with the PCA approach:\n\nEach new dataset requires recalculation of both, landscape metrics and principal components analysis (PCA)\nHighly correlated landscape metrics are used\nPCA results interpretation is not straightforward",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#it-metrics",
    "href": "slides/slides_session2.html#it-metrics",
    "title": "Landscape Patterns",
    "section": "IT metrics",
    "text": "IT metrics\n\nFive information theory metrics based on a co-occurrence matrix exist (Nowosad and Stepinski, 2019, https://doi.org/10.1007/s10980-019-00830-x)\nMarginal entropy [H(x)] - diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative mutual information [U] - clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns)\nH(x) and U are uncorrelated\n\n\n\n\n\n\nEntropy\n\n\n\n\n\n\nRelative mutual information",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#it-metrics-1",
    "href": "slides/slides_session2.html#it-metrics-1",
    "title": "Landscape Patterns",
    "section": "IT metrics",
    "text": "IT metrics\n2D parametrization of categorical rasters’ configurations based on two weakly correlated IT metrics groups similar patterns into distinct regions of the parameters space",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#it-metrics-final-results",
    "href": "slides/slides_session2.html#it-metrics-final-results",
    "title": "Landscape Patterns",
    "section": "IT metrics final results",
    "text": "IT metrics final results\n\n\n\n\n\nLand cover data\n\n\n\n\n\n\nParametrization of two IT metrics",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#exercises",
    "href": "slides/slides_session2.html#exercises",
    "title": "Landscape Patterns",
    "section": "Exercises",
    "text": "Exercises\n\nThe marginal entropy and relative mutual information can be calculated using the landscapemetrics package’s functions: lsm_l_ent() and lsm_l_relmutinf(). Calculate both of these metrics for the exdata/lc_small.tif raster.\nRead the exdata/lc_europe.tif raster using rast() from the **terra** package and theexdata/polygons.gpkgvector data using theread_sf()function from the **sf** package. Calculate the marginal entropy and relative mutual information for each polygon using thesample_lsm()` function.\nJoin the calculated values with the polygons (see https://r-spatialecology.github.io/landscapemetrics/articles/irregular_areas.html for more details).\nCalculate SHDI and AI for the polygons. Compare the values of SHDI and AI with the marginal entropy and relative mutual information (e.g., using a scatterplot or by calculating the correlation coefficient). Are the results similar?\n(Extra) Create your own polygonal grid using st_make_grid() function from the sf package for the area from the exdata/polygons.gpkg file. Calculate the marginal entropy and relative mutual information for each square using the sample_lsm() function. Visualize the results.",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#it-metrics-2",
    "href": "slides/slides_session2.html#it-metrics-2",
    "title": "Landscape Patterns",
    "section": "IT metrics",
    "text": "IT metrics\nThese metrics still leave some questions open…\n\nRelative mutual information is a result of dividing mutual information by entropy. What to do when the entropy is zero?\nHow to incorporate the meaning of categories into the analysis?\n\n\n\n\nParametrization of two IT metrics\n\n\n\n\n\n&lt;gisma 2023&gt;\n\n\n\nPCA 1\nPCA 2\nPCA 2\nPCA 1\nPCA 2\nPCA 1 PCA 2 UK EU\nEntropy\nRelative mutual information\nLand cover data\nParametrization of two IT metrics\nParametrization of two IT metrics",
    "crumbs": [
      "FAQ",
      "Slides",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#spatial-patterns",
    "href": "slides/slides_session4.html#spatial-patterns",
    "title": "Pattern-based spatial analysis",
    "section": "Spatial Patterns",
    "text": "Spatial Patterns\nIn recent years, the ideas of analyzing spatial patterns have been extended through an approach called pattern-based spatial analysis (Long in in. 2010; Cardille in in. 2010; Cardille in in. 2012; Jasiewicz i in. 2013; Jasiewicz i in. 2015).\nThe fundamental idea is to divide a big area into a large number of smaller areas which we may call local landscapes patches.\n\n\n\nNote: The patch size is depending on spatial data resolution and scale of the landscape\n\n\n\n\n\n\n\nThe idea is to represent each of this arbitrary areas using a statistical description of the spatial pattern - a spatial signature.\n\n\nThis spatial signatures can be compared using a large number of existing distance or dissimilarity measures (Lin 1991; Cha 2007), which enables spatial analyses such as searching, change detection, clustering, or segmentation.",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#spatial-signatures",
    "href": "slides/slides_session4.html#spatial-signatures",
    "title": "Pattern-based spatial analysis",
    "section": "Spatial Signatures",
    "text": "Spatial Signatures\nMost landscape metrics are single numbers representing specific features of a local landscape.\nSpatial signatures, on the other hand, are multi-element representations of landscape composition and configuration.\n\n\n\n\n\nThe basic signature is the co-occurrence matrix:",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#spatial-signatures-dimension-reduction-normalisation",
    "href": "slides/slides_session4.html#spatial-signatures-dimension-reduction-normalisation",
    "title": "Pattern-based spatial analysis",
    "section": "Spatial signatures dimension reduction & normalisation",
    "text": "Spatial signatures dimension reduction & normalisation\n\n\n\n\n\n\n\n\nReduced co-occurrence vector (cove)\n\n\n\n\n\n\n\nNormalized co-occurrence vector(cove)\n\n\n\n\n\n\n\nCo-occurrence vector (cove)",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#dissimilarity-measures-example-1",
    "href": "slides/slides_session4.html#dissimilarity-measures-example-1",
    "title": "Pattern-based spatial analysis",
    "section": "Dissimilarity measures Example 1",
    "text": "Dissimilarity measures Example 1\nMeasuring the distance between two signatures in the form of normalised vectors allows the dissimilarity between spatial structures to be determined. The package [motif] (https://jakubnowosad.com/motif/) is designed to do this work.\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\nJensen-Shannon distance between the above rasters: 0.0684",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#dissimilarity-measures-example-2",
    "href": "slides/slides_session4.html#dissimilarity-measures-example-2",
    "title": "Pattern-based spatial analysis",
    "section": "Dissimilarity measures Example 2",
    "text": "Dissimilarity measures Example 2\nMeasuring the distance between two signatures in the form of normalized vectors allows determining dissimilarity between spatial structures.\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\nJensen-Shannon distance between the above rasters: 0.444",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#pattern-based-spatial-analysis",
    "href": "slides/slides_session4.html#pattern-based-spatial-analysis",
    "title": "Pattern-based spatial analysis",
    "section": "Pattern-based spatial analysis",
    "text": "Pattern-based spatial analysis\nThe distance between spatial signatures provides a powerful possibility to identify (dis)similarities in several contexts.\n\n\n\n\n\n\nfinding similar spatial structures - one to many comparison\n\n\n\n\n\n\n\nquantitative assessment of changes in spatial structures - one to one comparison\n\n\n\n\n\n\n\nclustering similar spatial structures - many to many comparison",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#one-to-many",
    "href": "slides/slides_session4.html#one-to-many",
    "title": "Pattern-based spatial analysis",
    "section": "One to many",
    "text": "One to many\nFinding areas with similar topography to the Suwalski Landscape Park\n\n\n\n\n\n\nTopography Indices\n\n\n\n\n\n\n\nJSD Index",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#one-to-one",
    "href": "slides/slides_session4.html#one-to-one",
    "title": "Pattern-based spatial analysis",
    "section": "One to one",
    "text": "One to one\nThe left maps are showing that many areas in the Amazon have undergone significant land cover changes between 1992 and 2018. The challenge now is to determine which areas have changed the most. The right map shows these areas identified by high JSD values.\n\n\n\nNote that changes in both category and spatial configuration are measured.\n\n\n\n\n\n\n\n\n\nLanduse Types\n\n\n\n\n\n\n\nJSD Index",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#many-to-many",
    "href": "slides/slides_session4.html#many-to-many",
    "title": "Pattern-based spatial analysis",
    "section": "Many to many",
    "text": "Many to many\nAreas in Africa with similar spatial structures for two themes have been identified - land cover and landforms.",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#many-to-many-1",
    "href": "slides/slides_session4.html#many-to-many-1",
    "title": "Pattern-based spatial analysis",
    "section": "Many to many",
    "text": "Many to many\n\n\n\n\n\nThe quality of each cluster can be assessed using metrics:\n\nIntra-cluster heterogeneity: determines distances between all landscapes within a group\nInter-cluster isolation: determines distances between a given group and all others",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#examples",
    "href": "slides/slides_session4.html#examples",
    "title": "Pattern-based spatial analysis",
    "section": "Examples",
    "text": "Examples\ncoma\n\nlibrary(terra)\nlibrary(motif)\nr9 = rast(\"../exdata/r9.tif\")\nr9_sign_coma = lsp_signature(r9, type = \"coma\")\nr9_sign_coma\nr9_sign_coma$signature\n\ncove\n\nr9_sign = lsp_signature(r9, type = \"cove\")\nr9_sign\nr9_sign$signature\n\nsearch\n\nlibrary(sf)\nlandcover = rast(system.file(\"raster/landcover2015s.tif\", package = \"motif\"))\necoregions = read_sf(system.file(\"vector/ecoregionss.gpkg\", package = \"motif\"))\necoregion1 = ecoregions[1, ]\nlandcover1 = crop(landcover, ecoregion1, mask = TRUE)\nplot(landcover)\nplot(landcover1)\n\nsearch_result = lsp_search(landcover1, landcover, \n                           type = \"cove\", dist_fun = \"jensen-shannon\", window = 25,\n                           output = \"sf\")\nplot(search_result[\"dist\"])\n\nmin/max\n\nsearch_result$id[which.min(search_result$dist)]\nsearch_results_75 = lsp_extract(landcover, window = 25, id = 75)\nplot(search_results_75)\n\nsearch_result$id[which.max(search_result$dist)]\nsearch_results_215 = lsp_extract(landcover, window = 25, id = 215)\nplot(search_results_215)",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#exercises",
    "href": "slides/slides_session4.html#exercises",
    "title": "Pattern-based spatial analysis",
    "section": "Exercises",
    "text": "Exercises\n\nRead the study area polygon from the exdata/harz_borders.gpkg file using the read_sf() function from the sf package.\nRead the land cover raster data for Europe from the file exdata/lc_europe.tif using the function rast() from the package terra. Visualise both datasets.\nCrop and mask the raster to the polygon boundaries. Visualise the results.\nCompute a spatial signature for the study area. Can you understand its meaning?\nFind out which areas of the Europe raster are most similar to the study area (this may take a minute or so). Try different window sizes (e.g. 200 or 500).\n\n\n\n\n&lt;gisma 2024&gt;\n\n\n\nReduced co-occurrence vector (cove)\nNormalized co-occurrence vector(cove)\nCo-occurrence vector (cove)\nJensen-Shannon distance between the above rasters: 0.0684\nJensen-Shannon distance between the above rasters: 0.444\nfinding similar spatial structures - one to many comparison\nquantitative assessment of changes in spatial structures - one to one comparison\nclustering similar spatial structures - many to many comparison\nTopography Indices\nJSD Index\nLanduse Types\nJSD Index",
    "crumbs": [
      "FAQ",
      "Slides",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session6.html#retrieving-sentinel-data",
    "href": "slides/slides_session6.html#retrieving-sentinel-data",
    "title": "Change detection part 1",
    "section": "Retrieving Sentinel data",
    "text": "Retrieving Sentinel data"
  },
  {
    "objectID": "slides/slides_session6.html#cloud-optimised-geotiffs-cogs",
    "href": "slides/slides_session6.html#cloud-optimised-geotiffs-cogs",
    "title": "Change detection part 1",
    "section": "Cloud-Optimised GeoTIFFs (COGs)",
    "text": "Cloud-Optimised GeoTIFFs (COGs)\nUnfortunately, the official Sentinel-2 archives are anything but user-friendly. Even with very convenient tools such as sen2r it is sometimes tedious to process them.Technically, the processed product levels are available for download pre-processed as L1C and L2A products in JP2K format. The preferred file format is JP2K, which is storage efficient but has to be downloaded in its entirety locally by the user, resulting in high access costs and huge local storage requirements. The cloud-optimised GeoTIFFs (COGs) allow only the areas of interest to be downloaded and are also much faster to process. However, this requires optimised cloud services and a technically different access logic than in the processing chains used so far."
  },
  {
    "objectID": "slides/slides_session6.html#spatiotemporal-asset-catalog-stac",
    "href": "slides/slides_session6.html#spatiotemporal-asset-catalog-stac",
    "title": "Change detection part 1",
    "section": "SpatioTemporal Asset Catalog (STAC)",
    "text": "SpatioTemporal Asset Catalog (STAC)\nThe [Spatial-Temporal Asset Catalogue] (https://stacspec.org/) (STAC) provides a common language for simplified indexing and discovery of geospatial data. A “Spatio-Temporal Asset” is a file that contains information in a specific space and time.\nThis approach allows any provider of spatio-temporal data (imagery, SAR, point clouds, data cubes, full motion video, etc.) to provide Spatio-Temporal Asset Catalogues (STAC) for their data. STAC focuses on an easy-to-implement standard that organisations can use to make their data available in a durable and reliable way.\nElement84 has provided a public API called Earth-search, a central search catalogue for all public AWS datasets using STAC (including the new Sentinel-2 COGs), which contains more than 11.4 million Sentinel-2 scenes worldwide as of 1 November 2017."
  },
  {
    "objectID": "slides/slides_session6.html#setup-the-working-environment",
    "href": "slides/slides_session6.html#setup-the-working-environment",
    "title": "Change detection part 1",
    "section": "Setup the working environment",
    "text": "Setup the working environment\n\n# adapt your directory this is following the concept of the envimaR structure\ntutorialDir = path.expand(\"data/\")\n\nlibrary(sf)\nlibrary(raster)\nlibrary(tidyverse)\nlibrary(downloader)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(mapview)\nlibrary(gdalcubes)\nlibrary(OpenStreetMap)\nlibrary(stars)\nlibrary(colorspace)\nlibrary(rstac)\n\n\nndvi.col = function(n) {\n  rev(sequential_hcl(n, \"Green-Yellow\"))\n}\n\nano.col = diverging_hcl(7, palette = \"Red-Green\",  register = \"rg\")"
  },
  {
    "objectID": "slides/slides_session6.html#defining-the-area-of-interest",
    "href": "slides/slides_session6.html#defining-the-area-of-interest",
    "title": "Change detection part 1",
    "section": "Defining the Area of Interest",
    "text": "Defining the Area of Interest\nOne major challenge is the fact that most of the earth surface related remote sensing activities are heavily “disturbed” by the atmosphere, especially by clouds. So to find cloud free satellite imagery is a common and cumbersome task. This task is supported by the rstac package which provides a convenient tool to find and filter adequate Sentinel-2 images out of the COG data storage. However, to address the AOI we need to provide the extend via the bbox argument of the corresponding function stac_search(). So first we need to derive and transform the required bounding box to WGS84 geo-coordinates, easily done with the sf functions st_bbox() and st_transform(). In addition we adapt the projection of the referencing vector objects to all other later projection needs.\n::::: boxSuccess \n\nPlease note to project to three different CRS is for this examples convenience and clarity and somewhat superfluous. Only the corner coordinates of the sections are required and not the complete geometries. However, it creates more clarity for the later process to already have the data needed in different projections."
  },
  {
    "objectID": "slides/slides_session6.html#reductions",
    "href": "slides/slides_session6.html#reductions",
    "title": "Change detection part 1",
    "section": "Reductions",
    "text": "Reductions\nPossible reducers include \"min\", \"mean\", \"median\", \"max\", \"count\" (count non-missing values), \"sum\", \"var\" (variance), and \"sd\" (standard deviation). Reducer expressions are always given as a string starting with the reducer name followed by the band name in parentheses. Notice that it is possible to mix reducers and bands.\nCounting\nTo get an idea of the data, we can compute simple summary statistics applying a reducer function over dimensions. For example using the count reducer we learn about the temporal coverage for each aggregated pixel and hence an initial understanding of the data set temporal quality.\n\nncdf_cube(file.path(tutorialDir,\"MOF_10.nc\")) |&gt;\n  reduce_time(\"count(B04)\") |&gt; \n  plot(key.pos = 1, zlim=c(25,45), col = viridis::viridis, nbreaks = 10)\n\nWe can see that most time series contain valid observations about 40 months, which should be sufficient for our example. Similarly, it is also possible to reduce over space, leading to summary time series.\nBelow you will find various examples dealing with the common NDVI and the kNDVI Indices.\nkNDVI\nBelow, we derive mean monthly kNdvi values over all pixel time series.\n\nncdf_cube(file.path(tutorialDir,\"MOF_10.nc\")) |&gt;\n    apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  reduce_time(\"mean(kNDVI)\") |&gt;\n  plot(key.pos = 1,  col = ndvi.col, nbreaks = 12)"
  },
  {
    "objectID": "slides/slides_session6.html#ndvi",
    "href": "slides/slides_session6.html#ndvi",
    "title": "Change detection part 1",
    "section": "NDVI",
    "text": "NDVI\nBelow, we derive mean monthly Ndvi values over all pixel time series.\n\nncdf_cube(file.path(tutorialDir,\"MOF_10.nc\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(\"mean(NDVI)\") |&gt;\n  plot(key.pos = 1, zlim=c(-0.2,1), col = ndvi.col, nbreaks = 12)\n\nZonal Statistics\n\nzstats = ncdf_cube(file.path(tutorialDir,\"MOF_10.nc\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  extract_geom(forest_32632,FUN = median) \n\nforest_32632$FID = rownames(forest_32632)\nx = merge(forest_32632, zstats, by = \"FID\")\nmapview(x[x$time == \"2018-07\", \"NDVI\"],col.regions = ndvi.col)\n\nTimeseries\n\nncdf_cube(file.path(tutorialDir,\"MOF_10.nc\")) |&gt;\n  apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  reduce_space(\"min(kNDVI)\", \"max(kNDVI)\", \"mean(kNDVI)\") |&gt;\n  plot(join.timeseries = TRUE)\n\nCalculate the annual NDVI difference - using functions\nThe following function will iterate through 3 years of NDVI data and calculating NDVI difference maps for each pair of years.\n\ngdalcubes_options(parallel =  12)\n\n# ndvi operand\nndvi_type = \"median(kNDVI)\"\n\n# time slots as tibble\nts = tibble(t0 = c(\"2018-04-01\",\"2019-04-01\",\"2020-04-01\",\"2021-04-01\"), \n            t1= c(\"2018-09-01\",\"2019-09-01\",\"2020-09-01\",\"2021-09-01\"))\n\n# names of the time slots\nnames_ndvi = paste(ts$t0,ts$t1,sep = \" to \")\n\nbasic_kndvi &lt;- function(fncube=NULL, t0=NULL, t1=NULL) {\n  ncdf_cube(fncube) |&gt; \n    select_bands(c(\"B04\", \"B08\")) |&gt;\n    select_time(c(t0,t1)) |&gt;\n  apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n    reduce_time(ndvi_type)\n}\n\n# (stac-search -&gt; stac and cloud filter image collection -&gt; create cube -&gt; call user function)\nndvi = list()\nfor (i in 1:nrow(ts)){\n  # call user function\n  basic_kndvi(fncube = file.path(tutorialDir,\"MOF_10.nc\"), t0 = ts$t0[i], t1 = ts$t1[i]) %&gt;%\n    st_as_stars() -&gt; ndvi[[i]]\n  \n}\n\n# now we may create a mask according to the NDVI extent and resolution\nstars::write_stars(ndvi[[1]] * 0, paste0(tutorialDir,\"forest/only_forestmask.tif\"))\ngi=link2GI::linkGDAL(searchLocation = \"/usr/bin/\")\ncmd=gi$bin[[1]]$gdal_bin[14]\nsystem(paste(cmd ,'-burn 1.0 -tr 10.0 10.0 -a_nodata 0.0 -te ', st_bbox(ndvi[[1]])$xmin,' ',st_bbox(ndvi[[1]])$ymin,' ',st_bbox(ndvi[[1]])$xmax,' ',st_bbox(ndvi[[1]])$ymax,' -ot Float32 -of GTiff', paste0(tutorialDir,\"forest/MOF_mask_pr.shp \"),paste0(tutorialDir,\"forest/only_forestmask.tif\")))\nmask &lt;- raster(paste0(tutorialDir,\"forest/only_forestmask.tif\"))\nplot(mask)                                \n\nmask[mask == 0] = NA\n\n# mapping the results\ntmap_mode(\"plot\")\ntm_ndvi = lapply(seq(2:length(names_ndvi) -1),function(i){\n  m = tm_shape(osm_forest) + tm_rgb() +\n    tm_shape((ndvi[[i]] - ndvi[[i+1]]) * st_as_stars(mask )) +\n    tm_raster(title = ndvi_type,pal =diverging_hcl(11, \"rg\")) +\n    tm_layout(panel.labels = paste(\"Difference \",names_ndvi[[i]],\"/\",names_ndvi[[i+1]]),\n              legend.show = TRUE,\n              panel.label.color = \"darkblue\",\n              panel.label.size =0.5,\n              panel.label.height=1.2,\n              legend.text.size = 0.3,\n              legend.outside = TRUE) +\n    tm_grid()\n})\n\n\n# tmap_arrange(tm_ndvi,nrow = 1,asp = NA,widths = c(0.33,0.33,0.33),outer.margins = 0.001)\n\nResulting maps"
  },
  {
    "objectID": "slides/slides_session6.html#use-case-spatial-identification-of-magnitudes-and-time-periods-of-kndvi-changes",
    "href": "slides/slides_session6.html#use-case-spatial-identification-of-magnitudes-and-time-periods-of-kndvi-changes",
    "title": "Change detection part 1",
    "section": "Use case: Spatial identification of magnitudes and time periods of kNDVI changes",
    "text": "Use case: Spatial identification of magnitudes and time periods of kNDVI changes\nTo apply a more complex time series method such as bfastmonitor(), the data cube operations below allow to provide custom user-defined R functions instead of string expressions, which translate to built-in reducers. It is very important that these functions receive arrays as input and must return arrays as output, too. Depending on the operation, the dimensionality of the arrays is different:\n\n\n\n\n\n\n\n\nOperator\nInput\nOutput\n\n\n\n\napply_pixel\nVector of band values for one pixel\nVector of band values of one pixel\n\n\nreduce_time\nMulti-band time series as a matrix\nVector of band values\n\n\nreduce_space\nThree-dimensional array with dimensions bands, x, and y\nVector of band values\n\n\napply_time\nMulti-band time series as a matrix\nMulti-band time series as a matrix\n\n\n\nThere is almost no limit of what R function we use, but we must take care of a few things: 1. The reducer function is executed in a new R process without access to the current workspace. It is not possible to access variables defined outside of the function and packages must be loaded within the function. 2. The reducer function must always return a vector with the same length (for all time series). 3. It is a good idea to think about NA values, i.e. you should check whether the complete time series is NA, and that missing values do not produce errors.\nAnother possibility to apply R functions to data cubes is of course to convert data cubes to stars objects and use the stars package for further analysis.\nApplying bfastmonitor as a user-defined reducer function\nIn our example, bfastmonitor returns change date and change magnitude values per time series so we can use reduce_time(). The script below (1) calculates the kNDVI, (2) applies bfastmonitor(), and properly handles errors e.g. due to missing data with tryCatch(), and (3) finally writes out the resulting change dates and magnitudes of change for all pixels of the time series as a netCDF file. The results shows the changes starting at 1/2019 until 12/2021 and identify pretty well the dynamical impacts of drought and bark beetle in the Marburg University Forest (MOF).\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(colorspace)\nndvi.col = function(n) {\n  rev(sequential_hcl(n, \"Green-Yellow\"))\n}\ntutorialDir = path.expand(\"~/edu/agis/doc/data/tutorial/\")\n\nfigtrim &lt;- function(path) {\n  img &lt;- magick::image_trim(magick::image_read(path))\n  magick::image_write(img, path)\n  path\n}\ngdalcubes_options(parallel = 12)\n## start analysis\nsystem.time(\n  ncdf_cube(file.path(tutorialDir,\"MOF_10.nc\")) |&gt;\n    reduce_time(names = c(\"change_date\", \"change_magnitude\",\"kndvi\"), FUN = function(x) {\n      kndvi = tanh(((x['B08',]-x['B04',])/(x['B08',]+x['B04',]))^2)\n      if (all(is.na(kndvi))) {\n        return(c(NA,NA))\n      }\n      kndvi_ts = ts(kndvi, start = c(2017, 1), frequency = 12)\n      library(bfast)\n      tryCatch({\n        result = bfastmonitor(kndvi_ts, start = c(2020,1), \n                              history = \"all\", level = 0.01)\n        return(c(result$breakpoint, result$magnitude))\n      }, error = function(x) {\n        return(c(NA,NA))\n      })\n    }) |&gt;\n    write_ncdf(paste0(tutorialDir,\"bf_results.nc\"),overwrite = TRUE))\n\nNow we can use the netCDF file and map the results with any preferred visualisation tool. In this case tmap.\n\nlibrary(tmap)\n\nmask &lt;- raster(paste0(tutorialDir,\"forest/only_forestmask.tif\"))\n# plotting it from the local ncdf  \ntmap_mode(\"view\")\ngdalcubes::ncdf_cube(paste0(tutorialDir,\"bf_results.nc\")) |&gt;\n  stars::st_as_stars() -&gt; x\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[1] * st_as_stars(mask )) + \n  tm_raster(n = 6)  +\n  tm_layout(\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid()\n\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[2]* st_as_stars(mask ))  + tm_raster() +\n  tm_layout(legend.title.size = 1,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            legend.text.size = 0.4,\n            legend.outside = TRUE) +\n  tm_grid()\n\n\n\n\nMagnitude Change Map\n\n\n\n\n\nPeriod of Change Map\n\nRunning bfastmonitor() is computationally expensive. However, since in common (not in our case) the data should be located in the cloud, it would be obvious to launch one of the (payed) more powerful machine instance types with many processors. Parallelization within one instance can be controlled entirely by gdalcubes using gdalcubes_options() which is extremely simple.\nFrom a scientific point of view we need to tune some aspects. The kNDVI Index together with the bfastmonitor approach is somewhat sophisticated and need a validation strategy. Also the correlation to the different tree species could be promising. So there is certainly a need for more analysis to understand the processes and to identify false results."
  },
  {
    "objectID": "slides/slides_session6.html#minimum-temperature-c",
    "href": "slides/slides_session6.html#minimum-temperature-c",
    "title": "Change detection part 1",
    "section": "Minimum temperature (°C)",
    "text": "Minimum temperature (°C)\n\n\n\n\n\nCMIP6 downscaled future climate projection for 2061-2080 [model: CNRM-ESM2-1; ssp: “585”]\n\n\n\n\n\n\nWorldClim version 2.1 climate data for 1970-2000\n\n\n\n\n\n\n\n&lt;gisma 2024&gt;\n\n\n\nCMIP6 downscaled future climate projection for 2061-2080 [model: CNRM-ESM2-1; ssp: “585”]\nWorldClim version 2.1 climate data for 1970-2000"
  },
  {
    "objectID": "worksheets/ws-00.html",
    "href": "worksheets/ws-00.html",
    "title": "Worksheets HowTo",
    "section": "",
    "text": "The Working Sheets will provide weekly assignments related to the general task of reaching out for the course goals.\n\nMandatory assignments (Studienleistung) will be marked as \nGraded examinations (Prüfungsleistung) will be marked as"
  },
  {
    "objectID": "worksheets/ws-02.html",
    "href": "worksheets/ws-02.html",
    "title": "Worksheet 1: Warm Up Exercises",
    "section": "",
    "text": "Read the data from exdata/lc_small.tif and visualize it. What is the location of the data? What are the extent of the data and its spatial resolution? How many categories it contains?\nCalculate Aggregation Index (AI) for the raster. Interpret the results.\nCalculate Total Edge (TE) for the raster. Interpret the results. Next, read the data from `exdata/lc_small2.tif, calculate AI and TE for this raster, and compare the results with the previous raster.\nCalculate Total Edge (TE) for the raster, but this time in a moving window of 9 by 9 cells. Visualize the results.\n(Extra) Using the read_sf() function from the sf package, read the exdata/points.gpkg file. Next, calculate SHDI and AI of an area of 3000 meters from each sampling point (see the sample_lsm() function).",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 1: Warm Up Exercises"
    ]
  },
  {
    "objectID": "worksheets/ws-02.html#exercises-ws-1",
    "href": "worksheets/ws-02.html#exercises-ws-1",
    "title": "Worksheet 1: Warm Up Exercises",
    "section": "",
    "text": "Read the data from exdata/lc_small.tif and visualize it. What is the location of the data? What are the extent of the data and its spatial resolution? How many categories it contains?\nCalculate Aggregation Index (AI) for the raster. Interpret the results.\nCalculate Total Edge (TE) for the raster. Interpret the results. Next, read the data from `exdata/lc_small2.tif, calculate AI and TE for this raster, and compare the results with the previous raster.\nCalculate Total Edge (TE) for the raster, but this time in a moving window of 9 by 9 cells. Visualize the results.\n(Extra) Using the read_sf() function from the sf package, read the exdata/points.gpkg file. Next, calculate SHDI and AI of an area of 3000 meters from each sampling point (see the sample_lsm() function).",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 1: Warm Up Exercises"
    ]
  },
  {
    "objectID": "worksheets/ws-04.html",
    "href": "worksheets/ws-04.html",
    "title": "Worksheet 3: Landscape metrics part 2",
    "section": "",
    "text": "Please navigate to the landscapemetrics package.\nRun the following tutorials with the example data of this course:\n\n\n\n\n\n\n\nSample lsm\nIrregular Areas\nUtilities\n\n\n\n\nFor the irregular vector data download the OSM data e.g. from Geofabrik and use the category landuse. Note you need to merge Niedersachsen and Sachsen-Anhalt\n\n\n\n\n\n\n\n\n\nDownload the fragstats tutorial Analyzing a single grid.\nRun the tutorial but use landscapemetrics instead",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 3: Landscape metrics part 2"
    ]
  },
  {
    "objectID": "worksheets/ws-04.html#exercises-ws-3",
    "href": "worksheets/ws-04.html#exercises-ws-3",
    "title": "Worksheet 3: Landscape metrics part 2",
    "section": "",
    "text": "Please navigate to the landscapemetrics package.\nRun the following tutorials with the example data of this course:\n\n\n\n\n\n\n\nSample lsm\nIrregular Areas\nUtilities\n\n\n\n\nFor the irregular vector data download the OSM data e.g. from Geofabrik and use the category landuse. Note you need to merge Niedersachsen and Sachsen-Anhalt\n\n\n\n\n\n\n\n\n\nDownload the fragstats tutorial Analyzing a single grid.\nRun the tutorial but use landscapemetrics instead",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 3: Landscape metrics part 2"
    ]
  },
  {
    "objectID": "worksheets/ws-06.html",
    "href": "worksheets/ws-06.html",
    "title": "Worksheet 5: Regionalisation",
    "section": "",
    "text": "Regionalize Germany’s climates.\n\nUse Worldclim versus CMIP data\nUse the upper helper functions and code for Great Britain\n\n\n\n\n\n\n\n\n\n\nMake sure you get an idea of a “scientific” climate zoning concept.\nThe following links provide an initial random selection of some thoughts on this concept.\n\nClimate Extremes are Becoming More Frequent, Co-occurring, and Persistent in Europe\nA systematic review of GIS-based local climate zone mapping studies\nEuropean summer weather regimes 1990-2019: Automatic classification and representation in a small - global climate model ensemble\n\nWhat are the main aims of the following articles in relation to climate zones?\nHow do you assess the proposed solution using supercells etc. in comparison?\nUse the LCZ-generator and compare it to your results.",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 5: Regionalisation"
    ]
  },
  {
    "objectID": "worksheets/ws-06.html#exercises-ws-5",
    "href": "worksheets/ws-06.html#exercises-ws-5",
    "title": "Worksheet 5: Regionalisation",
    "section": "",
    "text": "Regionalize Germany’s climates.\n\nUse Worldclim versus CMIP data\nUse the upper helper functions and code for Great Britain\n\n\n\n\n\n\n\n\n\n\nMake sure you get an idea of a “scientific” climate zoning concept.\nThe following links provide an initial random selection of some thoughts on this concept.\n\nClimate Extremes are Becoming More Frequent, Co-occurring, and Persistent in Europe\nA systematic review of GIS-based local climate zone mapping studies\nEuropean summer weather regimes 1990-2019: Automatic classification and representation in a small - global climate model ensemble\n\nWhat are the main aims of the following articles in relation to climate zones?\nHow do you assess the proposed solution using supercells etc. in comparison?\nUse the LCZ-generator and compare it to your results.",
    "crumbs": [
      "FAQ",
      "Worksheets",
      "Worksheet 5: Regionalisation"
    ]
  }
]