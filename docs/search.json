[
  {
    "objectID": "reader/mc3.html",
    "href": "reader/mc3.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Let’s take a closer look at proximity, which is mentioned frequently. What exactly is it? How can proximity/neighborliness be expressed in such a way that the space becomes meaningful?\nIn general, spatial relationships are described in terms of neighborhoods (positional) and distances (metric). In spatial analysis or prediction, however, it is important to be able to name the spatial influence, i.e. the evaluation or weighting of this relationship, either qualitatively or quantitatively. Tobler did this for a specific objective by stating that “near” is more important than “far”. But what about in other cases? The challenge is that spatial influence can only be measured directly in exceptional cases. There are many ways to estimate it, however.\n\n\nNeighborhood is perhaps the most important concept. Higher dimensional geo-objects can be considered neighboring if they touch each other, e.g. neighboring countries. For zero-dimensional objects (points), the most common approach is to use distance in combination with a number of points to determine neighborhood.\n\n\n\nProximity or neighborhood analyses are often concerned with areas of influence or catchment areas, i.e. spatial patterns of effects or processes.\nThis section discusses some methods for calculating distances between spatial objects. Because of the different ways of discretizing space, we must make the – already familiar – distinction between vector and raster data models.\nInitially, it is often useful to work without spatially restrictive conditions in a first analysis, e.g. when this information is missing. The term “proximity” inherently implies a certain imprecision. Qualitative terms that can be used for this are: “near”, “far” or “in the neighborhood of”. Representation and data-driven analysis require these terms to be objectified and operationalized. So, this metric must be based on a distance concept, e.g. Euclidean distance or travel times. In a second interpretative step, we must decide which units define this type of proximity. In terms of the objective of a question, there are only suitable and less-suitable measures; there is no correct or incorrect. Therefore, it is critical to define a meaningful neighborhood relationship for the objects under investigation.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Filling Gaps"
    ]
  },
  {
    "objectID": "reader/mc3.html#distance-and-data-representation",
    "href": "reader/mc3.html#distance-and-data-representation",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Let’s take a closer look at proximity, which is mentioned frequently. What exactly is it? How can proximity/neighborliness be expressed in such a way that the space becomes meaningful?\nIn general, spatial relationships are described in terms of neighborhoods (positional) and distances (metric). In spatial analysis or prediction, however, it is important to be able to name the spatial influence, i.e. the evaluation or weighting of this relationship, either qualitatively or quantitatively. Tobler did this for a specific objective by stating that “near” is more important than “far”. But what about in other cases? The challenge is that spatial influence can only be measured directly in exceptional cases. There are many ways to estimate it, however.\n\n\nNeighborhood is perhaps the most important concept. Higher dimensional geo-objects can be considered neighboring if they touch each other, e.g. neighboring countries. For zero-dimensional objects (points), the most common approach is to use distance in combination with a number of points to determine neighborhood.\n\n\n\nProximity or neighborhood analyses are often concerned with areas of influence or catchment areas, i.e. spatial patterns of effects or processes.\nThis section discusses some methods for calculating distances between spatial objects. Because of the different ways of discretizing space, we must make the – already familiar – distinction between vector and raster data models.\nInitially, it is often useful to work without spatially restrictive conditions in a first analysis, e.g. when this information is missing. The term “proximity” inherently implies a certain imprecision. Qualitative terms that can be used for this are: “near”, “far” or “in the neighborhood of”. Representation and data-driven analysis require these terms to be objectified and operationalized. So, this metric must be based on a distance concept, e.g. Euclidean distance or travel times. In a second interpretative step, we must decide which units define this type of proximity. In terms of the objective of a question, there are only suitable and less-suitable measures; there is no correct or incorrect. Therefore, it is critical to define a meaningful neighborhood relationship for the objects under investigation.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Filling Gaps"
    ]
  },
  {
    "objectID": "reader/mc3.html#filling-spatial-gaps",
    "href": "reader/mc3.html#filling-spatial-gaps",
    "title": "Spatial Interpolation",
    "section": "Filling spatial gaps",
    "text": "Filling spatial gaps\nNow that we have learned the basic concepts of distance, neighborhood and filling spatial gaps, let’s take a look at interpolating or predicting values in space.\nFor many decades, deterministic interpolation techniques (inverse distance weighting, nearest neighbor, kriging) have been the most popular spatial interpolation techniques. External drift kriging and regression kriging, in particular, are fundamental techniques that use spatial autocorrelation and covariate information, i.e. sophisticated regression statistics.\nMachine learning algorithms like random forest have become very popular for spatial environmental prediction. One major reason for this is that they are can take into account non-linear and complex relationships, i.e. compensate for certain disadvantages that are present in the usual regression methods.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Filling Gaps"
    ]
  },
  {
    "objectID": "reader/mc3.html#proximity-concepts",
    "href": "reader/mc3.html#proximity-concepts",
    "title": "Spatial Interpolation",
    "section": "Proximity concepts",
    "text": "Proximity concepts\n\nVoronoi polygons – dividing space geometrically\nVoronoi polygons (aka Thiessen polygons) are an elementary method for geometrically determining proximity or neighborhoods. Voronoi polygons (see figure below) divide an area into regions that are closest to a given point in a set of irregularly distributed points. In two dimensions, a Voronoi polygon encompasses an area around a point, such that every spatial point within the Voronoi polygon is closer to this point than to any other point in the set. Such constructs can also be formed in higher dimensions, giving rise to Voronoi polyhedra.\n&lt;/frame&gt;\n&lt;iframe width=\"780\" height=\"1280\" src=\"https://geomoer.github.io/geoAI//assets/images/unit01/suisse6.html\" title=\"Interpol\"&gt;\n\n\nThe blue dots are a typical example of irregularly distributed points in space – in this case, rain gauges in Switzerland. The overlaid polygons are the corresponding Voronoi segments that define the corresponding closest geometrical areas (gisma 2021)“\n\n\nSince Voronoi polygons correspond to an organizational principle frequently observed in both nature (e.g. plant cells) and in the spatial sciences (e.g. central places , according to Christaller), there are manifold possible applications. Two things must be assumed, however: First, that nothing else is known about the space between the sampled locations and, second, that the boundary line between two samples is incomplete idea.\nVoronoi polygons can also be used to delineate catchment areas of shops, service facilities or wells, like in the example of the Soho cholera outbreak. Please note that within a polygon, one of the spatial features is isomorphic, i.e. the spatial features are identical.\nBut what if we know more about the spatial relationships of the features? Let’s have a look at some crucial concepts.\n\n\nSpatial interpolation of data\nSpatially interpolating data points provides us with a modeled quasi-continuous estimation of features under the corresponding assumptions. But what is spatial interpolation? Essentially, this means using known values to calculate neighboring values that are unknown. Most of these techniques are among the most complex methods of spatial analysis, so we will deliberately limit ourselves here to a basic overview of the methods. Some of the best-known and common interpolation methods found in spatial sciences are nearest neighbor inverse distance, spline interpolations, kriging, and regression methods.\n\n\nContinously filling the gaps by interpolation\nTo get started, take a look at the following figure, which shows six different interpolation methods to derive the spatial distribution of precipitation in Switzerland (in addition to the overlaid Voronoi tessellation).\n\n\n\nThe blue dots are a typical example of irregularly distributed points in space – in this case, rain gauges in Switzerland. The size of each dot corresponds to the amount of precipitation in mm. The overlaid polygons are the corresponding Voronoi segments that define the corresponding closest geometrical areas (gisma 2021)” top left: Nearest neighbor interpolation based on 3-5 nearest neighbors, top right: Inverse Distance weighting (IDW) interpolation method middle left: AutoKriging with no additional parameters, middle right: Thin plate spline regression interpolation method bottom left: Triangular irregular net (TIN) surface interpolation, bottom right: additive model (GAM) interpolation\n\n\nIn the example of precipitation in Switzerland, the positions of the weather stations are fixed and cannot be freely chosen.\nWhen choosing an appropriate interpolation method, we need to pay attention to several properties of the samples (distribution and properties of the measurement points):\n\nRepresentativeness of measurement points: The sample should represent the phenomenon being analyzed in all of its manifestations.\nHomogeneity of measurement points: The spatial interdependence of the data is a very important basic requirement for further meaningful analysis.\nSpatial distribution of measurement points: The spatial distribution is of great importance. It can be completely random, regular or clustered.\nNumber of measurement points: The number of measurement points depends on the phenomenon and the area. In most cases, the choice of sample size is subject to practical limitations.\n\nWhat makes things even more complex is that these four factors – representativeness, homogeneity, spatial distribution and size – are all interrelated. For example, a sample size of 5 measuring stations for estimating precipitation for all of Switzerland is hardly meaningful and therefore not representative. Equally unrepresentative would be selecting every measuring station in German-speaking Switzerland to estimate precipitation for the entire country. In this case, the number alone might be sufficient, but the spatial distribution would not be. If we select every station at an altitude below 750 m asl, the sample could be correct in terms of both size and spatial distribution, but the phenomenon is not homogeneously represented in the sample. An estimate based on this sample would be clearly distorted, especially in areas above 750 m asl. In practice, virtually every natural spatially-continuous phenomenon is governed by stochastic fluctuations, so, mathematically speaking, it can only be described in approximate terms.\n\n\nMachine learning\nMachine learning (ML) methods such as random forest can also produce spatial and temporal predictions (i.e. produce maps from point observations). These methods are particularly robust because they take spatial autocorrelation into account, which can improve predictions or interpolations by adding geographic distances. This ultimately leads to better maps with much more complex relationships and dependencies.\nIn the simplest case, the results are comparable to the well-known model-based geostatistics. The advantage of ML methods over model-based geostatistics, however, is that they make fewer assumptions, can take non-linearities into account and are easier to automate.\n\n\n\nThe original dataset (top left) is a terrain model reduced to 8 meters with 48384 single pixels. For interpolation, 1448 points were randomly drawn and interpolated with conventional kriging (top right), support vector machines (SVM) (middle left), neural networks (middle right), and two variants of random forest (bottom row). In each method, only the distance of the drawn points is used as a dependency.\n\n\nEach interpolation method was applied using the “default” settings. Tuning could possibly lead to significant changes in all of them. Fascinatingly, the error measures correlate to the visual results: Kriging and the neural network show the best performance, followed by the random forest models and the support-vector machine.\n\n\n\nmodel\ntotal_error\nmean_error\nsd_error\n\n\n\n\nKriging\n15797773.0\n54.2\n67.9\n\n\nNeural Network\n19772241.0\n67.8\n80.5\n\n\nRandom Forest\n20540628.1\n70.4\n82.5\n\n\nNormalized Random Forest\n20597969.8\n70.6\n82.7\n\n\nSupport Vector Machine\n21152987.7\n72.5\n68.3\n\n\n\n\n\nAdditional references\nGet the Most Out of AI, Machine Learning, and Deep Learning Part 1 (10:52) and Part 2 (13:18)\nWhy You Should NOT Learn Machine Learning! (6:17)\nGeoAI: Machine Learning meets ArcGIS (8:50)",
    "crumbs": [
      "FAQ",
      "Reader",
      "Filling Gaps"
    ]
  },
  {
    "objectID": "reader/mc3.html#hands-on-our-data",
    "href": "reader/mc3.html#hands-on-our-data",
    "title": "Spatial Interpolation",
    "section": "Hands on our data",
    "text": "Hands on our data\nDownload the exercises repository as a ZIP file from and unpack it on your computer.\nThen, you can open the .Rproj file and start working on the exercises in RStudio.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Filling Gaps"
    ]
  },
  {
    "objectID": "reader/mc3.html#further-hands-on-examples",
    "href": "reader/mc3.html#further-hands-on-examples",
    "title": "Spatial Interpolation",
    "section": "Further Hands on examples",
    "text": "Further Hands on examples\nClimate Data Inmterpolation Hands on The Forgenius Pinus Pinaster Project provides an fully integrated GIS source code and field data dealing with prediction classificaten of UAV and station realted data.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Filling Gaps"
    ]
  },
  {
    "objectID": "reader/cd-1.html",
    "href": "reader/cd-1.html",
    "title": "Change Detection - Sentinel",
    "section": "",
    "text": "In the geosciences, remote sensing is the only measurement technique that allows complete coverage of large spatial areas, up to the entire Earth’s surface. Its successful application requires both the use of existing methods and the adaptation and development of new ones.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#introduction",
    "href": "reader/cd-1.html#introduction",
    "title": "Change Detection - Sentinel",
    "section": "Introduction",
    "text": "Introduction\nIn geospatial or environmental informatics, the detection of changes to the Earth’s surface using satellite, aircraft or drone images, known as change detection analysis, is an important application. These results are often linked to biophysical, geophysical or anthropogenic processes in order to gain both a deeper understanding and the possibility of developing predictive models. Methods of image analysis are of outstanding importance for generating spatial information from the underlying processes. Since both the quantity and quality of this “image data” are playing an increasingly important role in environmental monitoring and modeling, it is becoming more and more necessary to integrate “big data” concepts into the analyses. This means performing reproducible analyses with large amounts of data (&gt;&gt; 1 TB). This is essential for both scientific knowledge gain and future societal challenges.\nAs already explained in the introduction, we start with a scalable change detection analysis of forest damage in low mountain ranges, which is a typical application-oriented task. Scalable means that we limit the analysis to a manageable area, the Nordwestharz, and to two time slices. However, the resulting algorithm can be applied to different or larger areas and to more time slices.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#goals",
    "href": "reader/cd-1.html#goals",
    "title": "Change Detection - Sentinel",
    "section": "Goals",
    "text": "Goals\nThis example shows how change detection methods can be applied conventionally to individual satellite scenes and in a modern way in cloud computing environments using rstac (Brazil Data Cube Team 2021) and gdalcubes (Appel and Pebesma 2019) or openeo (Lahn 2024). In addition to classical supervised classification methods such as Maximum Likelihood and Random Forest, the bfast (Verbesselt, Zeileis, and Herold 2012) is used, which includes an unsupervised method for detecting structural breaks in vegetation index time series.\nOther packages used in this tutorial include stars (Pebesma 2019), tmap (Tennekes 2018) and mapview (Appelhans et al. 2019) for creating interactive maps, sf (Pebesma 2018) for processing vector data, and colorspace (Zeileis et al. 2020) for visualizations with accessible colors.\nThis study employs a variety of approaches to time series and difference analyses with different indices, using the Harz Mountains as a case study for the period between 2018 and 2023. The objective is to analyze or classify the data.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#information-from-satellite-imagery",
    "href": "reader/cd-1.html#information-from-satellite-imagery",
    "title": "Change Detection - Sentinel",
    "section": "Information from satellite imagery",
    "text": "Information from satellite imagery\nUnprocessed satellite images are not necessarily informative. While our eyes can interpret a true-color image relatively conclusively and intuitively, a reliable and reproducible, i.e. scientifically sound, interpretation requires other approaches. A major advantage of typical image analysis methods over visual interpretation is the derivation of additional, so-called invisible information.\nTo obtain useful or meaningful information, e.g. about the land cover in an area, we have to analyze the data according to the question at hand. Probably the best known and most widely used approach is the supervised classification of image data into categories of interest.\nIn this unit, you will learn about the classification of satellite image data. This includes both data acquisition on the Copernicus portal and the various steps from digitising the training data to evaluating the quality of the classifications.\nWe will cover the following topics:\n\nTheoretical principles\nCase Study Harz Mountains\n\nPreparing the work environment\nRetrieving Sentinel and auxilliary data\nUnsupervised classification (k-means clustering)\nRecording training areas.\nSupervised classification (Random Forest, Maximum Likelihood)\nEstimating model quality",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#theoretical-principles",
    "href": "reader/cd-1.html#theoretical-principles",
    "title": "Change Detection - Sentinel",
    "section": "Theoretical principles",
    "text": "Theoretical principles\nPlease note that all types of classification usually require extensive data pre-processing. The focus is then on model building and quality assessment, which can be seen as the technical basis for classification, in order to finally derive the interpretation of the results in terms of content in the data post-processing.\nWe will go through this process step by step.\n\nUnsupervised Classification - k-means clustering\nProbably the best-known unsupervised classification technique is K-means clustering, which is also referred to as the “simplest machine learning algorithm”.\nK-means clustering is a technique commonly used in satellite image classification to group pixels with similar spectral characteristics. Treating each pixel as an observation, the algorithm assigns pixels to clusters based on their spectral values, with each cluster having a mean (or centroid) that represents its central spectral signature. This results in the segmentation of the image into distinct regions (similar to Voronoi cells) corresponding to land cover types, such as water, vegetation or urban areas, facilitating further analysis. It is often used to obtain an initial overview of whether the raster data can be sufficiently separated in feature space.\n\nFigure: Convergence of k-means clustering from an unfavorable starting position (two initial cluster centers are fairly close). Chire [CC BY-SA 4.0] via wikipedia.org\n\n\nSupervised classification\nIn supervised land cover classification, a model is derived from a limited amount of training land cover data that predicts land cover for the entire data set. The land cover types are defined a priori, and the model attempts to predict these types based on the similarity between the characteristics of the training data and the rest of the data set.\n Classifiers (e.g. the maximum likelihood classifier) or machine learning algorithms (such as Random Forest) use the training data to determine descriptive models that represent statistical signatures, classification trees or other functions. Within the limits of the quality of the training data, such models are suitable and representative for making predictions for areas if the predictors from the model are available for the entire area.\nWe now want to predict the spatial characteristics of clear-felling/no forest using a maximum likelihood classification and random forest, and apply standard methods of random validation and model quality assessment.\nThe goal is to separate clearcuts from all other pixels and to quantify the differences between 2018 and 2022.\n\nMaximum Likelihood Classification\nMaximum likelihood classification assumes that the distribution of data for each class and in each channel is normally distributed. Under this assumption, the probability that a particular pixel belongs to a particular class is calculated. Since the probabilities can also be specified as a threshold, without this restriction, all pixels are assigned regardless of how unlikely they are. Each pixel is assigned to the class that has the highest probability (i.e., the maximum probability).\n\n\n\nRandom forest\nRandom forests can be used for both regression and classification tasks, with the latter being particularly relevant in environmental remote sensing. Like any machine learning method, the random forest model learns to recognize patterns and structures in the data itself. Since the random forest algorithm also requires training data, it is also a supervised learning method. !\nFigure: Simplified illustration of data classification by random forest during training. Venkata Jagannath [CC BY-SA 4.0] via wikipedia.org\nA random forest algorithm learns from the data by creating random decision trees – hence the name. For classification tasks, the algorithm takes a suitable instance of a decision tree from the training data set and assigns the corresponding class to the pixel. This is repeated with all available decision trees. Finally, the pixel is assigned to the class that has the most trees, according to the winner-takes-all principle.\nFrom a pragmatic point of view, classification tasks generally require the following steps:\n\nCreation of a comprehensive input data set that contains one or more raster layers.\nSelection of training areas, i.e. subsets of the input data set for which the land cover type is known to the remote sensing expert. Knowledge of the land cover can be obtained, for example, from one’s own or third-party in situ observations, management information or other remote sensing products (e.g. high-resolution aerial photographs).\nTraining a model using the training sites. For validation purposes, the training sites are often subdivided into one or more test and training sites to evaluate the performance of the model algorithm.\nApplying the trained model to the entire data set, i.e. predicting the land cover type based on the similarity of the data at each site to the class characteristics of the training data set.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#change-detection-case-study-harz-mountains",
    "href": "reader/cd-1.html#change-detection-case-study-harz-mountains",
    "title": "Change Detection - Sentinel",
    "section": "Change detection case study: Harz Mountains",
    "text": "Change detection case study: Harz Mountains\nSince 2018, there has been a notable increase in the incidence of extensive forest dieback in the Harz Mountains. During this period, the combination of repeated years of drought, extreme heat waves and the resulting weakening of the spruce trees led to an exponential increase in the population of bark beetles. The combined impact of these factors resulted in the extensive mortality of spruce stands across an area of approximately 30,000 hectares over a period of approximately five to six years. This equates to approximately 35% of the total forest area of the Harz.\nIt is important to note that the prolonged drought in 2018, 2019 and 2020, which is considered one of the most severe in the region, has significantly exacerbated the damage in the Harz Mountains.\nIn this context, a time series analysis or a change detection analysis is an essential technique for quantifying and localising the damage and characterising its dynamics.\n\nSetting up the work environment\nHowever, the work environment is usually loaded first.\n\n# ---- 0 Projekt Setup ----\nrequire(\"pacman\")\n#remotes::install_github(\"zivankaraman/CDSE\")\n# packages installing if necessary and loading\npacman::p_load(mapview, mapedit, tmap, tmaptools, raster, terra, stars, gdalcubes, sf,webshot, dplyr,CDSE,webshot, downloader, tidyverse,RStoolbox,rprojroot, exactextractr, randomForest, ranger, e1071, caret, link2GI, rstac, OpenStreetMap,colorspace,ows4R,httr)\n#--- Switch to determine whether digitization is required. If set to FALSE, the\nroot_folder = find_rstudio_root_file()\n\nndvi.col = function(n) {\n  rev(sequential_hcl(n, \"Green-Yellow\"))\n}\n\nano.col = diverging_hcl(7, palette = \"Red-Green\",  register = \"rg\")\n\nPlease add any missing or defective packages in the above setup script (if error messages occur). On the basis of the available Sentinel data, the first step should be to identify suitable data sets for a surface classification.\n\n\nDefining the Area of Interest\nPlease note to project to different CRS is for this examples convenience and clarity and somewhat superfluous. Only the corner coordinates of the sections are required and not the complete geometries. However, it creates more clarity for the later process to already have the data needed in different projections.\n\n\n# download the Harz region from the UBA WFS server\n# nre_regions &lt;- \"https://geodienste.bfn.de/ogc/wfs/gliederungen?\"\n# regions_client &lt;- WFSClient$new(nre_regions, \n#                             serviceVersion = \"2.0.0\",)\n# regions_client$getFeatureTypes(pretty = TRUE)\n# \n# url &lt;- parse_url(nre_regions)\n# url$query &lt;- list(service = \"wfs\",\n#                   #version = \"2.0.0\", # optional\n#                   request = \"GetFeature\",\n#                   typename = \"Haupteinheiten\",\n#                   srsName = \"EPSG:4326\"\n#                   )\n# request &lt;- build_url(url)\n# \n# nre_regions_sf &lt;- read_sf(request)\n# harz = nre_regions_sf |&gt; \n#   filter(NAME_ORD3 %in% c( \"Oberharz\")) \n# plot(harz)\n# harz_bbox = bb(harz,projection=4326)\n# harz_32632 =sf::st_transform(harz,crs = 32632)\n# harz_bbox_32632 = bb(harz_32632,projection=32632)\n\n# Download training data which is also used for the extend\nutils::download.file(url=\"https://github.com/gisma/gismaData/raw/master/geoinfo/train_areas_2019_2020.gpkg\",destfile=file.path(\"../data/train_areas_2019_2020.gpkg\"))\ntrain_areas_2019_2020 = st_read(file.path(\"../data/train_areas_2019_2020.gpkg\"))\n\nReading layer `train_areas_2019_2020' from data source \n  `/home/creu/edu/gisma-courses/LV-19-d19-006-24/data/train_areas_2019_2020.gpkg' \n  using driver `GPKG'\nSimple feature collection with 87 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 10.21106 ymin: 51.84931 xmax: 10.39679 ymax: 51.94446\nGeodetic CRS:  WGS 84\n\ntrain_areas_2019_2020  |&gt;\n  st_bbox() -&gt; bbox\n\n# mapping the extents and boundaries of the choosen geometries\n# tmap_mode(\"view\")\n# tmap_options(check.and.fix = TRUE) + \n#   tm_basemap(server = c(\"Esri.WorldGrayCanvas\", \"OpenStreetMap\", \"Esri.WorldTopoMap\",\"Esri.WorldImagery\")) +\n#   #tm_shape(harz) +   \n#   tm_polygons(alpha = 0.4)\n\n#--- Reading the data from the directories\n\n##--- This describes how to process the Corine land use and land cover dataset\n## The necessary file can also be downloaded from the repository\n## An account is required for the download https://land.copernicus.eu/pan-european/corine-land-cover\n## Therefore, download the data manually and unzip \n## U2018_CLC2018_V2020_20u1.tif into the data directory \n## Then continue\nif (!file.exists(file.path(root_folder,\"data/corine_harz.tif\"))){\n  corine = rast(file.path(\"../data/U2018_CLC2018_V2020_20u1.tif\"))\n  corine = terra::project(corine,\"EPSG:4326\" )\n  corine_harz = terra::crop(corine,vect(train_areas_2019_2020))\n  terra::writeRaster(corine_harz,file.path(root_folder,\"data/corine_harz.tif\"),overwrite=TRUE)\n  }\ncorine_harz = rast(file.path(root_folder,\"data/corine_harz.tif\"))\n# Create a forest mask from corine\n# Agro-forestry areas code=22, Broad-leaved forest code=23,\n# Coniferous forest code=24, Mixed forest code=25\nm &lt;- c(-100, 22, 0,\n       22, 26, 1,\n       26, 500, 0)\nrclmat &lt;- matrix(m, ncol=3, byrow=TRUE)\nharz_forest_mask &lt;- classify(corine_harz, rclmat, include.lowest=TRUE)\n\n\nmapview(corine_harz)+mapview(train_areas_2019_2020,zcol=\"class\")+harz_forest_mask\n\n\n\n\n# create extents\ne &lt;- ext(harz_forest_mask)\np &lt;- as.polygons(e, crs=\"EPSG:4326\")\ncm=sf::st_as_sf(p)\nharz_bbox = bb(train_areas_2019_2020,projection=4326)",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-1-retrieving-sentinel-data",
    "href": "reader/cd-1.html#step-1-retrieving-sentinel-data",
    "title": "Change Detection - Sentinel",
    "section": "Step 1: Retrieving Sentinel data",
    "text": "Step 1: Retrieving Sentinel data\nSentinel-2 is currently the most important platform for Earth observation in all areas, but especially for climate change, land use and ecological issues at all levels, from the upper micro to the global scale.\nThere are two operational Sentinel-2 satellites: Sentinel-2A and Sentinel-2B, both in sun-synchronous polar orbits and 180 degrees out of phase. This arrangement allows them to cover the mid-latitudes with an orbital period of about 5 days.\nThe Sentinel-2 data are therefore predestined to record spatial and temporal changes on the Earth’s surface (the forest was green in early summer, it has disappeared by late summer). They are ideal for timely studies before and after natural disasters, or for biomass balancing, etc.\n\nAternative 1: Using gdalcubes\nThe gdalcubes tool has been developed with the objective of facilitating the processing of extensive collections of satellite images. It has been designed to enhance the efficiency, speed, intuitiveness and interactivity of this task.\n\nCloud-Optimised GeoTIFFs (COGs)\nUnfortunately, the official Sentinel-2 archives are anything but user-friendly. Technically, the processed product levels are available for download pre-processed as L1C and L2A products in JP2K format. The preferred file format is JP2K, which is storage efficient but has to be downloaded in its entirety locally by the user, resulting in high access costs and huge local storage requirements. The cloud-optimised GeoTIFFs (COGs) allow only the areas of interest to be downloaded and are also much faster to process. However, this requires optimised cloud services and a technically different access logic than in the traditional processing chains used so far.\n\n\nSpatioTemporal Asset Catalog (STAC)\nThe [Spatial-Temporal Asset Catalogue] (https://stacspec.org/) (STAC) provides a common language for simplified indexing and discovery of geospatial data. A “Spatio-Temporal Asset” is a file that contains information in a specific space and time.\nThis approach allows any provider of spatio-temporal data (imagery, SAR, point clouds, data cubes, full motion video, etc.) to provide Spatio-Temporal Asset Catalogues (STAC) for their data. STAC focuses on an easy-to-implement standard that organisations can use to make their data available in a durable and reliable way.\nElement84 has provided a public API called Earth-search, a central search catalogue for all public AWS datasets using STAC (including the new Sentinel-2 COGs), which contains more than 11.4 million Sentinel-2 scenes worldwide as of 1 November 2017.\nOne major challenge is the fact that most of the earth surface related remote sensing activities are heavily “disturbed” by the atmosphere, especially by clouds. So to find cloud free satellite imagery is a common and cumbersome task. This task is supported by the rstac package which provides a convenient tool to find and filter adequate Sentinel-2 images out of the COG data storage. However, to address the AOI we need to provide the extend via the bbox argument of the corresponding function stac_search(). So first we need to derive and transform the required bounding box to WGS84 geo-coordinates, easily done with the sf functions st_bbox() and st_transform(). In addition we adapt the projection of the referencing vector objects to all other later projection needs.\n\n\nQuerying images with rstac\nUsing the rstac package, we first request all available images from 2018 to 208 that intersect with our region of interest. Here, since the polygon has WGS84 as CRS, we do not need to transform the bounding box before using the stac_search() function.\n\n# search the data stack for the given period and area\ns = stac(\"https://earth-search.aws.element84.com/v0\")\n\n\nitems &lt;- s |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox= c(bbox[\"xmin\"],bbox[\"ymin\"],bbox[\"xmax\"],bbox[\"ymax\"]), , \n              datetime = c(\"2018-06-01/2022-09-01\"),\n              limit = 600) |&gt;\n  post_request() \nitems\n\n###Items\n- matched feature(s): 626\n- features (600 item(s) / 26 not fetched):\n  - S2A_32UNC_20220901_0_L2A\n  - S2A_32UNC_20220829_0_L2A\n  - S2B_32UNC_20220827_0_L2A\n  - S2B_32UNC_20220824_0_L2A\n  - S2A_32UNC_20220822_0_L2A\n  - S2A_32UNC_20220819_1_L2A\n  - S2B_32UNC_20220817_0_L2A\n  - S2B_32UNC_20220814_0_L2A\n  - S2A_32UNC_20220812_0_L2A\n  - S2A_32UNC_20220809_0_L2A\n  - ... with 590 more feature(s).\n- assets: \nAOT, B01, B02, B03, B04, B05, B06, B07, B08, B09, B11, B12, B8A, info, metadata, overview, SCL, thumbnail, visual, WVP\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, properties.sentinel:boa_offset_applied, stac_extensions, stac_version, type\n\n\nThis gives us 334 matching images recorded between 2019-06-01 and 2021-09-01.\n\n\nCreating a monthly Sentinel-2 data cube\nTo obtain a Sentinel data cube, a gdalcube image collection must be created from the STAC query result. To do this, the asset names must be explicitly named in order to apply the SCL channel with the quality characteristics per pixel (classification as clouds, cloud shadows, etc.). In this query, a filter is set to cloud cover &lt;= 50%.\n\ns2_collection &lt;- stac_image_collection(items$features,\n                                      asset_names = \nc(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\", \"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"SCL\"), \n                                      property_filter = function(x) {x[[\"eo:cloud_cover\"]] &lt; 5}) \ns2_collection\n\nImage collection object, referencing 78 images with 12 bands\nImages:\n                      name     left      top   bottom    right\n1 S2B_32UNC_20220824_0_L2A 9.494450 52.34699 51.35264 10.61137\n2 S2A_32UNC_20220812_0_L2A 8.999721 52.35046 51.35264 10.61137\n3 S2A_32UNC_20220623_0_L2A 8.999721 52.35046 51.35264 10.61137\n4 S2B_32UNC_20220618_0_L2A 8.999721 52.35046 51.35264 10.61137\n5 S2B_32UNC_20220615_0_L2A 9.471038 52.34716 51.35264 10.61137\n6 S2B_32UNC_20220519_1_L2A 8.999721 52.35046 51.35264 10.61137\n             datetime        srs\n1 2022-08-24T10:26:32 EPSG:32632\n2 2022-08-12T10:36:42 EPSG:32632\n3 2022-06-23T10:36:42 EPSG:32632\n4 2022-06-18T10:36:34 EPSG:32632\n5 2022-06-15T10:26:36 EPSG:32632\n6 2022-05-19T10:36:29 EPSG:32632\n[ omitted 72 images ] \n\nBands:\n   name offset scale unit nodata image_count\n1   B01      0     1                      78\n2   B02      0     1                      78\n3   B03      0     1                      78\n4   B04      0     1                      78\n5   B05      0     1                      78\n6   B06      0     1                      78\n7   B07      0     1                      78\n8   B08      0     1                      78\n9   B09      0     1                      78\n10  B11      0     1                      78\n11  B8A      0     1                      78\n12  SCL      0     1                      78\n\n\nThe result is 118 images, i.e. approx. 1.8 images per month, from which we can now create a data cube. To do this, we use the UTM bounding box of our polygon as a spatial boundary, a spatial resolution of 10 metres, a bilinear spatial interpolation (useful for the spatially lower-resolution sentinel channels) and calculate monthly median values for all pixel values from the available images of a month. In addition, we add a buffer (b) on each side of the cube.\nThe gdalcube image collection can be considered as a proxy structure object which will be applied on the COGs.\n\nst_as_sfc(bbox) |&gt;\n  st_transform(\"EPSG:32632\") |&gt;\n  st_bbox() -&gt; bbox_utm\nv = cube_view(srs = \"EPSG:32632\", extent = list(t0 = \"2018-06\", t1 = \"2022-09\", left = bbox_utm[\"xmin\"] - 10, right = bbox_utm[\"xmax\"] + 10, bottom = bbox_utm[\"ymin\"] - 10, top = bbox_utm[\"ymax\"] + 10),\n              dx = 10, dy = 10, dt = \"P1M\", aggregation = \"median\", resampling = \"bilinear\")\n\nv\n\nA data cube view object\n\nDimensions:\n               low             high count pixel_size\nt       2018-06-01       2022-09-30    52        P1M\ny  5744956.9172092  5755796.9172092  1084         10\nx 583230.473549458 596220.473549458  1299         10\n\nSRS: \"EPSG:32632\"\nTemporal aggregation method: \"median\"\nSpatial resampling method: \"bilinear\"\n\n\nNext we create a data cube, subset the red and near infrared bands and crop by our polygon, which simply sets pixel values outside the polygon to NA. We then save the data cube as a single netCDF file. Note that this is not necessary, but saving intermediate results sometimes makes debugging easier, especially if the methods applied afterwards are computationally intensive.\nOnly calling a final action will start the processing on the COG-Server. In this case ‘write_ncdf’.\n\n## ##\n#| \n# we \"download\" the data and write it t a netcdf file\n  s2.mask = image_mask(\"SCL\", values = c(3,8,9))\n  gdalcubes_options(parallel = 16, \n                    ncdf_compression_level = 5)\n  raster_cube(s2_collection, v, mask = s2.mask) |&gt;\n    write_ncdf(file.path(root_folder,\"../data/harz_2018_2022_all.nc\"),overwrite=TRUE)\n\n\n\nkNDVI\nBelow, we derive mean monthly kNDVI values over all pixel time series.\n\nncdf_cube(file.path(root_folder,\"../data/harz_2018_2022_all.nc\")) |&gt;\n    apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  reduce_time(\"mean(kNDVI)\") |&gt;\n  plot(key.pos = 1,  col = ndvi.col(11), nbreaks = 12)\n\n\n\n\n\n\n\n\n\n\n\nAlternative 2: Copernicus Data Space Ecosystem API Wrapper CDSE\nThe CDSE package provides another simple way to get Sentinel/Copernicus data sets.\n\n#------------\n# NOTE: You must create an Copernicus account and provide the token credtials have a look at:\n#       https://zivankaraman.github.io/CDSE/articles/BeforeYouStart.html#accessing-cdse-data-and-services\n#------------\n\nid &lt;- Sys.getenv(\"CDSE_ID\")\nsecret &lt;- Sys.getenv(\"CDSE_SECRET\")\nOAuthClient &lt;- GetOAuthClient(id = id, secret = secret)\ncollections &lt;- GetCollections(as_data_frame = TRUE)\ncollections\n\n                  id                title\n1     sentinel-2-l1c       Sentinel 2 L1C\n2 sentinel-3-olci-l2   Sentinel 3 OLCI L2\n3    sentinel-3-olci      Sentinel 3 OLCI\n4   sentinel-3-slstr     Sentinel 3 SLSTR\n5     sentinel-1-grd       Sentinel 1 GRD\n6     sentinel-2-l2a       Sentinel 2 L2A\n7     sentinel-5p-l2 Sentinel 5 Precursor\n                                                   description\n1                     Sentinel 2 imagery processed to level 1C\n2 Sentinel 3 data derived from imagery captured by OLCI sensor\n3                   Sentinel 3 imagery captured by OLCI sensor\n4                  Sentinel 3 imagery captured by SLSTR sensor\n5                     Sentinel 1 Ground Range Detected Imagery\n6                     Sentinel 2 imagery processed to level 2A\n7      Sentinel 5 Precursor imagery captured by TROPOMI sensor\n                 since instrument  gsd bands constellation long.min lat.min\n1 2015-11-01T00:00:00Z        msi   10    13    sentinel-2     -180     -56\n2 2016-04-17T11:33:13Z       olci  300    NA          &lt;NA&gt;     -180     -85\n3 2016-04-17T11:33:13Z       olci  300    21          &lt;NA&gt;     -180     -85\n4 2016-04-17T11:33:13Z      slstr 1000    11          &lt;NA&gt;     -180     -85\n5 2014-10-03T00:00:00Z      c-sar   NA    NA    sentinel-1     -180     -85\n6 2016-11-01T00:00:00Z        msi   10    12    sentinel-2     -180     -56\n7 2018-04-30T00:18:50Z    tropomi 5500    NA          &lt;NA&gt;     -180     -85\n  long.max lat.max\n1      180      83\n2      180      85\n3      180      85\n4      180      85\n5      180      85\n6      180      83\n7      180      85\n\nimages &lt;- SearchCatalog(bbox = st_bbox(train_areas_2019_2020), from = \"2018-05-01\", to = \"2022-12-31\", \n    collection = \"sentinel-2-l2a\", with_geometry = TRUE, client = OAuthClient)\n\nimages\n\nSimple feature collection with 689 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 8.999721 ymin: 51.35264 xmax: 10.61137 ymax: 52.35046\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   acquisitionDate tileCloudCover areaCoverage   satellite\n1       2022-12-30         100.00          100 sentinel-2a\n2       2022-12-27          79.15          100 sentinel-2a\n3       2022-12-25          99.99          100 sentinel-2b\n4       2022-12-22          99.45          100 sentinel-2b\n5       2022-12-20          97.60          100 sentinel-2a\n6       2022-12-17          29.56          100 sentinel-2a\n7       2022-12-15           3.80          100 sentinel-2b\n8       2022-12-10          95.63          100 sentinel-2a\n9       2022-12-07          96.07          100 sentinel-2a\n10      2022-12-05          99.99          100 sentinel-2b\n   acquisitionTimestampUTC acquisitionTimestampLocal\n1      2022-12-30 10:36:29       2022-12-30 11:36:29\n2      2022-12-27 10:26:31       2022-12-27 11:26:31\n3      2022-12-25 10:36:29       2022-12-25 11:36:29\n4      2022-12-22 10:26:31       2022-12-22 11:26:31\n5      2022-12-20 10:36:28       2022-12-20 11:36:28\n6      2022-12-17 10:26:31       2022-12-17 11:26:31\n7      2022-12-15 10:36:28       2022-12-15 11:36:28\n8      2022-12-10 10:36:30       2022-12-10 11:36:30\n9      2022-12-07 10:26:33       2022-12-07 11:26:33\n10     2022-12-05 10:36:27       2022-12-05 11:36:27\n                                                            sourceId long.min\n1  S2A_MSIL2A_20221230T103431_N0509_R108_T32UNC_20221230T134707.SAFE 8.999721\n2  S2A_MSIL2A_20221227T102431_N0509_R065_T32UNC_20221227T140052.SAFE 9.476927\n3  S2B_MSIL2A_20221225T103349_N0509_R108_T32UNC_20221225T114808.SAFE 8.999721\n4  S2B_MSIL2A_20221222T102339_N0509_R065_T32UNC_20221222T113435.SAFE 9.485113\n5  S2A_MSIL2A_20221220T103441_N0509_R108_T32UNC_20221220T134756.SAFE 8.999721\n6  S2A_MSIL2A_20221217T102431_N0509_R065_T32UNC_20221217T141955.SAFE 9.478651\n7  S2B_MSIL2A_20221215T103339_N0509_R108_T32UNC_20221215T114317.SAFE 8.999721\n8  S2A_MSIL2A_20221210T103431_N0509_R108_T32UNC_20221210T142357.SAFE 8.999721\n9  S2A_MSIL2A_20221207T102411_N0509_R065_T32UNC_20221207T135807.SAFE 9.476926\n10 S2B_MSIL2A_20221205T103319_N0400_R108_T32UNC_20221205T113923.SAFE 8.999721\n    lat.min long.max  lat.max                       geometry\n1  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n2  51.35264 10.61137 52.34711 POLYGON ((9.889693 52.34711...\n3  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n4  51.35264 10.61137 52.34705 POLYGON ((9.898795 52.34705...\n5  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n6  51.35264 10.61137 52.34710 POLYGON ((9.891455 52.3471,...\n7  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n8  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n9  51.35264 10.61137 52.34711 POLYGON ((9.890282 52.34711...\n10 51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n\nsummary(images$areaCoverage)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.48  100.00  100.00   99.83  100.00  100.00 \n\n# best 30 days without clouds\nday &lt;- images[order(images$tileCloudCover), ]$acquisitionDate[1:30]\n\n# read specific processing scripts\nscript_file_raw = system.file(\"scripts\", \"RawBands.js\", package = \"CDSE\")\nscript_file_savi = \"savi.js\"\nscript_file_kndvi = \"kndvi.js\"\nscript_file_evi = \"evi.js\"\n\n# first day 2018_07_01\nraw_2018_07_01 = GetImage(bbox = st_bbox(train_areas_2019_2020), \n                          time_range = day[12], \n                          script = script_file_raw, \n                          collection = \"sentinel-2-l2a\", \n                          format = \"image/tiff\", \n                          mosaicking_order = \"leastCC\",\n                          resolution = 10, \n                          mask = TRUE, \n                          buffer = 0.01, \n                          client = OAuthClient)\nnames(raw_2018_07_01) = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")\nterra::plot(raw_2018_07_01, main = paste(names(raw_2018_07_01), day[12]), cex.main = 0.75)\n\n\n\n\n\n\n\nkndvi_2018_07_01 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020),\n                             time_range = day[12], \n                             script = script_file_kndvi, \n                             collection = \"sentinel-2-l2a\", \n                             format = \"image/tiff\", \n                             mosaicking_order = \"leastCC\", \n                             resolution = 10, \n                             mask = TRUE, \n                             buffer = 0.01, \n                             client = OAuthClient)\nnames(kndvi_2018_07_01 )[1] = c(\"kNDVI\")\nmapview(kndvi_2018_07_01[[1]] , main = paste(names(kndvi_2018_07_01 ), day[12]), col = ndvi.col(11),nbreaks = 12, cex.main = 0.75)\n\n\n\n\nsavi_2018_07_01 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                            time_range = day[12], \n                            script = script_file_savi, \n                            collection = \"sentinel-2-l2a\", \n                            format = \"image/tiff\", \n                            mosaicking_order = \"leastCC\", \n                            resolution = 10,\n                            mask = TRUE, \n                            buffer = 0.01, \n                            client = OAuthClient)\nnames(savi_2018_07_01)[1] = c( \"SAVI\")\nterra::plot(savi_2018_07_01[[1]], main = paste(\"SAVI\", day[12]),col = ndvi.col(11), nbreaks = 12, cex.main = 0.75)\n\n\n\n\n\n\n\nevi_2018_07_01 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                           time_range = day[12], \n                           script = script_file_evi, \n                           collection = \"sentinel-2-l2a\", \n                           format = \"image/tiff\", \n                           mosaicking_order = \"leastCC\", \n                           resolution = 10, \n                           mask = TRUE, \n                           buffer = 0.01, \n                           client = OAuthClient)\nnames(evi_2018_07_01)[1] = c( \"EVI\")\nterra::plot(evi_2018_07_01[[1]], main = paste(\"EVI\",day[12]), col = ndvi.col(11), nbreaks = 12, cex.main = 0.75)\n\n\n\n\n\n\n\npred_stack_2018 = c(raw_2018_07_01,evi_2018_07_01[[1]],kndvi_2018_07_01[[1]],savi_2018_07_01[[1]])\n\n## second day 2022_06_23\nraw_2022_06_23 = GetImage(bbox = st_bbox(train_areas_2019_2020), \n                          time_range = day[1], \n                          script = script_file_raw, \n                          collection = \"sentinel-2-l2a\", \n                          format = \"image/tiff\", \n                          mosaicking_order = \"leastCC\", \n                          resolution = 10, \n                          mask = TRUE, \n                          buffer = 0.01, \n                          client = OAuthClient)\nnames(raw_2022_06_23) = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")\nterra::plot(raw_2022_06_23, main = paste(names(raw_2022_06_23), day[1]), cex.main = 0.75)\n\n\n\n\n\n\n\nkndvi_2022_06_23 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                             time_range = day[1], \n                             script = script_file_kndvi, \n                             collection = \"sentinel-2-l2a\", \n                             format = \"image/tiff\",\n                             mosaicking_order = \"leastCC\", \n                             resolution = 10, \n                             mask = TRUE, \n                             buffer = 0.01, \n                             client = OAuthClient)\nnames(kndvi_2022_06_23)[1] = c(\"kNDVI\")\nterra::plot(kndvi_2022_06_23[[1]] , main = paste(names(kndvi_2022_06_23 ), day[1]), col = ndvi.col(11), nbreaks = 12, cex.main = 0.75)\n\n\n\n\n\n\n\nsavi_2022_06_23 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                            time_range = day[1], \n                            script = script_file_savi, \n                            collection = \"sentinel-2-l2a\", \n                            format = \"image/tiff\", \n                            mosaicking_order = \"leastCC\", \n                            resolution = 10,\n                            mask = TRUE, \n                            buffer = 0.01, \n                            client = OAuthClient)\nnames(savi_2022_06_23)[1] = c( \"SAVI\")\nterra::plot(savi_2022_06_23[[1]], main = paste(\"SAVI\",day[1]),col = ndvi.col(11), nbreaks = 12, cex.main = 0.75)\n\n\n\n\n\n\n\nevi_2022_06_23 &lt;- GetImage(bbox = st_bbox(train_areas_2019_2020), \n                           time_range = day[1], \n                           script = script_file_evi, \n                           collection = \"sentinel-2-l2a\", \n                           format = \"image/tiff\",\n                           mosaicking_order = \"leastCC\",\n                           resolution = 10, \n                           mask = TRUE, \n                           buffer = 0.01, \n                           client = OAuthClient)\nnames(evi_2022_06_23)[1] = c( \"EVI\")\nterra::plot(evi_2022_06_23[[1]], main = paste(\"EVI\",day[1]), col = ndvi.col(11), nbreaks = 12, cex.main = 0.75)\n\n\n\n\n\n\n\npred_stack_2022 = c(raw_2022_06_23,evi_2022_06_23[[1]],kndvi_2022_06_23[[1]],savi_2022_06_23[[1]])\n\nterra::writeRaster(pred_stack_2018,file.path(root_folder,\"data/pred_stack_2018.tif\"),overwrite=TRUE)\nterra::writeRaster(pred_stack_2022,file.path(root_folder,\"data/pred_stack_2022.tif\"),overwrite=TRUE)",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step2-overview-unsupervised-classification",
    "href": "reader/cd-1.html#step2-overview-unsupervised-classification",
    "title": "Change Detection - Sentinel",
    "section": "Step2: Overview – Unsupervised classification",
    "text": "Step2: Overview – Unsupervised classification\n\nk-means clustering\nIn our example (applied to 5 classes and executed with the function unsuperClass from the RStoolbox package), this looks as follows. The cluster algorithm can achieve a fairly acceptable separation of the clearings/bald spots with 5 clusters, which makes a classification seem promising. Also experiment with other cluster settings and discuss the results.\n\n## k-means über RStoolbox\n\n#\"EVI\"   \"kNDVI\" \"SAVI\"\nprediction_kmeans_2018 = RStoolbox::unsuperClass(pred_stack_2018, \n                                                 nClasses = 5,\n                                                 norm = TRUE, \n                                                 algorithm = \"MacQueen\")\n# Klassifikation\nplot(prediction_kmeans_2018$map)\n\n\n\n\n\n\n\nprediction_kmeans_2022 = RStoolbox::unsuperClass(pred_stack_2022, \n                                                 nClasses = 5,norm = TRUE, \n                                                 algorithm = \"MacQueen\")\nplot(prediction_kmeans_2022$map)\n\n\n\n\n\n\n\n\n\n\nbfast: Spatial identification of magnitudes and time periods of kNDVI changes\nTo apply a more complex time series method such as Breaks For Additive Seasonal and Trend (BFAST), which is often used for land cover changes (e.g. (Wu et al. 2020)), the bfastmonitor() function in gdalcubes can be used, the data cube operations below allow you to provide custom user-defined R functions instead of string expressions that translate to built-in reducers.\nIn our example, bfastmonitor returns change date and change magnitude values per time series so we can use reduce_time(). The script below: 1. calculates the kNDVI, 1. applies bfastmonitor(), and properly handles errors e.g. due to missing data with tryCatch(), and 1. finally writes out the resulting change dates and magnitudes of change for all pixels of the time series as a netCDF file.\nThe results shows the changes starting at 7/2019 until 10/2021.\n\ngdalcubes_options(parallel = 16)\n\nncdf_cube(file.path(root_folder,\"..data/harz_2018_2022_all.nc\")) |&gt;\n  reduce_time(names = c(\"change_date\", \"change_magnitude\"), FUN = function(x) {\n    knr &lt;- exp(-((x[\"B08\",]/10000)-(x[\"B04\",]/10000))^2/(2))\n    kndvi &lt;- (1-knr) / (1+knr)    \n    if (all(is.na(kndvi))) {\n      return(c(NA,NA))\n    }\n    kndvi_ts = ts(kndvi, start = c(2018, 1), frequency = 12)\n    library(bfast)\n    tryCatch({\n        result = bfastmonitor(kndvi_ts, start = c(2019,1), level = 0.01)\n        return(c(result$breakpoint, result$magnitude))\n      }, error = function(x) {\n        return(c(NA,NA))\n      })\n  }) |&gt;\n  write_ncdf(file.path(root_folder,\"data/bfast_results.nc\"), overwrite = TRUE)\n\nNow we can use the netCDF file and map the results with any preferred visualisation tool. In this case tmap.\n\n# plotting it from the local ncdf  \n\ngdalcubes::ncdf_cube(file.path(root_folder,\"..data/bfast_results.nc\")) |&gt;\n   plot(key.pos = 1,  col = ndvi.col(11), nbreaks = 12)\n\n  stars::st_as_stars() -&gt; x\nplot(x[1])\nmapview(x[2])",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-3---generating-training-data",
    "href": "reader/cd-1.html#step-3---generating-training-data",
    "title": "Change Detection - Sentinel",
    "section": "Step 3 - Generating training data",
    "text": "Step 3 - Generating training data\nFor a supervised classification, we need data that indicates which surface class defined areas of the satellite image belong to. This data is referred to as training data and is very often obtained by manual digitization. This can be done quite comfortably in RStudio if only a few training areas have to be digitized quickly and effectively.\nWe assume that we want to classify two types of land cover: clearcut and other.\nFor larger tasks, it makes sense to use the convenient method described in the current QGIS documentation, for example in the digitizing tutorial.\nDigitizing training data in R\nAssuming we have digitized trainingdata using either QGIS or R We need now to extract the values according to the assigned areas:\n\npred_stack_2018 = rast(file.path(root_folder,\"data/pred_stack_2018.tif\"))\npred_stack_2022 = rast(file.path(root_folder,\"data/pred_stack_2022.tif\"))\n# use the provided training data set\n# Extract the training data for the digitized areas\ntDF_2019 = exactextractr::exact_extract(pred_stack_2018, filter(train_areas_2019_2020,year==2019), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\ntDF_2020 = exactextractr::exact_extract(pred_stack_2022, filter(train_areas_2019_2020,year==2020), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\n# again, copy together into a file\ntDF_2019 = dplyr::bind_rows(tDF_2019)\ntDF_2019$year = 2019\ntDF_2020 = dplyr::bind_rows(tDF_2020)\ntDF_2020$year = 2020\n# Delete any rows that contain NA (no data) values\ntDF_2019 = tDF_2019[complete.cases(tDF_2019) ,]\ntDF_2020 = tDF_2020[complete.cases(tDF_2020) ,]\n\ntDF= rbind(tDF_2019,tDF_2020)\n\n# check the extracted data\nsummary(tDF)\n\n    class                B01              B02              B03      \n Length:69883       Min.   :  86.0   Min.   :  45.0   Min.   :  66  \n Class :character   1st Qu.: 177.0   1st Qu.: 170.0   1st Qu.: 338  \n Mode  :character   Median : 216.0   Median : 230.0   Median : 466  \n                    Mean   : 333.4   Mean   : 378.6   Mean   : 598  \n                    3rd Qu.: 459.0   3rd Qu.: 530.0   3rd Qu.: 814  \n                    Max.   :2168.0   Max.   :5620.0   Max.   :7272  \n      B04              B05              B06            B07            B08      \n Min.   :  20.0   Min.   :  15.0   Min.   :   0   Min.   :   0   Min.   :   0  \n 1st Qu.: 190.0   1st Qu.: 642.0   1st Qu.:1751   1st Qu.:1981   1st Qu.:2042  \n Median : 278.0   Median : 854.0   Median :2498   Median :2929   Median :3168  \n Mean   : 555.7   Mean   : 994.3   Mean   :2357   Mean   :2902   Mean   :3085  \n 3rd Qu.: 850.0   3rd Qu.:1332.0   3rd Qu.:3107   3rd Qu.:3946   3rd Qu.:4164  \n Max.   :9344.0   Max.   :5904.0   Max.   :5991   Max.   :6260   Max.   :7032  \n      B8A            B09            B11            B12            EVI        \n Min.   :   0   Min.   :   0   Min.   :  19   Min.   :  17   Min.   :  0.00  \n 1st Qu.:2144   1st Qu.:2219   1st Qu.:1497   1st Qu.: 706   1st Qu.: 12.00  \n Median :3274   Median :3314   Median :1923   Median : 908   Median : 73.00  \n Mean   :3168   Mean   :3214   Mean   :1784   Mean   :1019   Mean   : 84.35  \n 3rd Qu.:4286   3rd Qu.:4349   3rd Qu.:2199   3rd Qu.:1395   3rd Qu.:121.00  \n Max.   :6582   Max.   :6119   Max.   :5012   Max.   :4536   Max.   :255.00  \n     kNDVI             SAVI              x               y        \n Min.   :  0.00   Min.   :  6.00   Min.   :10.21   Min.   :51.85  \n 1st Qu.:  0.00   1st Qu.: 14.00   1st Qu.:10.25   1st Qu.:51.90  \n Median : 27.00   Median : 73.00   Median :10.32   Median :51.93  \n Mean   : 64.77   Mean   : 84.91   Mean   :10.30   Mean   :51.92  \n 3rd Qu.:119.00   3rd Qu.:118.00   3rd Qu.:10.36   3rd Qu.:51.94  \n Max.   :255.00   Max.   :255.00   Max.   :10.40   Max.   :51.94  \n      cell         coverage_fraction      year     \n Min.   : 158288   Min.   :0.0000    Min.   :2019  \n 1st Qu.: 274404   1st Qu.:1.0000    1st Qu.:2019  \n Median : 449043   Median :1.0000    Median :2019  \n Mean   : 596758   Mean   :0.9199    Mean   :2019  \n 3rd Qu.: 813870   3rd Qu.:1.0000    3rd Qu.:2020  \n Max.   :1655492   Max.   :1.0000    Max.   :2020  \n\n# Save as R internal data format\n# is stored in the repo and can therefore be loaded (line below)\nsaveRDS(tDF, file.path(root_folder,\"data/tDF_2018_2022.rds\"))",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-4---supervised-classification",
    "href": "reader/cd-1.html#step-4---supervised-classification",
    "title": "Change Detection - Sentinel",
    "section": "Step 4 - supervised classification",
    "text": "Step 4 - supervised classification\nClassifiers (e.g. the maximum likelihood classifier) or machine learning algorithms (such as Random Forest) use the training data to determine descriptive models that represent statistical signatures, classification trees or other functions. Within the limits of the quality of the training data, such models are suitable and representative for making predictions for areas if the predictors from the model are available for the entire area.\nWe now want to predict the spatial characteristics of clear-felling/no forest using a maximum likelihood classification and random forest, and apply standard methods of random validation and model quality assessment.\nThe goal is to separate clearcuts from all other pixels and to quantify the differences between 2019 and 2020.\n\nMaximum Likelihood Classification\nSince the maximum likelihood algorithm requires training data, it is a supervised learning method. This means that we, as users, have to provide the algorithm with data that conveys knowledge about the classes to be predicted. This data is then divided into training and test data.\n\n# ---- Maximum Likelihood Classification ----\n\n## Here the caret utility package is used\n# Setting a \"seed\" enables reproducible randomness\nset.seed(123)\ntDF = readRDS( file.path(root_folder,\"data/tDF_2018_2022.rds\"))\n# Randomly draw 15% of the data (training/test)\nidx = createDataPartition(tDF$class,list = FALSE,p = 0.05)\ntrainDat = tDF[idx,]\ntestDat = tDF[-idx,]\n\n# Response variable (= \"class\" column) must be of the \"factor\" data type\ntrainDat$class &lt;- as.factor(trainDat$class)\ntestDat$class &lt;- as.factor(testDat$class)\n\n\n# superClass() function from the RSToolbox package requires the table to be converted into the\n# required (old) SpatialdataPoint object\n\nsp_trainDat = trainDat\nsp_testDat = testDat \nsp::coordinates(sp_trainDat) = ~x+y\nsp::coordinates(sp_testDat) = ~x+y\ncrs(sp_trainDat) = crs(pred_stack_2018)\ncrs(sp_testDat) = crs(pred_stack_2018)\n\n\n# superClass method \"mlc\" trains the model and then classifies it\nprediction_mlc_2018 &lt;- superClass(pred_stack_2018, trainData = sp_trainDat[,1:16],valData = sp_testDat[,1:16], responseCol = \"class\", model = \"mlc\", tuneLength = 1, trainPartition = 0.3,verbose = TRUE, filename=file.path(root_folder,\"data/prediction_mlc_2018.tif\"))\n\nMaximum Likelihood Classification \n\n3490 samples\n  15 predictor\n   2 classes: 'clearcut', 'other' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 2793, 2792, 2792, 2791, 2792 \nResampling results:\n\n  Accuracy   Kappa    \n  0.8999987  0.3932103\n\n[[1]]\n  TrainAccuracy TrainKappa method\n1     0.8999987  0.3932103 custom\n\n[[2]]\nCross-Validated (5 fold) Confusion Matrix \n\n(entries are average cell counts across resamples)\n \n          Reference\nPrediction clearcut other\n  clearcut     26.2  68.6\n  other         1.2 602.0\n                         \n Accuracy (average) : 0.9\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction clearcut other\n  clearcut     1265  2538\n  other          63 23130\n                                          \n               Accuracy : 0.9037          \n                 95% CI : (0.9001, 0.9071)\n    No Information Rate : 0.9508          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.4532          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.95256         \n            Specificity : 0.90112         \n         Pos Pred Value : 0.33263         \n         Neg Pred Value : 0.99728         \n             Prevalence : 0.04919         \n         Detection Rate : 0.04686         \n   Detection Prevalence : 0.14087         \n      Balanced Accuracy : 0.92684         \n                                          \n       'Positive' Class : clearcut        \n                                          \n\nprediction_mlc_2022 &lt;- superClass(pred_stack_2022, trainData = sp_trainDat[,1:16],valData = sp_testDat[,1:16], responseCol = \"class\",model = \"mlc\", tuneLength = 1, trainPartition = 0.3,verbose = TRUE,filename=file.path(root_folder,\"data/prediction_mlc_2022.tif\"))\n\nMaximum Likelihood Classification \n\n3490 samples\n  15 predictor\n   2 classes: 'clearcut', 'other' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 2791, 2792, 2792, 2793, 2792 \nResampling results:\n\n  Accuracy   Kappa    \n  0.9469852  0.5640534\n\n[[1]]\n  TrainAccuracy TrainKappa method\n1     0.9469852  0.5640534 custom\n\n[[2]]\nCross-Validated (5 fold) Confusion Matrix \n\n(entries are average cell counts across resamples)\n \n          Reference\nPrediction clearcut other\n  clearcut     26.2  35.8\n  other         1.2 634.8\n                           \n Accuracy (average) : 0.947\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction clearcut other\n  clearcut     1289  1789\n  other          39 23879\n                                          \n               Accuracy : 0.9323          \n                 95% CI : (0.9292, 0.9353)\n    No Information Rate : 0.9508          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.5545          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.97063         \n            Specificity : 0.93030         \n         Pos Pred Value : 0.41878         \n         Neg Pred Value : 0.99837         \n             Prevalence : 0.04919         \n         Detection Rate : 0.04775         \n   Detection Prevalence : 0.11402         \n      Balanced Accuracy : 0.95047         \n                                          \n       'Positive' Class : clearcut        \n                                          \n\nsaveRDS(prediction_mlc_2018, file.path(root_folder,\"data/prediction_mlc_2018.rds\"))\nsaveRDS(prediction_mlc_2022, file.path(root_folder,\"data/prediction_mlc_2022.rds\"))\n\n\n\nRandom forest\nA simplified version of the workflow proposed by Max Kuhn (Kuhn and Max 2008) and improved by Hanna Meyer et al. (Meyer et al. 2024) is used for the random forest classification. For further understanding visit The CAST documentation\n\n\nPrediction on the original data\nNow we are ready to apply the verified model to our data set. In remote sensing, this is usually called classification.\n\nrf_model = readRDS(file.path(root_folder,\"data/rf_model.rds\"))\n\n# Classification (also known as prediction)\nprediction_rf_2018  = terra::predict(pred_stack_2018 ,rf_model)\nprediction_rf_2022  = terra::predict(pred_stack_2022 ,rf_model)\nsaveRDS(prediction_rf_2018, file.path(root_folder,\"data/prediction_rf_2018.rds\"))\nsaveRDS(prediction_rf_2022, file.path(root_folder,\"data/prediction_rf_2022.rds\"))\n\n\n## ##\nprediction_rf_2018 = readRDS(file.path(root_folder,\"data/prediction_rf_2018.rds\"))\nprediction_rf_2022 = readRDS(file.path(root_folder,\"data/prediction_rf_2022.rds\"))\nprediction_mlc_2018 = rast(file.path(root_folder,\"data/prediction_mlc_2018.tif\"))\nprediction_mlc_2022 = rast(file.path(root_folder,\"data/prediction_mlc_2022.tif\"))\n\n## ---- Visualisierung mit mapview ----\nmask = resample(harz_forest_mask,pred_stack_2022)\n\nplot(mask*prediction_rf_2022 - mask*prediction_rf_2018) \n\n\n\n\n\n\n\nplot(mask*prediction_mlc_2022-mask*prediction_mlc_2018)\n\n\n\n\n\n\n\n\n\n\nA visual comparison shows that the Random Forest and Maximum Likelihood classifications provide results of comparable quality. But does this impression stand up to quantitative analysis?\n\n\nStep 5: Estimation model quality\nThe test data are now used for the independent quality check of the model. A confusion matrix indicates how accurately the model predicts the correct classes. The main diagonal of the matrix indicates the cases in which the model applies. In our classification of only two classes, however, a special case applies: evaluation of a binary classifier. Detailed explanations for the function used here can be found in the caret help.\nThe main statements about model quality are:\n\n‘Positive’ Class = clearcut: is measured with the sensitivity (true positive rate), which indicates the probability that a positive object is correctly classified as positive.\n‘Negative Class’ = other: is measured with the specificity (true negative rate) and indicates the probability that a negative object is correctly classified as negative.\nPositive and negative predictive values indicate the actual performance for clearcut and other. They are corrected for the actual frequency distribution and are a measure of the precision and performance of the model with regard to the respective classes.\n\nDespite the high values, we see that the clearcut class drops off significantly here. This can certainly be taken as an indication of the need to improve the classification.\nOverall, however, the model can be considered good.\n\n## ##\n# ----Calculation of the confusion matrix  ----\ncm_rf &lt;- confusionMatrix(data = predict(rf_model, newdata = testDat), testDat$class)\ncm_rf\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction clearcut other\n  clearcut     1396   319\n  other        1204 63469\n                                          \n               Accuracy : 0.9771          \n                 95% CI : (0.9759, 0.9782)\n    No Information Rate : 0.9608          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6357          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.53692         \n            Specificity : 0.99500         \n         Pos Pred Value : 0.81399         \n         Neg Pred Value : 0.98138         \n             Prevalence : 0.03916         \n         Detection Rate : 0.02103         \n   Detection Prevalence : 0.02583         \n      Balanced Accuracy : 0.76596         \n                                          \n       'Positive' Class : clearcut",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#further-support-supervised-classification",
    "href": "reader/cd-1.html#further-support-supervised-classification",
    "title": "Change Detection - Sentinel",
    "section": "Further support supervised classification",
    "text": "Further support supervised classification\nConsider the following resources as examples of how a specific conceptual and technical approach to answering a question can be “crystallized” step by step from the wide range of instructions available on the internet. After a lot of research and critical cross-checking, a “state of research” that is currently considered to be certain within the scientific community can be identified, which can be regarded as a sufficient basis for good scientific practice.\nWork/read through the following selection of blogs and guides, even for practice purposes.\n\nThe core of GIScience Download The editors Tolpekin & Stein 2012 are providing an excellent insight into GI concepts.\nRobert J. Hijmans rspatial - supervised classification\nIvan Lizarazo RPubs Tutorial\nSydney Goldstein blog\nJoão Gonçalves supervised classification\nValentin Stefan pixel-based supervised classification\n\nIn the articles, you will always find both technical instructions and conceptual or specific technical questions and solutions. They are by no means a substitute for specialized scientific knowledge. But they show how technical and conceptual understanding can be developed step by step and, by “replicating” and applying, support the skills needed to approach questions independently.\nI would like to explicitly quote Valentin Stefan, the author of the blog post pixel-based supervised classification:\n\n\n\n\n\n\n“[…] Consider this content a blog post and nothing more. It does not claim to be an exhaustive exercise or a substitute for your critical thinking […].” :::",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "reader/cd-1.html#final-remarks",
    "href": "reader/cd-1.html#final-remarks",
    "title": "Change Detection - Sentinel",
    "section": "Final Remarks",
    "text": "Final Remarks\nYou may have noticed that we have mixed two different approaches to processing satellite data here in a somewhat arbitrary way. On the one hand, the conventional approach of downloading individual data sets and processing them locally, and on the other hand, an introduction to cloud computing, which mainly takes place on the corresponding servers. We have refrained from calculating exclusively on the servers, but technically this is identical. This is didactically questionable, but it shows the direction in which the processing and analysis of mass data is currently moving. The use of cloud-based techniques is essential, especially for long time series and valid systematic data-driven analyses.For further information, please refer to gdalcubes tutorials and (not mentioned in this reader) openeo and the R-interface to openeo.",
    "crumbs": [
      "FAQ",
      "Reader",
      "Classification Workflow"
    ]
  },
  {
    "objectID": "worksheets/ws-07.html",
    "href": "worksheets/ws-07.html",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "",
    "text": "Achieving reliable monitoring hinges on the reproducibility of both field data collection and subsequent analyses. In the context of spatial micro-scale analysis and modeling, the precise geo-object locations play a central role, presenting an additional challenge when considering the 3D structure. Incorporating the temporal aspect, such as accurately recording repeated flights, further elevates these demands. Whether utilizing low-cost or professional UAV systems, these factors must be acknowledged to enable meaningful analysis or modeling. While LiDAR systems are the preferred choice for acquiring forest structure, their high cost, both in acquisition and processing, poses challenges. A feasible alternative lies in off-the-shelf UAV systems with integrated image acquisition. These systems allow for the generation of not only orthorectified planar image data but also quasi-three-dimensional point clouds through photo reconstruction and image processing techniques, providing elevation information for each coordinate."
  },
  {
    "objectID": "worksheets/ws-07.html#aims-and-goals",
    "href": "worksheets/ws-07.html#aims-and-goals",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "Aims and Goals",
    "text": "Aims and Goals\nReproducibility in Data Acquisition and Analysis:\nAims: Establish a framework for reproducible and automated data acquisition and analysis, crucial for robust monitoring.\nGoals: Ensure that the 3D parameters and forest ecological metrics are calculated in a fully reproducible manner, enabling accurate spatial micro-scale modeling.\nIntegration of Temporal and Spatial Aspects:\nAims: Address the challenges posed by the 3D structure and temporal aspects in UAV-based monitoring. Goals: Achieve meaningful analysis and modeling by acknowledging and incorporating precise geo-object locations and accurate recording of repeated flights.\nCost-Effective Alternatives for Forest Structure Acquisition:\nAims: Explore alternatives to expensive LiDAR systems for acquiring forest structure. Goals: Utilize off-the-shelf UAV systems with integrated image acquisition to generate quasi-three-dimensional point clouds, providing elevation information in a cost-effective manner.\nAutomated and Reproducible Workflows:\nAims: Develop automated workflows that streamline data processing and analysis. Goals: Enable a robust and efficient classification method using machine learning object-based image analysis (OBIA), ensuring complex classification goals can be achieved with minimal technical expertise."
  },
  {
    "objectID": "worksheets/ws-07.html#project-repository",
    "href": "worksheets/ws-07.html#project-repository",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "Project Repository",
    "text": "Project Repository\n\nforenius-pp repository for additional functions"
  }
]